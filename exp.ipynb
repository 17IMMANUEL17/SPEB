{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ea737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: activations + derivs\n",
    "# -----------------------------\n",
    "def act(u: torch.Tensor, kind: str) -> torch.Tensor:\n",
    "    if kind == \"tanh\":\n",
    "        return torch.tanh(u)\n",
    "    if kind == \"relu\":\n",
    "        return torch.relu(u)\n",
    "    if kind == \"linear\":\n",
    "        return u\n",
    "    raise ValueError(f\"Unknown activation: {kind}\")\n",
    "\n",
    "\n",
    "def act_prime_from_u(u: torch.Tensor, kind: str) -> torch.Tensor:\n",
    "    if kind == \"tanh\":\n",
    "        a = torch.tanh(u)\n",
    "        return 1.0 - a * a\n",
    "    if kind == \"relu\":\n",
    "        return (u > 0).to(u.dtype)\n",
    "    if kind == \"linear\":\n",
    "        return torch.ones_like(u)\n",
    "    raise ValueError(f\"Unknown activation: {kind}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model definition (layerwise)\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class MLP:\n",
    "    Ws: List[torch.Tensor]\n",
    "    bs: List[torch.Tensor]\n",
    "    acts: List[str]\n",
    "\n",
    "    @property\n",
    "    def L(self) -> int:\n",
    "        return len(self.Ws)\n",
    "\n",
    "    @property\n",
    "    def sizes(self) -> List[int]:\n",
    "        return [self.Ws[0].shape[1]] + [W.shape[0] for W in self.Ws]\n",
    "\n",
    "\n",
    "def forward_layerwise(net: MLP, x0: torch.Tensor) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "    a = x0\n",
    "    zs, acts_out = [], []\n",
    "    for W, b, k in zip(net.Ws, net.bs, net.acts):\n",
    "        z = W @ a + b\n",
    "        a = act(z, k)\n",
    "        zs.append(z)\n",
    "        acts_out.append(a)\n",
    "    return zs, acts_out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Packing/unpacking global state\n",
    "# -----------------------------\n",
    "def layer_slices(sizes: List[int]) -> List[slice]:\n",
    "    idx = 0\n",
    "    sl = []\n",
    "    for n in sizes[1:]:\n",
    "        sl.append(slice(idx, idx + n))\n",
    "        idx += n\n",
    "    return sl\n",
    "\n",
    "\n",
    "def pack_layers(layers: List[torch.Tensor]) -> torch.Tensor:\n",
    "    return torch.cat(layers, dim=0)\n",
    "\n",
    "\n",
    "def unpack_global(v: torch.Tensor, sl: List[slice]) -> List[torch.Tensor]:\n",
    "    return [v[s] for s in sl]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Global operators without forming big W\n",
    "# -----------------------------\n",
    "def compute_u(net: MLP, x0: torch.Tensor, m_layers: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "    u = []\n",
    "    for l in range(net.L):\n",
    "        inp = x0 if l == 0 else m_layers[l - 1]\n",
    "        u.append(net.Ws[l] @ inp + net.bs[l])\n",
    "    return u\n",
    "\n",
    "\n",
    "def apply_WT_Ds(net: MLP, Ds_layers: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "    L = net.L\n",
    "    v = [torch.zeros_like(Ds_layers[l]) for l in range(L)]\n",
    "    for j in range(1, L):\n",
    "        v[j - 1] = v[j - 1] + net.Ws[j].T @ Ds_layers[j]\n",
    "    return v\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Grad extraction from (m,s) readout\n",
    "# -----------------------------\n",
    "def relaxation_gradients(\n",
    "    net: MLP,\n",
    "    x0: torch.Tensor,\n",
    "    m_layers: List[torch.Tensor],\n",
    "    u_layers: List[torch.Tensor],\n",
    "    s_layers: List[torch.Tensor],\n",
    ") -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "    deltas = []\n",
    "    for l in range(net.L):\n",
    "        D = act_prime_from_u(u_layers[l], net.acts[l])\n",
    "        deltas.append(D * s_layers[l])\n",
    "\n",
    "    dWs, dbs = [], []\n",
    "    for l in range(net.L):\n",
    "        inp = x0 if l == 0 else m_layers[l - 1]\n",
    "        dWs.append(deltas[l].unsqueeze(1) @ inp.unsqueeze(0))\n",
    "        dbs.append(deltas[l].clone())\n",
    "    return dWs, dbs\n",
    "\n",
    "\n",
    "def rel_error(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-12) -> float:\n",
    "    return (torch.norm(a - b) / (torch.norm(b) + eps)).item()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Baseline: autograd gradients\n",
    "# -----------------------------\n",
    "def autograd_gradients(net: MLP, x0: torch.Tensor, y: torch.Tensor) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "    Ws = [W.clone().detach().requires_grad_(True) for W in net.Ws]\n",
    "    bs = [b.clone().detach().requires_grad_(True) for b in net.bs]\n",
    "    tmp = MLP(Ws=Ws, bs=bs, acts=net.acts)\n",
    "\n",
    "    _, a = forward_layerwise(tmp, x0)\n",
    "    out = a[-1]\n",
    "    loss = 0.5 * torch.sum((out - y) ** 2)\n",
    "    loss.backward()\n",
    "\n",
    "    dWs = [W.grad.detach().clone() for W in Ws]\n",
    "    dbs = [b.grad.detach().clone() for b in bs]\n",
    "    return dWs, dbs\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# x-z relaxation (arbitrary eta) with CORRECTED step counting\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def xz_relaxation(\n",
    "    net: MLP,\n",
    "    x0: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    eta: float,\n",
    "    T: int = 5000,\n",
    "    tol: float = 1e-12,\n",
    "    log_every: int = 1,\n",
    ") -> Dict[str, Any]:\n",
    "    sizes = net.sizes\n",
    "    sl = layer_slices(sizes)\n",
    "    n_global = sum(sizes[1:])\n",
    "\n",
    "    x = torch.zeros(n_global)\n",
    "    z = torch.zeros(n_global)\n",
    "\n",
    "    # reference gradients via standard backprop (needs grad enabled)\n",
    "    with torch.enable_grad():\n",
    "        dWs_bp, dbs_bp = autograd_gradients(net, x0, y)\n",
    "    flat_bp_W = torch.cat([g.flatten() for g in dWs_bp])\n",
    "    flat_bp_b = torch.cat([g.flatten() for g in dbs_bp])\n",
    "\n",
    "    def embed_output_grad(gL: torch.Tensor) -> torch.Tensor:\n",
    "        g = torch.zeros(n_global)\n",
    "        g[sl[-1]] = gL\n",
    "        return g\n",
    "\n",
    "    # logs: step k means \"after k Euler updates\"\n",
    "    steps: List[int] = []\n",
    "    times: List[float] = []\n",
    "    grad_err_W: List[float] = []\n",
    "    grad_err_b: List[float] = []\n",
    "    step_norms: List[float] = []\n",
    "\n",
    "    def log_state(k: int):\n",
    "        m = 0.5 * (x + z)\n",
    "        s = (x - z)\n",
    "        m_layers = unpack_global(m, sl)\n",
    "        s_layers = unpack_global(s, sl)\n",
    "        u_layers = compute_u(net, x0, m_layers)\n",
    "        dWs_xz, dbs_xz = relaxation_gradients(net, x0, m_layers, u_layers, s_layers)\n",
    "\n",
    "        flat_xz_W = torch.cat([g.flatten() for g in dWs_xz])\n",
    "        flat_xz_b = torch.cat([g.flatten() for g in dbs_xz])\n",
    "\n",
    "        steps.append(k)\n",
    "        times.append(k * eta)\n",
    "        grad_err_W.append(rel_error(flat_xz_W, flat_bp_W, eps=1e-18))\n",
    "        grad_err_b.append(rel_error(flat_xz_b, flat_bp_b, eps=1e-18))\n",
    "\n",
    "    # log initial state (k=0)\n",
    "    log_state(0)\n",
    "\n",
    "    k = 0\n",
    "    while k < T:\n",
    "        # compute RHS at current (x,z)\n",
    "        m = 0.5 * (x + z)\n",
    "        s = (x - z)\n",
    "\n",
    "        m_layers = unpack_global(m, sl)\n",
    "        s_layers = unpack_global(s, sl)\n",
    "\n",
    "        u_layers = compute_u(net, x0, m_layers)\n",
    "        sig_layers = [act(u_layers[l], net.acts[l]) for l in range(net.L)]\n",
    "        sig = pack_layers(sig_layers)\n",
    "\n",
    "        # loss gradient wrt mean output (MSE)\n",
    "        mL = m_layers[-1]\n",
    "        gL = (mL - y)\n",
    "        g = embed_output_grad(gL)\n",
    "\n",
    "        # coupling term W^T (D ⊙ s)\n",
    "        Ds_layers = []\n",
    "        for l in range(net.L):\n",
    "            D = act_prime_from_u(u_layers[l], net.acts[l])\n",
    "            Ds_layers.append(D * s_layers[l])\n",
    "        WT_Ds = pack_layers(apply_WT_Ds(net, Ds_layers))\n",
    "\n",
    "        dx = (sig - x) + 0.5 * WT_Ds + 0.5 * g\n",
    "        dz = (sig - z) - 0.5 * WT_Ds - 0.5 * g\n",
    "\n",
    "        x_next = x + eta * dx\n",
    "        z_next = z + eta * dz\n",
    "\n",
    "        sn = (torch.norm(x_next - x) + torch.norm(z_next - z)).item()\n",
    "        step_norms.append(sn)\n",
    "\n",
    "        x, z = x_next, z_next\n",
    "        k += 1\n",
    "\n",
    "        if (k % log_every) == 0:\n",
    "            log_state(k)\n",
    "\n",
    "        if sn < tol:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"eta\": eta,\n",
    "        \"L\": net.L,\n",
    "        \"steps\": steps,\n",
    "        \"times\": times,\n",
    "        \"grad_err_W\": grad_err_W,\n",
    "        \"grad_err_b\": grad_err_b,\n",
    "        \"step_norms\": step_norms,   # length = number of updates performed\n",
    "        \"stop_step\": k,             # EXACT number of Euler updates performed\n",
    "        \"stop_time\": k * eta,\n",
    "        \"converged\": (k < T),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Build a random MLP\n",
    "# -----------------------------\n",
    "def make_mlp(n0: int, hidden_width: int, hidden_layers: int, nL: int, seed: int = 0) -> MLP:\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    sizes = [n0] + [hidden_width] * hidden_layers + [nL]\n",
    "    L = len(sizes) - 1\n",
    "    acts = [\"tanh\"] * (L - 1) + [\"linear\"]\n",
    "\n",
    "    Ws, bs = [], []\n",
    "    for l in range(1, len(sizes)):\n",
    "        n_in, n_out = sizes[l - 1], sizes[l]\n",
    "        W = 0.3 * torch.randn(n_out, n_in, generator=g)\n",
    "        b = 0.1 * torch.randn(n_out, generator=g)\n",
    "        Ws.append(W)\n",
    "        bs.append(b)\n",
    "    return MLP(Ws=Ws, bs=bs, acts=acts)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Sweep many etas + plots\n",
    "# -----------------------------\n",
    "def main():\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # fixed network\n",
    "    n0, nL = 10, 5\n",
    "    hidden_width = 32\n",
    "    hidden_layers = 3  # => L = 4\n",
    "    net = make_mlp(n0, hidden_width, hidden_layers, nL, seed=123)\n",
    "\n",
    "    x0 = torch.randn(n0)\n",
    "    y = torch.randn(nL)\n",
    "\n",
    "    etas = [0.02, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "    T = 20000\n",
    "    tol = 1e-12\n",
    "\n",
    "    results = []\n",
    "    for eta in etas:\n",
    "        # log more sparsely for small eta to keep plots light\n",
    "        log_every = 50 if eta <= 0.05 else (10 if eta < 0.2 else 1)\n",
    "        out = xz_relaxation(net, x0, y, eta=eta, T=T, tol=tol, log_every=log_every)\n",
    "        results.append(out)\n",
    "        print(\n",
    "            f\"eta={eta:g}  converged={out['converged']}  \"\n",
    "            f\"stop_step={out['stop_step']}  stop_time={out['stop_time']:.3f}  \"\n",
    "            f\"final relerrW={out['grad_err_W'][-1]:.3e}\"\n",
    "        )\n",
    "\n",
    "    # Plot 1: gradient error vs steps (k = #updates)\n",
    "    plt.figure()\n",
    "    for out in results:\n",
    "        plt.semilogy(out[\"steps\"], out[\"grad_err_W\"], marker=\"o\", markersize=3, linewidth=1)\n",
    "    plt.xlabel(\"Euler updates k\")\n",
    "    plt.ylabel(\"Relative gradient error (dW)\")\n",
    "    plt.title(\"x-z relaxation: gradient match vs updates (eta sweep)\")\n",
    "    plt.grid(True, which=\"both\", linestyle=\":\")\n",
    "    plt.legend([f\"eta={out['eta']:g}\" for out in results])\n",
    "\n",
    "    # Plot 2: gradient error vs physical time t=eta*k\n",
    "    plt.figure()\n",
    "    for out in results:\n",
    "        plt.semilogy(out[\"times\"], out[\"grad_err_W\"], marker=\"o\", markersize=3, linewidth=1)\n",
    "    plt.xlabel(\"Physical time t = eta * k\")\n",
    "    plt.ylabel(\"Relative gradient error (dW)\")\n",
    "    plt.title(\"x-z relaxation: gradient match vs physical time\")\n",
    "    plt.grid(True, which=\"both\", linestyle=\":\")\n",
    "    plt.legend([f\"eta={out['eta']:g}\" for out in results])\n",
    "\n",
    "    # Plot 3: step norm vs update index (step_norms length = stop_step)\n",
    "    plt.figure()\n",
    "    for out in results:\n",
    "        k = list(range(1, out[\"stop_step\"] + 1))\n",
    "        plt.semilogy(k, out[\"step_norms\"], marker=\"o\", markersize=2, linewidth=1)\n",
    "    plt.xlabel(\"Euler update index\")\n",
    "    plt.ylabel(\"Step norm ||Δx|| + ||Δz||\")\n",
    "    plt.title(\"x-z relaxation: convergence residual vs updates\")\n",
    "    plt.grid(True, which=\"both\", linestyle=\":\")\n",
    "    plt.legend([f\"eta={out['eta']:g}\" for out in results])\n",
    "\n",
    "    # Plot 4: steps-to-converge vs eta\n",
    "    plt.figure()\n",
    "    eta_ok = [out[\"eta\"] for out in results if out[\"converged\"]]\n",
    "    steps_ok = [out[\"stop_step\"] for out in results if out[\"converged\"]]\n",
    "    plt.plot(eta_ok, steps_ok, marker=\"o\")\n",
    "    plt.xlabel(\"eta\")\n",
    "    plt.ylabel(\"Updates to tol (k)\")\n",
    "    plt.title(\"Updates-to-converge vs eta\")\n",
    "    plt.grid(True, linestyle=\":\")\n",
    "\n",
    "    # Plot 5: physical time to converge vs eta\n",
    "    plt.figure()\n",
    "    time_ok = [out[\"stop_time\"] for out in results if out[\"converged\"]]\n",
    "    plt.plot(eta_ok, time_ok, marker=\"o\")\n",
    "    plt.xlabel(\"eta\")\n",
    "    plt.ylabel(\"Physical time to tol (eta * k)\")\n",
    "    plt.title(\"Physical convergence time vs eta\")\n",
    "    plt.grid(True, linestyle=\":\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Activations\n",
    "# -----------------------------\n",
    "def act(u: torch.Tensor, kind: str) -> torch.Tensor:\n",
    "    if kind == \"tanh\":\n",
    "        return torch.tanh(u)\n",
    "    if kind == \"relu\":\n",
    "        return torch.relu(u)\n",
    "    if kind == \"linear\":\n",
    "        return u\n",
    "    raise ValueError(kind)\n",
    "\n",
    "\n",
    "def act_prime_from_u(u: torch.Tensor, kind: str) -> torch.Tensor:\n",
    "    if kind == \"tanh\":\n",
    "        a = torch.tanh(u)\n",
    "        return 1.0 - a * a\n",
    "    if kind == \"relu\":\n",
    "        return (u > 0).to(u.dtype)\n",
    "    if kind == \"linear\":\n",
    "        return torch.ones_like(u)\n",
    "    raise ValueError(kind)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Simple MLP container\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class MLP:\n",
    "    Ws: List[torch.Tensor]\n",
    "    bs: List[torch.Tensor]\n",
    "    acts: List[str]\n",
    "\n",
    "    @property\n",
    "    def L(self) -> int:\n",
    "        return len(self.Ws)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        a = x\n",
    "        for W, b, k in zip(self.Ws, self.bs, self.acts):\n",
    "            a = act(a @ W.t() + b, k)\n",
    "        return a\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# xz relaxation for a batch\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def xz_relax_batch_grad(\n",
    "    net: MLP,\n",
    "    x0: torch.Tensor,            # (B, n0)\n",
    "    y_onehot: torch.Tensor,      # (B, nL)\n",
    "    eta: float = 0.1,\n",
    "    K: int = 40,\n",
    ") -> Tuple[List[torch.Tensor], List[torch.Tensor], float]:\n",
    "    device = x0.device\n",
    "    B = x0.shape[0]\n",
    "    L = net.L\n",
    "\n",
    "    x_layers = [torch.zeros(B, net.Ws[l].shape[0], device=device) for l in range(L)]\n",
    "    z_layers = [torch.zeros_like(x_layers[l]) for l in range(L)]\n",
    "\n",
    "    def compute_u_and_sig(m_layers):\n",
    "        u_layers, sig_layers = [], []\n",
    "        for l in range(L):\n",
    "            inp = x0 if l == 0 else m_layers[l - 1]\n",
    "            u = inp @ net.Ws[l].t() + net.bs[l]\n",
    "            u_layers.append(u)\n",
    "            sig_layers.append(act(u, net.acts[l]))\n",
    "        return u_layers, sig_layers\n",
    "\n",
    "    for _ in range(K):\n",
    "        m_layers = [(x_layers[l] + z_layers[l]) * 0.5 for l in range(L)]\n",
    "        s_layers = [x_layers[l] - z_layers[l] for l in range(L)]\n",
    "\n",
    "        u_layers, sig_layers = compute_u_and_sig(m_layers)\n",
    "\n",
    "        # --- Cross-entropy gradient wrt logits at output ---\n",
    "        logits = m_layers[-1]\n",
    "        p = torch.softmax(logits, dim=1)\n",
    "        gL = (p - y_onehot)\n",
    "\n",
    "        # q_l = D_l ⊙ s_l\n",
    "        q_layers = []\n",
    "        for l in range(L):\n",
    "            D = act_prime_from_u(u_layers[l], net.acts[l])\n",
    "            q_layers.append(D * s_layers[l])\n",
    "\n",
    "        # WTq propagation: (WTq)_{l-1} += q_l @ W_l  (since q_l is row-batch)\n",
    "        WTq_layers = [torch.zeros_like(q_layers[l]) for l in range(L)]\n",
    "        for l in range(1, L):\n",
    "            WTq_layers[l - 1] = WTq_layers[l - 1] + (q_layers[l] @ net.Ws[l])\n",
    "\n",
    "        for l in range(L):\n",
    "            g = gL if l == (L - 1) else 0.0\n",
    "            dx = (sig_layers[l] - x_layers[l]) + 0.5 * WTq_layers[l] + 0.5 * g\n",
    "            dz = (sig_layers[l] - z_layers[l]) - 0.5 * WTq_layers[l] - 0.5 * g\n",
    "            x_layers[l] = x_layers[l] + eta * dx\n",
    "            z_layers[l] = z_layers[l] + eta * dz\n",
    "\n",
    "    # Readout gradients\n",
    "    m_layers = [(x_layers[l] + z_layers[l]) * 0.5 for l in range(L)]\n",
    "    s_layers = [x_layers[l] - z_layers[l] for l in range(L)]\n",
    "    u_layers, _ = compute_u_and_sig(m_layers)\n",
    "\n",
    "    deltas = []\n",
    "    for l in range(L):\n",
    "        D = act_prime_from_u(u_layers[l], net.acts[l])\n",
    "        deltas.append(D * s_layers[l])\n",
    "\n",
    "    dWs, dbs = [], []\n",
    "    for l in range(L):\n",
    "        inp = x0 if l == 0 else m_layers[l - 1]\n",
    "        dW = torch.einsum(\"bi,bj->ij\", deltas[l], inp) / B\n",
    "        db = deltas[l].mean(dim=0)\n",
    "        dWs.append(dW)\n",
    "        dbs.append(db)\n",
    "\n",
    "    # For logging only (not the optimized objective): CE value\n",
    "    ce = -(y_onehot * torch.log(torch.softmax(m_layers[-1], dim=1) + 1e-12)).sum(dim=1).mean().item()\n",
    "    return dWs, dbs, ce\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SGD with momentum + weight decay + clipping\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def sgd_momentum_step(\n",
    "    net: MLP,\n",
    "    dWs: List[torch.Tensor],\n",
    "    dbs: List[torch.Tensor],\n",
    "    vWs: List[torch.Tensor],\n",
    "    vbs: List[torch.Tensor],\n",
    "    lr: float,\n",
    "    momentum: float = 0.9,\n",
    "    weight_decay: float = 1e-4,\n",
    "    clip: float = 1.0,\n",
    "):\n",
    "    # weight decay\n",
    "    for l in range(net.L):\n",
    "        dWs[l] = dWs[l] + weight_decay * net.Ws[l]\n",
    "\n",
    "    # global grad norm clipping\n",
    "    gn2 = 0.0\n",
    "    for g in dWs:\n",
    "        gn2 += float(g.norm() ** 2)\n",
    "    for g in dbs:\n",
    "        gn2 += float(g.norm() ** 2)\n",
    "    gn = gn2 ** 0.5\n",
    "    scale = 1.0 if gn <= clip else (clip / (gn + 1e-12))\n",
    "    if scale != 1.0:\n",
    "        dWs = [g * scale for g in dWs]\n",
    "        dbs = [g * scale for g in dbs]\n",
    "\n",
    "    # momentum update\n",
    "    for l in range(net.L):\n",
    "        vWs[l].mul_(momentum).add_(dWs[l])\n",
    "        vbs[l].mul_(momentum).add_(dbs[l])\n",
    "        net.Ws[l].sub_(lr * vWs[l])\n",
    "        net.bs[l].sub_(lr * vbs[l])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(net: MLP, loader: DataLoader, device: torch.device, max_batches: int = 200) -> float:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        x = x.to(device).view(x.size(0), -1)\n",
    "        y = y.to(device)\n",
    "        logits = net.forward(x)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "\n",
    "def lr_schedule(epoch: int) -> float:\n",
    "    # simple 3-phase schedule\n",
    "    if epoch <= 10:\n",
    "        return 0.01\n",
    "    if epoch <= 20:\n",
    "        return 0.005\n",
    "    return 0.001\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=tfm)\n",
    "    test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=tfm)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # MLP: 784 -> 256 -> 256 -> 10\n",
    "    n0, n1, n2, nL = 784, 256, 256, 10\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    init = 0.02\n",
    "    W1 = init * torch.randn(n1, n0, device=device)\n",
    "    b1 = torch.zeros(n1, device=device)\n",
    "    W2 = init * torch.randn(n2, n1, device=device)\n",
    "    b2 = torch.zeros(n2, device=device)\n",
    "    W3 = init * torch.randn(nL, n2, device=device)\n",
    "    b3 = torch.zeros(nL, device=device)\n",
    "\n",
    "    net = MLP(Ws=[W1, W2, W3], bs=[b1, b2, b3], acts=[\"relu\", \"relu\", \"linear\"])\n",
    "\n",
    "    # momentum buffers\n",
    "    vWs = [torch.zeros_like(W) for W in net.Ws]\n",
    "    vbs = [torch.zeros_like(b) for b in net.bs]\n",
    "\n",
    "    epochs = 15\n",
    "    eta = 1  # xz-relaxation step size\n",
    "    K = 40         # inner steps per batch\n",
    "    momentum = 0.9\n",
    "    weight_decay = 5e-4\n",
    "    clip = 1.0\n",
    "\n",
    "    train_ce_hist = []\n",
    "    test_acc_hist = []\n",
    "    step_hist = []\n",
    "    epoch_acc_hist = []\n",
    "    epoch_hist = []\n",
    "    global_step = 0\n",
    "    eval_every = 200\n",
    "\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        lr = lr_schedule(ep)\n",
    "        running = 0.0\n",
    "\n",
    "        for bi, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device).view(x.size(0), -1)\n",
    "            y = y.to(device)\n",
    "            y_onehot = F.one_hot(y, num_classes=10).to(x.dtype)\n",
    "\n",
    "            dWs, dbs, ce = xz_relax_batch_grad(net, x, y_onehot, eta=eta, K=K)\n",
    "            sgd_momentum_step(\n",
    "                net, dWs, dbs, vWs, vbs,\n",
    "                lr=lr, momentum=momentum, weight_decay=weight_decay, clip=clip\n",
    "            )\n",
    "\n",
    "\n",
    "            global_step += 1\n",
    "            train_ce_hist.append(ce)\n",
    "            step_hist.append(global_step)\n",
    "\n",
    "            if (global_step % eval_every) == 0:\n",
    "                acc = accuracy(net, test_loader, device=device, max_batches=400)\n",
    "                test_acc_hist.append(acc)\n",
    "                print(f\"step {global_step}: train-CE~{sum(train_ce_hist[-eval_every:])/eval_every:.4f}  test-acc={acc*100:.2f}%\")\n",
    "\n",
    "\n",
    "            running += ce\n",
    "            if (bi + 1) % 200 == 0:\n",
    "                acc = accuracy(net, test_loader, device=device, max_batches=200)\n",
    "                print(f\"epoch {ep:02d} batch {bi+1:04d}: CE={running/200:.4f}  lr={lr:.4g}  test-acc~{acc*100:.2f}%\")\n",
    "                running = 0.0\n",
    "\n",
    "        epoch_hist.append(ep)\n",
    "        epoch_acc_hist.append(acc)\n",
    "\n",
    "        acc = accuracy(net, test_loader, device=device, max_batches=400)\n",
    "        print(f\"END epoch {ep:02d}: lr={lr:.4g}  test-acc~{acc*100:.2f}%\")\n",
    "\n",
    "    acc = accuracy(net, test_loader, device=device, max_batches=800)\n",
    "    print(f\"Final test-acc (approx): {acc*100:.2f}%\")\n",
    "    # ---- Plot 1: Train CE vs step (smoothed)\n",
    "    plt.figure()\n",
    "    # simple moving average\n",
    "    win = 200\n",
    "    if len(train_ce_hist) >= win:\n",
    "        sm = [sum(train_ce_hist[i-win:i])/win for i in range(win, len(train_ce_hist)+1)]\n",
    "        plt.plot(step_hist[win-1:], sm)\n",
    "    else:\n",
    "        plt.plot(step_hist, train_ce_hist)\n",
    "    plt.xlabel(\"Update step\")\n",
    "    plt.ylabel(\"Train cross-entropy (moving avg)\")\n",
    "    plt.title(\"Training loss vs steps\")\n",
    "    plt.grid(True, linestyle=\":\")\n",
    "\n",
    "    # ---- Plot 2: Test accuracy vs step\n",
    "    plt.figure()\n",
    "    eval_steps = [eval_every*(i+1) for i in range(len(test_acc_hist))]\n",
    "    plt.plot(eval_steps, [100*a for a in test_acc_hist], marker=\"o\")\n",
    "    plt.xlabel(\"Update step\")\n",
    "    plt.ylabel(\"Test accuracy (%)\")\n",
    "    plt.title(\"Test accuracy vs steps\")\n",
    "    plt.grid(True, linestyle=\":\")\n",
    "\n",
    "    # ---- Plot 3 (optional): Test accuracy vs epoch\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_hist, [100*a for a in epoch_acc_hist], marker=\"o\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Test accuracy (%)\")\n",
    "    plt.title(\"Test accuracy vs epoch\")\n",
    "    plt.grid(True, linestyle=\":\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245ed025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400: lr=0.01976  train-CE~0.8341  EMA acc=79.43%  avg_relax_steps=30.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 402\u001b[39m\n\u001b[32m    397\u001b[39m     plt.show()\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 338\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    333\u001b[39m lr = cosine_lr(global_step, total_steps, lr_max=\u001b[32m0.02\u001b[39m, lr_min=\u001b[32m2e-4\u001b[39m)\n\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# CHANGED CALL: now returns ce + steps_taken\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m gradsW, gradsb, ce, steps_taken = \u001b[43mxz_relax_batch_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43meta\u001b[49m\u001b[43m=\u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarm_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    344\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m sgd_momentum_step(net, gradsW, gradsb, vW, vb,\n\u001b[32m    347\u001b[39m                   lr=lr, momentum=momentum, weight_decay=weight_decay, clip=clip)\n\u001b[32m    349\u001b[39m ema_update(ema_net, net, decay=ema_decay)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 146\u001b[39m, in \u001b[36mxz_relax_batch_grad\u001b[39m\u001b[34m(net, x0, y, eta, K, state, tol, warm_start)\u001b[39m\n\u001b[32m    142\u001b[39m x4.add_(dx4, alpha=eta); z4.add_(dz4, alpha=eta)\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# early stop (cheap criterion)\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# (avoid .item() too often; but once per iter is usually fine)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m upd = \u001b[43m(\u001b[49m\u001b[43mdx1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx4\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m upd < tol:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from torch.nn.grad import conv2d_weight\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Activations``\n",
    "# -----------------------------\n",
    "def relu(u): return torch.relu(u)\n",
    "def relu_prime(u): return (u > 0).to(u.dtype)\n",
    "\n",
    "# -----------------------------\n",
    "# Model container (3 conv + fc)\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class CNN3:\n",
    "    W1: torch.Tensor; b1: torch.Tensor   # (64,1,3,3)\n",
    "    W2: torch.Tensor; b2: torch.Tensor   # (64,64,3,3)\n",
    "    W3: torch.Tensor; b3: torch.Tensor   # (128,64,3,3) stride 2\n",
    "    W4: torch.Tensor; b4: torch.Tensor   # (10, 128*14*14)\n",
    "\n",
    "    @property\n",
    "    def device(self): return self.W1.device\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Forward at mean states\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def forward_u_sig(net: CNN3, x0, m1, m2, m3):\n",
    "    # layer1 uses raw input\n",
    "    u1 = F.conv2d(x0, net.W1, net.b1, stride=1, padding=1)  # (B,64,28,28)\n",
    "    sig1 = relu(u1)\n",
    "\n",
    "    # layer2 uses mean m1\n",
    "    u2 = F.conv2d(m1, net.W2, net.b2, stride=1, padding=1)  # (B,64,28,28)\n",
    "    sig2 = relu(u2)\n",
    "\n",
    "    # layer3 uses mean m2\n",
    "    u3 = F.conv2d(m2, net.W3, net.b3, stride=2, padding=1)  # (B,128,14,14)\n",
    "    sig3 = relu(u3)\n",
    "\n",
    "    # fc uses mean m3\n",
    "    B = x0.shape[0]\n",
    "    u4 = m3.reshape(B, -1) @ net.W4.t() + net.b4            # (B,10)\n",
    "    sig4 = u4  # linear logits\n",
    "    return (u1, sig1), (u2, sig2), (u3, sig3), (u4, sig4)\n",
    "\n",
    "class XZState:\n",
    "    def __init__(self):\n",
    "        self.x1 = None; self.z1 = None\n",
    "        self.x2 = None; self.z2 = None\n",
    "        self.x3 = None; self.z3 = None\n",
    "        self.x4 = None; self.z4 = None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# x-z relaxation gradient for batch\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def xz_relax_batch_grad(\n",
    "    net: CNN3, x0, y,\n",
    "    eta=1.0, K=30,\n",
    "    state: XZState | None = None,\n",
    "    tol: float = 1e-4,\n",
    "    warm_start: bool = True,\n",
    "):\n",
    "    device = net.device\n",
    "    B = x0.shape[0]\n",
    "    y_onehot = F.one_hot(y, num_classes=10).to(x0.dtype)\n",
    "\n",
    "    # -------------------------\n",
    "    # Init / warm-start x,z\n",
    "    # -------------------------\n",
    "    def alloc():\n",
    "        x1 = torch.zeros(B, 64, 28, 28, device=device); z1 = torch.zeros_like(x1)\n",
    "        x2 = torch.zeros(B, 64, 28, 28, device=device); z2 = torch.zeros_like(x2)\n",
    "        x3 = torch.zeros(B, 128, 14, 14, device=device); z3 = torch.zeros_like(x3)\n",
    "        x4 = torch.zeros(B, 10, device=device);         z4 = torch.zeros_like(x4)\n",
    "        return x1,z1,x2,z2,x3,z3,x4,z4\n",
    "\n",
    "    if (state is None) or (not warm_start) or (state.x1 is None) or (state.x1.shape[0] != B):\n",
    "        x1,z1,x2,z2,x3,z3,x4,z4 = alloc()\n",
    "        if state is not None:\n",
    "            state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4 = x1,z1,x2,z2,x3,z3,x4,z4\n",
    "    else:\n",
    "        x1,z1,x2,z2,x3,z3,x4,z4 = state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4\n",
    "\n",
    "    # -------------------------\n",
    "    # Relaxation loop\n",
    "    # -------------------------\n",
    "    steps_taken = 0\n",
    "    for _ in range(K):\n",
    "        steps_taken += 1\n",
    "\n",
    "        # mean/stress (avoid extra temporaries where possible)\n",
    "        m1 = (x1 + z1) * 0.5; s1 = (x1 - z1)\n",
    "        m2 = (x2 + z2) * 0.5; s2 = (x2 - z2)\n",
    "        m3 = (x3 + z3) * 0.5; s3 = (x3 - z3)\n",
    "        m4 = (x4 + z4) * 0.5; s4 = (x4 - z4)\n",
    "\n",
    "        (u1, sig1), (u2, sig2), (u3, sig3), (u4, sig4) = forward_u_sig(net, x0, m1, m2, m3)\n",
    "\n",
    "        # CE grad wrt logits\n",
    "        p = torch.softmax(m4, dim=1)\n",
    "        g4 = (p - y_onehot)\n",
    "\n",
    "        # q = D ⊙ s\n",
    "        q2 = relu_prime(u2) * s2\n",
    "        q3 = relu_prime(u3) * s3\n",
    "        q4 = s4  # linear\n",
    "\n",
    "        # W^T q propagation\n",
    "        WTq3 = (q4 @ net.W4).reshape(B, 128, 14, 14)\n",
    "        WTq2 = F.conv_transpose2d(q3, net.W3, bias=None, stride=2, padding=1, output_padding=1)\n",
    "        WTq1 = F.conv_transpose2d(q2, net.W2, bias=None, stride=1, padding=1)\n",
    "\n",
    "        # dynamics\n",
    "        dx1 = (sig1 - x1) + 0.5 * WTq1\n",
    "        dz1 = (sig1 - z1) - 0.5 * WTq1\n",
    "\n",
    "        dx2 = (sig2 - x2) + 0.5 * WTq2\n",
    "        dz2 = (sig2 - z2) - 0.5 * WTq2\n",
    "\n",
    "        dx3 = (sig3 - x3) + 0.5 * WTq3\n",
    "        dz3 = (sig3 - z3) - 0.5 * WTq3\n",
    "\n",
    "        dx4 = (sig4 - x4) + 0.5 * g4\n",
    "        dz4 = (sig4 - z4) - 0.5 * g4\n",
    "\n",
    "        # Euler update\n",
    "        x1.add_(dx1, alpha=eta); z1.add_(dz1, alpha=eta)\n",
    "        x2.add_(dx2, alpha=eta); z2.add_(dz2, alpha=eta)\n",
    "        x3.add_(dx3, alpha=eta); z3.add_(dz3, alpha=eta)\n",
    "        x4.add_(dx4, alpha=eta); z4.add_(dz4, alpha=eta)\n",
    "\n",
    "        # early stop (cheap criterion)\n",
    "        # (avoid .item() too often; but once per iter is usually fine)\n",
    "        upd = (dx1.abs().mean() + dx2.abs().mean() + dx3.abs().mean() + dx4.abs().mean()).item()\n",
    "        if upd < tol:\n",
    "            break\n",
    "\n",
    "    # keep warm-start buffers for next batch\n",
    "    if state is not None and warm_start:\n",
    "        state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4 = x1,z1,x2,z2,x3,z3,x4,z4\n",
    "\n",
    "    # -------------------------\n",
    "    # Final readout\n",
    "    # -------------------------\n",
    "    m1 = (x1 + z1) * 0.5; s1 = (x1 - z1)\n",
    "    m2 = (x2 + z2) * 0.5; s2 = (x2 - z2)\n",
    "    m3 = (x3 + z3) * 0.5; s3 = (x3 - z3)\n",
    "    m4 = (x4 + z4) * 0.5; s4 = (x4 - z4)\n",
    "\n",
    "    (u1, _), (u2, _), (u3, _), (u4, _) = forward_u_sig(net, x0, m1, m2, m3)\n",
    "\n",
    "    delta1 = relu_prime(u1) * s1\n",
    "    delta2 = relu_prime(u2) * s2\n",
    "    delta3 = relu_prime(u3) * s3\n",
    "    delta4 = s4\n",
    "\n",
    "    # -------------------------\n",
    "    # Faster weight grads (no unfold)\n",
    "    # -------------------------\n",
    "    # conv2d_weight computes sum over batch; divide by B for mean like before\n",
    "    dW1 = conv2d_weight(x0, net.W1.shape, delta1, stride=1, padding=1) / B\n",
    "    dW2 = conv2d_weight(m1, net.W2.shape, delta2, stride=1, padding=1) / B\n",
    "    dW3 = conv2d_weight(m2, net.W3.shape, delta3, stride=2, padding=1) / B\n",
    "\n",
    "    db1 = delta1.mean(dim=(0,2,3))\n",
    "    db2 = delta2.mean(dim=(0,2,3))\n",
    "    db3 = delta3.mean(dim=(0,2,3))\n",
    "\n",
    "    m3_flat = m3.reshape(B, -1)\n",
    "    dW4 = (delta4.t() @ m3_flat) / B\n",
    "    db4 = delta4.mean(dim=0)\n",
    "\n",
    "    ce = F.cross_entropy(m4, y).item()\n",
    "\n",
    "    # optionally return steps_taken so you can see average K used\n",
    "    return (dW1,dW2,dW3,dW4), (db1,db2,db3,db4), ce, steps_taken\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SGD + momentum + wd + clip + EMA\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def sgd_momentum_step(net: CNN3, gradsW, gradsb, vW, vb,\n",
    "                      lr=0.01, momentum=0.9, weight_decay=5e-4, clip=1.0):\n",
    "    dW1,dW2,dW3,dW4 = gradsW\n",
    "    db1,db2,db3,db4 = gradsb\n",
    "\n",
    "    # weight decay on weights only\n",
    "    dW1 = dW1 + weight_decay * net.W1\n",
    "    dW2 = dW2 + weight_decay * net.W2\n",
    "    dW3 = dW3 + weight_decay * net.W3\n",
    "    dW4 = dW4 + weight_decay * net.W4\n",
    "\n",
    "    # global norm clip\n",
    "    gn2 = float(dW1.norm()**2 + dW2.norm()**2 + dW3.norm()**2 + dW4.norm()**2 +\n",
    "                db1.norm()**2 + db2.norm()**2 + db3.norm()**2 + db4.norm()**2)\n",
    "    gn = gn2**0.5\n",
    "    scale = 1.0 if gn <= clip else (clip / (gn + 1e-12))\n",
    "    if scale != 1.0:\n",
    "        dW1,dW2,dW3,dW4 = dW1*scale, dW2*scale, dW3*scale, dW4*scale\n",
    "        db1,db2,db3,db4 = db1*scale, db2*scale, db3*scale, db4*scale\n",
    "\n",
    "    # momentum buffers\n",
    "    vW[0].mul_(momentum).add_(dW1); vb[0].mul_(momentum).add_(db1)\n",
    "    vW[1].mul_(momentum).add_(dW2); vb[1].mul_(momentum).add_(db2)\n",
    "    vW[2].mul_(momentum).add_(dW3); vb[2].mul_(momentum).add_(db3)\n",
    "    vW[3].mul_(momentum).add_(dW4); vb[3].mul_(momentum).add_(db4)\n",
    "\n",
    "    net.W1.sub_(lr * vW[0]); net.b1.sub_(lr * vb[0])\n",
    "    net.W2.sub_(lr * vW[1]); net.b2.sub_(lr * vb[1])\n",
    "    net.W3.sub_(lr * vW[2]); net.b3.sub_(lr * vb[2])\n",
    "    net.W4.sub_(lr * vW[3]); net.b4.sub_(lr * vb[3])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(ema_net: CNN3, net: CNN3, decay=0.999):\n",
    "    for name in [\"W1\",\"b1\",\"W2\",\"b2\",\"W3\",\"b3\",\"W4\",\"b4\"]:\n",
    "        a = getattr(ema_net, name)\n",
    "        b = getattr(net, name)\n",
    "        a.mul_(decay).add_(b, alpha=(1.0 - decay))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(net: CNN3, loader, device, max_batches=800):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x,y) in enumerate(loader):\n",
    "        if i >= max_batches: break\n",
    "        x = x.to(device); y = y.to(device)\n",
    "\n",
    "        h1 = relu(F.conv2d(x, net.W1, net.b1, stride=1, padding=1))\n",
    "        h2 = relu(F.conv2d(h1, net.W2, net.b2, stride=1, padding=1))\n",
    "        h3 = relu(F.conv2d(h2, net.W3, net.b3, stride=2, padding=1))\n",
    "        logits = h3.reshape(x.size(0), -1) @ net.W4.t() + net.b4\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "\n",
    "def cosine_lr(step, total_steps, lr_max=0.02, lr_min=2e-4):\n",
    "    t = step / max(1, total_steps)\n",
    "    return lr_min + 0.5*(lr_max - lr_min)*(1.0 + math.cos(math.pi * t))\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    # (optional but good for speed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    use_aug = True\n",
    "    if use_aug:\n",
    "        train_tfm = transforms.Compose([\n",
    "            transforms.RandomAffine(degrees=10, translate=(0.1,0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    else:\n",
    "        train_tfm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    test_tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_ds = datasets.MNIST(\"./data\", train=True, download=True, transform=train_tfm)\n",
    "    test_ds  = datasets.MNIST(\"./data\", train=False, download=True, transform=test_tfm)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # model init\n",
    "    torch.manual_seed(0)\n",
    "    init = 0.02\n",
    "    W1 = init * torch.randn(64, 1, 3, 3, device=device);   b1 = torch.zeros(64, device=device)\n",
    "    W2 = init * torch.randn(64, 64, 3, 3, device=device);  b2 = torch.zeros(64, device=device)\n",
    "    W3 = init * torch.randn(128, 64, 3, 3, device=device); b3 = torch.zeros(128, device=device)\n",
    "    W4 = init * torch.randn(10, 128*14*14, device=device); b4 = torch.zeros(10, device=device)\n",
    "\n",
    "    net = CNN3(W1,b1,W2,b2,W3,b3,W4,b4)\n",
    "    ema_net = CNN3(W1.clone(),b1.clone(),W2.clone(),b2.clone(),W3.clone(),b3.clone(),W4.clone(),b4.clone())\n",
    "\n",
    "    vW = [torch.zeros_like(net.W1), torch.zeros_like(net.W2), torch.zeros_like(net.W3), torch.zeros_like(net.W4)]\n",
    "    vb = [torch.zeros_like(net.b1), torch.zeros_like(net.b2), torch.zeros_like(net.b3), torch.zeros_like(net.b4)]\n",
    "\n",
    "    # hyperparams\n",
    "    epochs = 12\n",
    "    eta = 1\n",
    "    K = 30\n",
    "    tol = 1e-9        # <-- NEW: early-stop tolerance\n",
    "    momentum = 0.9\n",
    "    weight_decay = 5e-4\n",
    "    clip = 1.0\n",
    "    ema_decay = 0.999\n",
    "\n",
    "    total_steps = epochs * len(train_loader)\n",
    "    global_step = 0\n",
    "    eval_every = 400\n",
    "\n",
    "    ce_hist, acc_hist, step_hist = [], [], []\n",
    "\n",
    "    # -------------------------\n",
    "    # NEW: persistent warm-start buffers\n",
    "    # -------------------------\n",
    "    state = XZState()\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        running = 0.0\n",
    "        steps_sum = 0\n",
    "        steps_count = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            lr = cosine_lr(global_step, total_steps, lr_max=0.02, lr_min=2e-4)\n",
    "\n",
    "            # -------------------------\n",
    "            # CHANGED CALL: now returns ce + steps_taken\n",
    "            # -------------------------\n",
    "            gradsW, gradsb, ce, steps_taken = xz_relax_batch_grad(\n",
    "                net, x, y,\n",
    "                eta=eta, K=K,\n",
    "                state=state,\n",
    "                tol=tol,\n",
    "                warm_start=True\n",
    "            )\n",
    "\n",
    "            sgd_momentum_step(net, gradsW, gradsb, vW, vb,\n",
    "                              lr=lr, momentum=momentum, weight_decay=weight_decay, clip=clip)\n",
    "\n",
    "            ema_update(ema_net, net, decay=ema_decay)\n",
    "\n",
    "            global_step += 1\n",
    "            running += ce\n",
    "            ce_hist.append(ce)\n",
    "            step_hist.append(global_step)\n",
    "\n",
    "            steps_sum += steps_taken\n",
    "            steps_count += 1\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                acc = accuracy(ema_net, test_loader, device=device, max_batches=800)\n",
    "                acc_hist.append(acc)\n",
    "\n",
    "                avgK = steps_sum / max(1, steps_count)\n",
    "                print(f\"step {global_step}: lr={lr:.4g}  train-CE~{running/eval_every:.4f}  \"\n",
    "                      f\"EMA acc={acc*100:.2f}%  avg_relax_steps={avgK:.1f}\")\n",
    "                running = 0.0\n",
    "                steps_sum = 0\n",
    "                steps_count = 0\n",
    "\n",
    "        acc = accuracy(ema_net, test_loader, device=device, max_batches=800)\n",
    "        print(f\"END epoch {ep:02d}: EMA test-acc~{acc*100:.2f}%\")\n",
    "\n",
    "    final_acc = accuracy(ema_net, test_loader, device=device, max_batches=1200)\n",
    "    print(f\"Final EMA test-acc (approx): {final_acc*100:.2f}%\")\n",
    "\n",
    "    # plots\n",
    "    plt.figure()\n",
    "    win = 200\n",
    "    if len(ce_hist) >= win:\n",
    "        sm = [sum(ce_hist[i-win:i])/win for i in range(win, len(ce_hist)+1)]\n",
    "        plt.plot(step_hist[win-1:], sm)\n",
    "    else:\n",
    "        plt.plot(step_hist, ce_hist)\n",
    "    plt.xlabel(\"Update step\")\n",
    "    plt.ylabel(\"Train CE (moving avg)\")\n",
    "    plt.title(\"3-Conv CNN + x-z relaxation: training loss\")\n",
    "    plt.grid(True, linestyle=\":\")\n",
    "\n",
    "    plt.figure()\n",
    "    eval_steps = [eval_every*(i+1) for i in range(len(acc_hist))]\n",
    "    plt.plot(eval_steps, [100*a for a in acc_hist], marker=\"o\")\n",
    "    plt.xlabel(\"Update step\")\n",
    "    plt.ylabel(\"Test accuracy (%)\")\n",
    "    plt.title(\"3-Conv CNN + x-z relaxation: EMA test accuracy\")\n",
    "    plt.grid(True, linestyle=\":\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e0a3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e55cd44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "step 400: lr=0.01976  train-CE~0.8612  EMA acc=79.21%\n",
      "END epoch 01: raw acc~95.84%  EMA acc~82.10%\n",
      "step 800: lr=0.01903  train-CE~0.2244  EMA acc=93.29%\n",
      "END epoch 02: raw acc~97.30%  EMA acc~95.06%\n",
      "step 1200: lr=0.01786  train-CE~0.1341  EMA acc=96.85%\n",
      "END epoch 03: raw acc~97.14%  EMA acc~97.47%\n",
      "step 1600: lr=0.01631  train-CE~0.0808  EMA acc=97.97%\n",
      "END epoch 04: raw acc~98.53%  EMA acc~98.36%\n",
      "step 2000: lr=0.01445  train-CE~0.0450  EMA acc=98.47%\n",
      "END epoch 05: raw acc~98.88%  EMA acc~98.67%\n",
      "step 2400: lr=0.01237  train-CE~0.0170  EMA acc=98.68%\n",
      "step 2800: lr=0.01018  train-CE~0.1119  EMA acc=98.84%\n",
      "END epoch 06: raw acc~98.78%  EMA acc~98.87%\n",
      "step 3200: lr=0.007989  train-CE~0.0970  EMA acc=98.97%\n",
      "END epoch 07: raw acc~98.99%  EMA acc~99.03%\n",
      "step 3600: lr=0.005899  train-CE~0.0713  EMA acc=99.03%\n",
      "END epoch 08: raw acc~99.00%  EMA acc~99.05%\n",
      "step 4000: lr=0.004019  train-CE~0.0518  EMA acc=99.08%\n",
      "END epoch 09: raw acc~98.98%  EMA acc~99.10%\n",
      "step 4400: lr=0.00244  train-CE~0.0338  EMA acc=99.12%\n",
      "END epoch 10: raw acc~99.11%  EMA acc~99.11%\n",
      "step 4800: lr=0.001241  train-CE~0.0188  EMA acc=99.10%\n",
      "END epoch 11: raw acc~99.14%  EMA acc~99.15%\n",
      "step 5200: lr=0.0004825  train-CE~0.0069  EMA acc=99.15%\n",
      "step 5600: lr=0.0002013  train-CE~0.0690  EMA acc=99.14%\n",
      "END epoch 12: raw acc~99.18%  EMA acc~99.14%\n",
      "Final test acc: raw=99.18%  EMA=99.14%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Model (same architecture as your relaxation CNN3)\n",
    "# -----------------------------\n",
    "class CNN3BP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)  # 28->14\n",
    "        self.fc    = nn.Linear(128 * 14 * 14, 10)\n",
    "\n",
    "        # match your init scale roughly\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)  # logits\n",
    "        return x\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# EMA helper\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def ema_update(ema_model: nn.Module, model: nn.Module, decay: float = 0.999):\n",
    "    msd = model.state_dict()\n",
    "    esd = ema_model.state_dict()\n",
    "    for k in esd.keys():\n",
    "        esd[k].mul_(decay).add_(msd[k], alpha=(1.0 - decay))\n",
    "    ema_model.load_state_dict(esd)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(model: nn.Module, loader: DataLoader, device, max_batches=1200) -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "\n",
    "def cosine_lr(step, total_steps, lr_max=0.02, lr_min=2e-4):\n",
    "    t = step / max(1, total_steps)\n",
    "    return lr_min + 0.5 * (lr_max - lr_min) * (1.0 + math.cos(math.pi * t))\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    # ---- data (match your setup) ----\n",
    "    use_aug = True\n",
    "    if use_aug:\n",
    "        train_tfm = transforms.Compose([\n",
    "            transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    else:\n",
    "        train_tfm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    test_tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_ds = datasets.MNIST(\"./data\", train=True, download=True, transform=train_tfm)\n",
    "    test_ds  = datasets.MNIST(\"./data\", train=False, download=True, transform=test_tfm)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # ---- model ----\n",
    "    torch.manual_seed(0)\n",
    "    model = CNN3BP().to(device)\n",
    "    ema_model = CNN3BP().to(device)\n",
    "    ema_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    # ---- optimizer ----\n",
    "    # match your \"momentum + weight_decay\"\n",
    "    momentum = 0.9\n",
    "    weight_decay = 5e-4\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.02, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # ---- training ----\n",
    "    epochs = 12\n",
    "    total_steps = epochs * len(train_loader)\n",
    "    global_step = 0\n",
    "    eval_every = 400\n",
    "    ema_decay = 0.999\n",
    "\n",
    "    ce_hist, acc_hist, step_hist = [], [], []\n",
    "\n",
    "    model.train()\n",
    "    for ep in range(1, epochs + 1):\n",
    "        running = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            lr = cosine_lr(global_step, total_steps, lr_max=0.02, lr_min=2e-4)\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg[\"lr\"] = lr\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            loss.backward()\n",
    "\n",
    "            # optional: clip grad norm for fairness (your method uses clip=1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            ema_update(ema_model, model, decay=ema_decay)\n",
    "\n",
    "            global_step += 1\n",
    "            running += loss.item()\n",
    "            ce_hist.append(loss.item())\n",
    "            step_hist.append(global_step)\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                acc = accuracy(ema_model, test_loader, device=device, max_batches=800)\n",
    "                acc_hist.append(acc)\n",
    "                print(f\"step {global_step}: lr={lr:.4g}  train-CE~{running/eval_every:.4f}  EMA acc={acc*100:.2f}%\")\n",
    "                running = 0.0\n",
    "\n",
    "        acc_raw = accuracy(model, test_loader, device=device, max_batches=800)\n",
    "        acc_ema = accuracy(ema_model, test_loader, device=device, max_batches=800)\n",
    "        print(f\"END epoch {ep:02d}: raw acc~{acc_raw*100:.2f}%  EMA acc~{acc_ema*100:.2f}%\")\n",
    "\n",
    "    final_raw = accuracy(model, test_loader, device=device, max_batches=1200)\n",
    "    final_ema = accuracy(ema_model, test_loader, device=device, max_batches=1200)\n",
    "    print(f\"Final test acc: raw={final_raw*100:.2f}%  EMA={final_ema*100:.2f}%\")\n",
    "\n",
    "    # ---- plots (same as your relaxation script) ----\n",
    "    plt.figure()\n",
    "    win = 200\n",
    "    if len(ce_hist) >= win:\n",
    "        sm = [sum(ce_hist[i-win:i])/win for i in range(win, len(ce_hist)+1)]\n",
    "        plt.plot(step_hist[win-1:], sm)\n",
    "    else:\n",
    "        plt.plot(step_hist, ce_hist)\n",
    "    plt.xlabel(\"Update step\")\n",
    "    plt.ylabel(\"Train CE (moving avg)\")\n",
    "    plt.title(\"Backprop baseline: training loss\")\n",
    "    plt.grid(True, linestyle=\":\")\n",
    "\n",
    "    plt.figure()\n",
    "    eval_steps = [eval_every*(i+1) for i in range(len(acc_hist))]\n",
    "    plt.plot(eval_steps, [100*a for a in acc_hist], marker=\"o\")\n",
    "    plt.xlabel(\"Update step\")\n",
    "    plt.ylabel(\"Test accuracy (%)\")\n",
    "    plt.title(\"Backprop baseline: EMA test accuracy\")\n",
    "    plt.grid(True, linestyle=\":\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f473d4",
   "metadata": {},
   "source": [
    "GOOD SPED UP NORMAL DYNAMICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61bf8ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "\n",
      "============================================================\n",
      "Training with eta = 1.0\n",
      "============================================================\n",
      "\n",
      "[eta=1.0 grad-compare step 0] cos(global W)=1.000000  normR(global W)=1.000000  relErr(global W)=0.000017\n",
      "[eta=1.0 grad-compare step 200] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000020\n",
      "[eta=1.0] step 400: lr=0.01787  train-CE~0.8312  EMA acc=77.91%  avg_relax_steps=14.2\n",
      "[eta=1.0 grad-compare step 400] cos(global W)=1.000000  normR(global W)=1.000005  relErr(global W)=0.000026\n",
      "[eta=1.0] END epoch 01: EMA test-acc~82.93%\n",
      "[eta=1.0 grad-compare step 600] cos(global W)=1.000000  normR(global W)=1.000004  relErr(global W)=0.000027\n",
      "[eta=1.0] step 800: lr=0.01238  train-CE~0.2186  EMA acc=93.67%  avg_relax_steps=19.0\n",
      "[eta=1.0 grad-compare step 800] cos(global W)=1.000000  normR(global W)=1.000003  relErr(global W)=0.000027\n",
      "[eta=1.0] END epoch 02: EMA test-acc~95.41%\n",
      "[eta=1.0 grad-compare step 1000] cos(global W)=1.000000  normR(global W)=1.000004  relErr(global W)=0.000023\n",
      "[eta=1.0] step 1200: lr=0.005909  train-CE~0.1263  EMA acc=96.96%  avg_relax_steps=19.7\n",
      "[eta=1.0 grad-compare step 1200] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000022\n",
      "[eta=1.0 grad-compare step 1400] cos(global W)=1.000000  normR(global W)=1.000003  relErr(global W)=0.000029\n",
      "[eta=1.0] END epoch 03: EMA test-acc~97.52%\n",
      "[eta=1.0] step 1600: lr=0.001246  train-CE~0.0723  EMA acc=97.89%  avg_relax_steps=25.0\n",
      "[eta=1.0 grad-compare step 1600] cos(global W)=1.000000  normR(global W)=1.000000  relErr(global W)=0.000027\n",
      "[eta=1.0 grad-compare step 1800] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000028\n",
      "[eta=1.0] END epoch 04: EMA test-acc~98.11%\n",
      "[eta=1.0] Final EMA test-acc (approx): 98.11%\n",
      "\n",
      "Generating ICML publication-quality plot...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from dataclasses import dataclass\n",
    "from torch.nn.grad import conv2d_weight\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Activations\n",
    "# -----------------------------\n",
    "def relu(u): return torch.relu(u)\n",
    "def relu_prime(u): return (u > 0).to(u.dtype)\n",
    "\n",
    "# -----------------------------\n",
    "# Model container (3 conv + fc)\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class CNN3:\n",
    "    W1: torch.Tensor; b1: torch.Tensor   # (64,1,3,3)\n",
    "    W2: torch.Tensor; b2: torch.Tensor   # (64,64,3,3)\n",
    "    W3: torch.Tensor; b3: torch.Tensor   # (128,64,3,3) stride 2\n",
    "    W4: torch.Tensor; b4: torch.Tensor   # (10, 128*14*14)\n",
    "\n",
    "    @property\n",
    "    def device(self): return self.W1.device\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Forward at mean states\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def forward_u_sig(net: CNN3, x0, m1, m2, m3):\n",
    "    u1 = F.conv2d(x0, net.W1, net.b1, stride=1, padding=1)  # (B,64,28,28)\n",
    "    sig1 = relu(u1)\n",
    "\n",
    "    u2 = F.conv2d(m1, net.W2, net.b2, stride=1, padding=1)  # (B,64,28,28)\n",
    "    sig2 = relu(u2)\n",
    "\n",
    "    u3 = F.conv2d(m2, net.W3, net.b3, stride=2, padding=1)  # (B,128,14,14)\n",
    "    sig3 = relu(u3)\n",
    "\n",
    "    B = x0.shape[0]\n",
    "    u4 = m3.reshape(B, -1) @ net.W4.t() + net.b4            # (B,10)\n",
    "    sig4 = u4  # linear logits\n",
    "    return (u1, sig1), (u2, sig2), (u3, sig3), (u4, sig4)\n",
    "\n",
    "class XZState:\n",
    "    def __init__(self):\n",
    "        self.x1 = None; self.z1 = None\n",
    "        self.x2 = None; self.z2 = None\n",
    "        self.x3 = None; self.z3 = None\n",
    "        self.x4 = None; self.z4 = None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Hamiltonian x-z relaxation gradient for batch (CNN)\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def xz_relax_batch_grad(\n",
    "    net: CNN3, x0, y,\n",
    "    eta=1.0, K=30,\n",
    "    state: XZState | None = None,\n",
    "    tol: float = 1e-4,\n",
    "    warm_start: bool = True,\n",
    "    beta: float = 1.0,\n",
    "):\n",
    "    device = net.device\n",
    "    B = x0.shape[0]\n",
    "    y_onehot = F.one_hot(y, num_classes=10).to(x0.dtype)\n",
    "\n",
    "    def alloc():\n",
    "        x1 = torch.zeros(B, 64, 28, 28, device=device); z1 = torch.zeros_like(x1)\n",
    "        x2 = torch.zeros(B, 64, 28, 28, device=device); z2 = torch.zeros_like(x2)\n",
    "        x3 = torch.zeros(B, 128, 14, 14, device=device); z3 = torch.zeros_like(x3)\n",
    "        x4 = torch.zeros(B, 10, device=device);         z4 = torch.zeros_like(x4)\n",
    "        return x1,z1,x2,z2,x3,z3,x4,z4\n",
    "\n",
    "    if (state is None) or (not warm_start) or (state.x1 is None) or (state.x1.shape[0] != B):\n",
    "        x1,z1,x2,z2,x3,z3,x4,z4 = alloc()\n",
    "        if state is not None:\n",
    "            state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4 = x1,z1,x2,z2,x3,z3,x4,z4\n",
    "    else:\n",
    "        x1,z1,x2,z2,x3,z3,x4,z4 = state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4\n",
    "\n",
    "    steps_taken = 0\n",
    "    for _ in range(K):\n",
    "        steps_taken += 1\n",
    "\n",
    "        m1 = (x1 + z1) * 0.5; s1 = (x1 - z1)\n",
    "        m2 = (x2 + z2) * 0.5; s2 = (x2 - z2)\n",
    "        m3 = (x3 + z3) * 0.5; s3 = (x3 - z3)\n",
    "        m4 = (x4 + z4) * 0.5; s4 = (x4 - z4)\n",
    "\n",
    "        (u1, sig1), (u2, sig2), (u3, sig3), (u4, sig4) = forward_u_sig(net, x0, m1, m2, m3)\n",
    "\n",
    "        F1 = sig1 - m1\n",
    "        F2 = sig2 - m2\n",
    "        F3 = sig3 - m3\n",
    "        F4 = sig4 - m4\n",
    "\n",
    "        p = torch.softmax(m4, dim=1)\n",
    "        g4 = (p - y_onehot)\n",
    "\n",
    "        q2 = relu_prime(u2) * s2\n",
    "        q3 = relu_prime(u3) * s3\n",
    "        q4 = s4\n",
    "\n",
    "        WTq3 = (q4 @ net.W4).reshape(B, 128, 14, 14)\n",
    "        WTq2 = F.conv_transpose2d(q3, net.W3, bias=None, stride=2, padding=1, output_padding=1)\n",
    "        WTq1 = F.conv_transpose2d(q2, net.W2, bias=None, stride=1, padding=1)\n",
    "\n",
    "        Jt1 = -s1 + WTq1\n",
    "        Jt2 = -s2 + WTq2\n",
    "        Jt3 = -s3 + WTq3\n",
    "        Jt4 = -s4\n",
    "\n",
    "        dx1 = F1 + 0.5 * Jt1\n",
    "        dz1 = F1 - 0.5 * Jt1\n",
    "\n",
    "        dx2 = F2 + 0.5 * Jt2\n",
    "        dz2 = F2 - 0.5 * Jt2\n",
    "\n",
    "        dx3 = F3 + 0.5 * Jt3\n",
    "        dz3 = F3 - 0.5 * Jt3\n",
    "\n",
    "        dx4 = F4 + 0.5 * Jt4 + 0.5 * beta * g4\n",
    "        dz4 = F4 - 0.5 * Jt4 - 0.5 * beta * g4\n",
    "\n",
    "        x1.add_(dx1, alpha=eta); z1.add_(dz1, alpha=eta)\n",
    "        x2.add_(dx2, alpha=eta); z2.add_(dz2, alpha=eta)\n",
    "        x3.add_(dx3, alpha=eta); z3.add_(dz3, alpha=eta)\n",
    "        x4.add_(dx4, alpha=eta); z4.add_(dz4, alpha=eta)\n",
    "\n",
    "        upd = (dx1.abs().mean() + dx2.abs().mean() + dx3.abs().mean() + dx4.abs().mean()).item()\n",
    "        if upd < tol:\n",
    "            break\n",
    "\n",
    "    if state is not None and warm_start:\n",
    "        state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4 = x1,z1,x2,z2,x3,z3,x4,z4\n",
    "\n",
    "    m1 = (x1 + z1) * 0.5; s1 = (x1 - z1)\n",
    "    m2 = (x2 + z2) * 0.5; s2 = (x2 - z2)\n",
    "    m3 = (x3 + z3) * 0.5; s3 = (x3 - z3)\n",
    "    m4 = (x4 + z4) * 0.5; s4 = (x4 - z4)\n",
    "\n",
    "    (u1, _), (u2, _), (u3, _), (_, _) = forward_u_sig(net, x0, m1, m2, m3)\n",
    "\n",
    "    delta1 = relu_prime(u1) * s1\n",
    "    delta2 = relu_prime(u2) * s2\n",
    "    delta3 = relu_prime(u3) * s3\n",
    "    delta4 = s4\n",
    "\n",
    "    dW1 = conv2d_weight(x0, net.W1.shape, delta1, stride=1, padding=1) / B\n",
    "    dW2 = conv2d_weight(m1, net.W2.shape, delta2, stride=1, padding=1) / B\n",
    "    dW3 = conv2d_weight(m2, net.W3.shape, delta3, stride=2, padding=1) / B\n",
    "\n",
    "    # IMPORTANT FIX: match PyTorch bias grad normalization (sum spatial, avg batch)\n",
    "    db1 = delta1.sum(dim=(0,2,3)) / B\n",
    "    db2 = delta2.sum(dim=(0,2,3)) / B\n",
    "    db3 = delta3.sum(dim=(0,2,3)) / B\n",
    "\n",
    "    m3_flat = m3.reshape(B, -1)\n",
    "    dW4 = (delta4.t() @ m3_flat) / B\n",
    "    db4 = delta4.mean(dim=0)\n",
    "\n",
    "    ce = F.cross_entropy(m4, y).item()\n",
    "    return (dW1,dW2,dW3,dW4), (db1,db2,db3,db4), ce, steps_taken\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SGD + momentum + wd + clip + EMA\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def sgd_momentum_step(net: CNN3, gradsW, gradsb, vW, vb,\n",
    "                      lr=0.01, momentum=0.9, weight_decay=5e-4, clip=1.0):\n",
    "    dW1,dW2,dW3,dW4 = gradsW\n",
    "    db1,db2,db3,db4 = gradsb\n",
    "\n",
    "    dW1 = dW1 + weight_decay * net.W1\n",
    "    dW2 = dW2 + weight_decay * net.W2\n",
    "    dW3 = dW3 + weight_decay * net.W3\n",
    "    dW4 = dW4 + weight_decay * net.W4\n",
    "\n",
    "    gn2 = float(dW1.norm()**2 + dW2.norm()**2 + dW3.norm()**2 + dW4.norm()**2 +\n",
    "                db1.norm()**2 + db2.norm()**2 + db3.norm()**2 + db4.norm()**2)\n",
    "    gn = gn2**0.5\n",
    "    scale = 1.0 if gn <= clip else (clip / (gn + 1e-12))\n",
    "    if scale != 1.0:\n",
    "        dW1,dW2,dW3,dW4 = dW1*scale, dW2*scale, dW3*scale, dW4*scale\n",
    "        db1,db2,db3,db4 = db1*scale, db2*scale, db3*scale, db4*scale\n",
    "\n",
    "    vW[0].mul_(momentum).add_(dW1); vb[0].mul_(momentum).add_(db1)\n",
    "    vW[1].mul_(momentum).add_(dW2); vb[1].mul_(momentum).add_(db2)\n",
    "    vW[2].mul_(momentum).add_(dW3); vb[2].mul_(momentum).add_(db3)\n",
    "    vW[3].mul_(momentum).add_(dW4); vb[3].mul_(momentum).add_(db4)\n",
    "\n",
    "    net.W1.sub_(lr * vW[0]); net.b1.sub_(lr * vb[0])\n",
    "    net.W2.sub_(lr * vW[1]); net.b2.sub_(lr * vb[1])\n",
    "    net.W3.sub_(lr * vW[2]); net.b3.sub_(lr * vb[2])\n",
    "    net.W4.sub_(lr * vW[3]); net.b4.sub_(lr * vb[3])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(ema_net: CNN3, net: CNN3, decay=0.999):\n",
    "    for name in [\"W1\",\"b1\",\"W2\",\"b2\",\"W3\",\"b3\",\"W4\",\"b4\"]:\n",
    "        a = getattr(ema_net, name)\n",
    "        b = getattr(net, name)\n",
    "        a.mul_(decay).add_(b, alpha=(1.0 - decay))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(net: CNN3, loader, device, max_batches=800):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x,y) in enumerate(loader):\n",
    "        if i >= max_batches: break\n",
    "        x = x.to(device); y = y.to(device)\n",
    "\n",
    "        h1 = relu(F.conv2d(x, net.W1, net.b1, stride=1, padding=1))\n",
    "        h2 = relu(F.conv2d(h1, net.W2, net.b2, stride=1, padding=1))\n",
    "        h3 = relu(F.conv2d(h2, net.W3, net.b3, stride=2, padding=1))\n",
    "        logits = h3.reshape(x.size(0), -1) @ net.W4.t() + net.b4\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "\n",
    "def cosine_lr(step, total_steps, lr_max=0.02, lr_min=2e-4):\n",
    "    t = step / max(1, total_steps)\n",
    "    return lr_min + 0.5*(lr_max - lr_min)*(1.0 + math.cos(math.pi * t))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Autograd gradient (reference)\n",
    "# -----------------------------\n",
    "def autograd_grads_like(net: CNN3, x, y):\n",
    "    # make differentiable copies (leaf tensors)\n",
    "    W1 = net.W1.detach().clone().requires_grad_(True)\n",
    "    b1 = net.b1.detach().clone().requires_grad_(True)\n",
    "    W2 = net.W2.detach().clone().requires_grad_(True)\n",
    "    b2 = net.b2.detach().clone().requires_grad_(True)\n",
    "    W3 = net.W3.detach().clone().requires_grad_(True)\n",
    "    b3 = net.b3.detach().clone().requires_grad_(True)\n",
    "    W4 = net.W4.detach().clone().requires_grad_(True)\n",
    "    b4 = net.b4.detach().clone().requires_grad_(True)\n",
    "\n",
    "    h1 = relu(F.conv2d(x, W1, b1, stride=1, padding=1))\n",
    "    h2 = relu(F.conv2d(h1, W2, b2, stride=1, padding=1))\n",
    "    h3 = relu(F.conv2d(h2, W3, b3, stride=2, padding=1))\n",
    "    logits = h3.reshape(x.size(0), -1) @ W4.t() + b4\n",
    "\n",
    "    loss = F.cross_entropy(logits, y)  # mean over batch\n",
    "    loss.backward()\n",
    "\n",
    "    gradsW = (W1.grad, W2.grad, W3.grad, W4.grad)\n",
    "    gradsb = (b1.grad, b2.grad, b3.grad, b4.grad)\n",
    "    return gradsW, gradsb, float(loss.detach())\n",
    "\n",
    "\n",
    "def flat_cat(tup):\n",
    "    return torch.cat([t.reshape(-1) for t in tup], dim=0)\n",
    "\n",
    "def cos_sim(a, b, eps=1e-12):\n",
    "    denom = (a.norm() * b.norm()).clamp_min(eps)\n",
    "    return float((a @ b) / denom)\n",
    "\n",
    "def norm_ratio(a, b, eps=1e-12):\n",
    "    return float(a.norm() / b.norm().clamp_min(eps))\n",
    "\n",
    "def relative_error(a, b, eps=1e-12):\n",
    "    \"\"\"Compute ||a - b|| / ||b||\"\"\"\n",
    "    return float((a - b).norm() / b.norm().clamp_min(eps))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PLOTTING FUNCTION FOR ICML (ADDED)\n",
    "# ---------------------------------------------------------\n",
    "def plot_results_icml(results, eta_values):\n",
    "    \"\"\"\n",
    "    Generates a publication-quality plot for ICML/NeurIPS.\n",
    "    Focus: Global Relative Error between Relaxed Gradients and Autograd.\n",
    "    \"\"\"\n",
    "    # Configure Matplotlib for LaTeX-like aesthetics\n",
    "    mpl.rcParams.update({\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times New Roman\", \"DejaVu Serif\"],\n",
    "        \"font.size\": 14,\n",
    "        \"axes.labelsize\": 16,\n",
    "        \"axes.titlesize\": 18,\n",
    "        \"xtick.labelsize\": 14,\n",
    "        \"ytick.labelsize\": 14,\n",
    "        \"legend.fontsize\": 14,\n",
    "        \"lines.linewidth\": 2.5,\n",
    "        \"lines.markersize\": 8,\n",
    "        \"figure.figsize\": (8, 6),\n",
    "        \"text.usetex\": False, # Set to True if you have local LaTeX installed\n",
    "    })\n",
    "\n",
    "    plt.figure()\n",
    "    \n",
    "    # Define styles for different eta values\n",
    "    styles = {\n",
    "        0.8: {'color': '#d62728', 'marker': 'o', 'linestyle': '-'},\n",
    "        1.0: {'color': '#1f77b4', 'marker': 's', 'linestyle': '--'},\n",
    "        # Add more if needed\n",
    "    }\n",
    "\n",
    "    for eta in eta_values:\n",
    "        if eta not in results: continue\n",
    "        \n",
    "        data = results[eta]\n",
    "        cos_steps = data['cos_steps']\n",
    "        rel_err = data['relerr_globalW_hist']\n",
    "        \n",
    "        if len(cos_steps) == 0: continue\n",
    "\n",
    "        style = styles.get(eta, {'color': 'black', 'marker': 'x', 'linestyle': ':'})\n",
    "        \n",
    "        plt.plot(cos_steps, rel_err, \n",
    "                 label=fr'$\\eta={eta}$', \n",
    "                 color=style['color'], \n",
    "                 marker=style['marker'], \n",
    "                 linestyle=style['linestyle'])\n",
    "\n",
    "    # Log scale is crucial for relative error plots\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.xlabel('Training Steps')\n",
    "    # Formal mathematical label\n",
    "    plt.ylabel(r'Relative Gradient Error $\\frac{||\\nabla_{xz} - \\nabla_{BP}||}{||\\nabla_{BP}||}$')\n",
    "    plt.title('Gradient Fidelity throughout Training')\n",
    "    \n",
    "    plt.grid(True, which=\"both\", ls=\":\", alpha=0.6)\n",
    "    plt.legend(frameon=True, fancybox=False, edgecolor='k')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    use_aug = True\n",
    "    if use_aug:\n",
    "        train_tfm = transforms.Compose([\n",
    "            transforms.RandomAffine(degrees=10, translate=(0.1,0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    else:\n",
    "        train_tfm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    test_tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_ds = datasets.MNIST(\"./data\", train=True, download=True, transform=train_tfm)\n",
    "    test_ds  = datasets.MNIST(\"./data\", train=False, download=True, transform=test_tfm)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    epochs = 4\n",
    "    K = 30\n",
    "    tol = 1e-6\n",
    "    momentum = 0.9\n",
    "    weight_decay = 5e-4\n",
    "    clip = 1.0\n",
    "    ema_decay = 0.999\n",
    "\n",
    "    eval_every = 400\n",
    "    compare_every = 200\n",
    "\n",
    "    # -------------------------\n",
    "    # Test different eta values\n",
    "    # -------------------------\n",
    "    eta_values = [1.0]\n",
    "    layer_names = [\"W1\",\"b1\",\"W2\",\"b2\",\"W3\",\"b3\",\"W4\",\"b4\"]\n",
    "    \n",
    "    # Store results for each eta\n",
    "    results = {}\n",
    "    \n",
    "    for eta in eta_values:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training with eta = {eta}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Reset network for fair comparison\n",
    "        torch.manual_seed(0)\n",
    "        init = 0.02\n",
    "        W1 = init * torch.randn(64, 1, 3, 3, device=device);   b1 = torch.zeros(64, device=device)\n",
    "        W2 = init * torch.randn(64, 64, 3, 3, device=device);  b2 = torch.zeros(64, device=device)\n",
    "        W3 = init * torch.randn(128, 64, 3, 3, device=device); b3 = torch.zeros(128, device=device)\n",
    "        W4 = init * torch.randn(10, 128*14*14, device=device); b4 = torch.zeros(10, device=device)\n",
    "\n",
    "        net = CNN3(W1,b1,W2,b2,W3,b3,W4,b4)\n",
    "        ema_net = CNN3(W1.clone(),b1.clone(),W2.clone(),b2.clone(),W3.clone(),b3.clone(),W4.clone(),b4.clone())\n",
    "\n",
    "        vW = [torch.zeros_like(net.W1), torch.zeros_like(net.W2), torch.zeros_like(net.W3), torch.zeros_like(net.W4)]\n",
    "        vb = [torch.zeros_like(net.b1), torch.zeros_like(net.b2), torch.zeros_like(net.b3), torch.zeros_like(net.b4)]\n",
    "\n",
    "        total_steps = epochs * len(train_loader)\n",
    "        global_step = 0\n",
    "\n",
    "        ce_hist, acc_hist, step_hist = [], [], []\n",
    "\n",
    "        # Gradient comparison histories\n",
    "        cos_globalW_hist = []\n",
    "        cos_layer_hist = {name: [] for name in layer_names}\n",
    "        cos_steps = []\n",
    "\n",
    "        norm_globalW_hist = []\n",
    "        norm_layer_hist = {name: [] for name in layer_names}\n",
    "\n",
    "        relerr_globalW_hist = []\n",
    "        relerr_layer_hist = {name: [] for name in layer_names}\n",
    "\n",
    "        state = XZState()\n",
    "\n",
    "        for ep in range(1, epochs+1):\n",
    "            running = 0.0\n",
    "            steps_sum = 0\n",
    "            steps_count = 0\n",
    "\n",
    "            for x, y in train_loader:\n",
    "                x = x.to(device, non_blocking=True)\n",
    "                y = y.to(device, non_blocking=True)\n",
    "\n",
    "                lr = cosine_lr(global_step, total_steps, lr_max=0.02, lr_min=2e-4)\n",
    "\n",
    "                # xz gradients\n",
    "                gradsW_xz, gradsb_xz, ce, steps_taken = xz_relax_batch_grad(\n",
    "                    net, x, y,\n",
    "                    eta=eta, K=K,\n",
    "                    state=state,\n",
    "                    tol=tol,\n",
    "                    warm_start=True\n",
    "                )\n",
    "\n",
    "                # autograd gradients (reference) every compare_every\n",
    "                if global_step % compare_every == 0:\n",
    "                    gradsW_ag, gradsb_ag, ce_ag = autograd_grads_like(net, x, y)\n",
    "\n",
    "                    # global weights cosine + norm ratio + relative error (W only)\n",
    "                    gx = flat_cat(gradsW_xz)\n",
    "                    ga = flat_cat(gradsW_ag)\n",
    "\n",
    "                    cos_globalW_hist.append(cos_sim(gx, ga))\n",
    "                    norm_globalW_hist.append(norm_ratio(gx, ga))\n",
    "                    relerr_globalW_hist.append(relative_error(gx, ga))  # ADDED\n",
    "                    cos_steps.append(global_step)\n",
    "\n",
    "                    # per-layer + bias cosine + norm ratios + relative errors\n",
    "                    xz_all = {\n",
    "                        \"W1\": gradsW_xz[0], \"W2\": gradsW_xz[1], \"W3\": gradsW_xz[2], \"W4\": gradsW_xz[3],\n",
    "                        \"b1\": gradsb_xz[0], \"b2\": gradsb_xz[1], \"b3\": gradsb_xz[2], \"b4\": gradsb_xz[3],\n",
    "                    }\n",
    "                    ag_all = {\n",
    "                        \"W1\": gradsW_ag[0], \"W2\": gradsW_ag[1], \"W3\": gradsW_ag[2], \"W4\": gradsW_ag[3],\n",
    "                        \"b1\": gradsb_ag[0], \"b2\": gradsb_ag[1], \"b3\": gradsb_ag[2], \"b4\": gradsb_ag[3],\n",
    "                    }\n",
    "                    for name in layer_names:\n",
    "                        cos_layer_hist[name].append(cos_sim(xz_all[name].reshape(-1), ag_all[name].reshape(-1)))\n",
    "                        norm_layer_hist[name].append(norm_ratio(xz_all[name].reshape(-1), ag_all[name].reshape(-1)))\n",
    "                        relerr_layer_hist[name].append(relative_error(xz_all[name].reshape(-1), ag_all[name].reshape(-1)))  # ADDED\n",
    "\n",
    "                    print(\n",
    "                        f\"[eta={eta} grad-compare step {global_step}] \"\n",
    "                        f\"cos(global W)={cos_globalW_hist[-1]:.6f}  \"\n",
    "                        f\"normR(global W)={norm_globalW_hist[-1]:.6f}  \"\n",
    "                        f\"relErr(global W)={relerr_globalW_hist[-1]:.6f}\"  # ADDED\n",
    "                    )\n",
    "\n",
    "                # update\n",
    "                sgd_momentum_step(net, gradsW_xz, gradsb_xz, vW, vb,\n",
    "                                  lr=lr, momentum=momentum, weight_decay=weight_decay, clip=clip)\n",
    "\n",
    "                ema_update(ema_net, net, decay=ema_decay)\n",
    "\n",
    "                global_step += 1\n",
    "                running += ce\n",
    "                ce_hist.append(ce)\n",
    "                step_hist.append(global_step)\n",
    "\n",
    "                steps_sum += steps_taken\n",
    "                steps_count += 1\n",
    "\n",
    "                if global_step % eval_every == 0:\n",
    "                    acc = accuracy(ema_net, test_loader, device=device, max_batches=800)\n",
    "                    acc_hist.append(acc)\n",
    "\n",
    "                    avgK = steps_sum / max(1, steps_count)\n",
    "                    print(f\"[eta={eta}] step {global_step}: lr={lr:.4g}  train-CE~{running/eval_every:.4f}  \"\n",
    "                          f\"EMA acc={acc*100:.2f}%  avg_relax_steps={avgK:.1f}\")\n",
    "                    running = 0.0\n",
    "                    steps_sum = 0\n",
    "                    steps_count = 0\n",
    "\n",
    "            acc = accuracy(ema_net, test_loader, device=device, max_batches=800)\n",
    "            print(f\"[eta={eta}] END epoch {ep:02d}: EMA test-acc~{acc*100:.2f}%\")\n",
    "\n",
    "        final_acc = accuracy(ema_net, test_loader, device=device, max_batches=1200)\n",
    "        print(f\"[eta={eta}] Final EMA test-acc (approx): {final_acc*100:.2f}%\")\n",
    "        \n",
    "        # Store results for this eta\n",
    "        results[eta] = {\n",
    "            'ce_hist': ce_hist,\n",
    "            'acc_hist': acc_hist,\n",
    "            'step_hist': step_hist,\n",
    "            'cos_globalW_hist': cos_globalW_hist,\n",
    "            'cos_layer_hist': cos_layer_hist.copy(),\n",
    "            'cos_steps': cos_steps,\n",
    "            'norm_globalW_hist': norm_globalW_hist,\n",
    "            'norm_layer_hist': norm_layer_hist.copy(),\n",
    "            'relerr_globalW_hist': relerr_globalW_hist,\n",
    "            'relerr_layer_hist': relerr_layer_hist.copy(),\n",
    "            'final_acc': final_acc,\n",
    "        }\n",
    "\n",
    "    # -------------------------\n",
    "    # CALL THE ICML PLOTTING FUNCTION\n",
    "    # -------------------------\n",
    "    print(\"\\nGenerating ICML publication-quality plot...\")\n",
    "    plot_results_icml(results, eta_values)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "427a5c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "\n",
      "============================================================\n",
      "Training with eta = 0.8\n",
      "============================================================\n",
      "\n",
      "[eta=0.8 grad-compare step 0] cos(global W)=1.000000  normR(global W)=1.000000  relErr(global W)=0.000016\n",
      "[eta=0.8 grad-compare step 100] cos(global W)=1.000000  normR(global W)=0.999999  relErr(global W)=0.000028\n",
      "[eta=0.8] step 200: lr=0.09984  train-CE~1.8854  EMA acc=27.77%  avg_relax_steps=19.3\n",
      "[eta=0.8 grad-compare step 200] cos(global W)=1.000000  normR(global W)=0.999993  relErr(global W)=0.000056\n",
      "[eta=0.8 grad-compare step 300] cos(global W)=1.000000  normR(global W)=0.999990  relErr(global W)=0.000127\n",
      "[eta=0.8] END epoch 01: EMA test-acc~35.90%\n",
      "[eta=0.8] step 400: lr=0.09937  train-CE~0.0633  EMA acc=36.28%  avg_relax_steps=19.1\n",
      "[eta=0.8 grad-compare step 400] cos(global W)=1.000000  normR(global W)=1.000006  relErr(global W)=0.000060\n",
      "[eta=0.8 grad-compare step 500] cos(global W)=1.000000  normR(global W)=0.999996  relErr(global W)=0.000039\n",
      "[eta=0.8] step 600: lr=0.09857  train-CE~1.3263  EMA acc=43.90%  avg_relax_steps=19.2\n",
      "[eta=0.8 grad-compare step 600] cos(global W)=1.000000  normR(global W)=0.999996  relErr(global W)=0.000035\n",
      "[eta=0.8 grad-compare step 700] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000034\n",
      "[eta=0.8] END epoch 02: EMA test-acc~49.52%\n",
      "[eta=0.8] step 800: lr=0.09747  train-CE~0.1038  EMA acc=49.95%  avg_relax_steps=19.3\n",
      "[eta=0.8 grad-compare step 800] cos(global W)=1.000000  normR(global W)=0.999998  relErr(global W)=0.000032\n",
      "[eta=0.8 grad-compare step 900] cos(global W)=1.000000  normR(global W)=0.999979  relErr(global W)=0.000120\n",
      "[eta=0.8] step 1000: lr=0.09607  train-CE~1.1711  EMA acc=54.87%  avg_relax_steps=19.6\n",
      "[eta=0.8 grad-compare step 1000] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000021\n",
      "[eta=0.8 grad-compare step 1100] cos(global W)=1.000000  normR(global W)=1.000005  relErr(global W)=0.000032\n",
      "[eta=0.8] END epoch 03: EMA test-acc~58.72%\n",
      "[eta=0.8] step 1200: lr=0.09437  train-CE~0.1523  EMA acc=59.30%  avg_relax_steps=19.6\n",
      "[eta=0.8 grad-compare step 1200] cos(global W)=1.000000  normR(global W)=1.000005  relErr(global W)=0.000045\n",
      "[eta=0.8 grad-compare step 1300] cos(global W)=1.000000  normR(global W)=1.000003  relErr(global W)=0.000081\n",
      "[eta=0.8] step 1400: lr=0.09239  train-CE~1.0733  EMA acc=63.07%  avg_relax_steps=19.8\n",
      "[eta=0.8 grad-compare step 1400] cos(global W)=1.000000  normR(global W)=0.999999  relErr(global W)=0.000042\n",
      "[eta=0.8 grad-compare step 1500] cos(global W)=1.000000  normR(global W)=1.000004  relErr(global W)=0.000031\n",
      "[eta=0.8] END epoch 04: EMA test-acc~65.64%\n",
      "[eta=0.8] step 1600: lr=0.09013  train-CE~0.1856  EMA acc=65.72%  avg_relax_steps=19.6\n",
      "[eta=0.8 grad-compare step 1600] cos(global W)=1.000000  normR(global W)=0.999998  relErr(global W)=0.000032\n",
      "[eta=0.8 grad-compare step 1700] cos(global W)=1.000000  normR(global W)=1.000000  relErr(global W)=0.000024\n",
      "[eta=0.8] step 1800: lr=0.08763  train-CE~1.0107  EMA acc=67.21%  avg_relax_steps=19.8\n",
      "[eta=0.8 grad-compare step 1800] cos(global W)=1.000000  normR(global W)=1.000004  relErr(global W)=0.000071\n",
      "[eta=0.8 grad-compare step 1900] cos(global W)=1.000000  normR(global W)=1.000003  relErr(global W)=0.000036\n",
      "[eta=0.8] END epoch 05: EMA test-acc~68.46%\n",
      "[eta=0.8] step 2000: lr=0.08488  train-CE~0.2211  EMA acc=68.80%  avg_relax_steps=19.9\n",
      "[eta=0.8 grad-compare step 2000] cos(global W)=1.000000  normR(global W)=1.000004  relErr(global W)=0.000027\n",
      "[eta=0.8 grad-compare step 2100] cos(global W)=1.000000  normR(global W)=0.999996  relErr(global W)=0.000039\n",
      "[eta=0.8] step 2200: lr=0.08191  train-CE~0.9691  EMA acc=70.44%  avg_relax_steps=19.9\n",
      "[eta=0.8 grad-compare step 2200] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000022\n",
      "[eta=0.8 grad-compare step 2300] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000036\n",
      "[eta=0.8] END epoch 06: EMA test-acc~71.41%\n",
      "[eta=0.8] step 2400: lr=0.07874  train-CE~0.2528  EMA acc=71.53%  avg_relax_steps=19.9\n",
      "[eta=0.8 grad-compare step 2400] cos(global W)=1.000000  normR(global W)=1.000007  relErr(global W)=0.000040\n",
      "[eta=0.8 grad-compare step 2500] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000032\n",
      "[eta=0.8] step 2600: lr=0.07538  train-CE~0.8952  EMA acc=72.78%  avg_relax_steps=19.9\n",
      "[eta=0.8 grad-compare step 2600] cos(global W)=1.000000  normR(global W)=1.000004  relErr(global W)=0.000032\n",
      "[eta=0.8 grad-compare step 2700] cos(global W)=1.000000  normR(global W)=1.000000  relErr(global W)=0.000035\n",
      "[eta=0.8] END epoch 07: EMA test-acc~73.45%\n",
      "[eta=0.8] step 2800: lr=0.07187  train-CE~0.2726  EMA acc=73.77%  avg_relax_steps=19.9\n",
      "[eta=0.8 grad-compare step 2800] cos(global W)=1.000000  normR(global W)=1.000012  relErr(global W)=0.000047\n",
      "[eta=0.8 grad-compare step 2900] cos(global W)=1.000000  normR(global W)=1.000000  relErr(global W)=0.000070\n",
      "[eta=0.8] step 3000: lr=0.06821  train-CE~0.8621  EMA acc=74.81%  avg_relax_steps=20.0\n",
      "[eta=0.8 grad-compare step 3000] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000041\n",
      "[eta=0.8 grad-compare step 3100] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000035\n",
      "[eta=0.8] END epoch 08: EMA test-acc~75.17%\n",
      "[eta=0.8] step 3200: lr=0.06445  train-CE~0.2996  EMA acc=75.36%  avg_relax_steps=20.0\n",
      "[eta=0.8 grad-compare step 3200] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000030\n",
      "[eta=0.8 grad-compare step 3300] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000038\n",
      "[eta=0.8] step 3400: lr=0.06059  train-CE~0.8101  EMA acc=76.00%  avg_relax_steps=20.0\n",
      "[eta=0.8 grad-compare step 3400] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000030\n",
      "[eta=0.8 grad-compare step 3500] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000043\n",
      "[eta=0.8] END epoch 09: EMA test-acc~76.20%\n",
      "[eta=0.8] step 3600: lr=0.05667  train-CE~0.3142  EMA acc=76.53%  avg_relax_steps=20.0\n",
      "[eta=0.8 grad-compare step 3600] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000036\n",
      "[eta=0.8 grad-compare step 3700] cos(global W)=1.000000  normR(global W)=0.999997  relErr(global W)=0.000030\n",
      "[eta=0.8] step 3800: lr=0.05271  train-CE~0.7752  EMA acc=77.20%  avg_relax_steps=20.0\n",
      "[eta=0.8 grad-compare step 3800] cos(global W)=1.000000  normR(global W)=0.999999  relErr(global W)=0.000036\n",
      "[eta=0.8 grad-compare step 3900] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000033\n",
      "[eta=0.8] END epoch 10: EMA test-acc~77.38%\n",
      "[eta=0.8] step 4000: lr=0.04873  train-CE~0.3347  EMA acc=77.57%  avg_relax_steps=20.0\n",
      "[eta=0.8 grad-compare step 4000] cos(global W)=1.000000  normR(global W)=1.000003  relErr(global W)=0.000036\n",
      "[eta=0.8 grad-compare step 4100] cos(global W)=1.000000  normR(global W)=0.999999  relErr(global W)=0.000039\n",
      "[eta=0.8] step 4200: lr=0.04477  train-CE~0.7347  EMA acc=77.83%  avg_relax_steps=20.1\n",
      "[eta=0.8 grad-compare step 4200] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000028\n",
      "[eta=0.8 grad-compare step 4300] cos(global W)=1.000000  normR(global W)=1.000013  relErr(global W)=0.000209\n",
      "[eta=0.8] END epoch 11: EMA test-acc~78.06%\n",
      "[eta=0.8] step 4400: lr=0.04084  train-CE~0.3454  EMA acc=78.28%  avg_relax_steps=20.1\n",
      "[eta=0.8 grad-compare step 4400] cos(global W)=1.000000  normR(global W)=1.000003  relErr(global W)=0.000034\n",
      "[eta=0.8 grad-compare step 4500] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000037\n",
      "[eta=0.8] step 4600: lr=0.03697  train-CE~0.6997  EMA acc=78.60%  avg_relax_steps=20.0\n",
      "[eta=0.8 grad-compare step 4600] cos(global W)=1.000000  normR(global W)=1.000006  relErr(global W)=0.000036\n",
      "[eta=0.8] END epoch 12: EMA test-acc~78.84%\n",
      "[eta=0.8 grad-compare step 4700] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000033\n",
      "[eta=0.8] step 4800: lr=0.03319  train-CE~0.3534  EMA acc=79.01%  avg_relax_steps=20.0\n",
      "[eta=0.8 grad-compare step 4800] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000039\n",
      "[eta=0.8 grad-compare step 4900] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000022\n",
      "[eta=0.8] step 5000: lr=0.02953  train-CE~0.6605  EMA acc=79.25%  avg_relax_steps=20.1\n",
      "[eta=0.8 grad-compare step 5000] cos(global W)=1.000000  normR(global W)=1.000003  relErr(global W)=0.000040\n",
      "[eta=0.8] END epoch 13: EMA test-acc~79.40%\n",
      "[eta=0.8 grad-compare step 5100] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000029\n",
      "[eta=0.8] step 5200: lr=0.026  train-CE~0.3745  EMA acc=79.46%  avg_relax_steps=20.1\n",
      "[eta=0.8 grad-compare step 5200] cos(global W)=1.000000  normR(global W)=1.000005  relErr(global W)=0.000031\n",
      "[eta=0.8 grad-compare step 5300] cos(global W)=1.000000  normR(global W)=1.000003  relErr(global W)=0.000028\n",
      "[eta=0.8] step 5400: lr=0.02262  train-CE~0.6158  EMA acc=79.72%  avg_relax_steps=20.1\n",
      "[eta=0.8 grad-compare step 5400] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000043\n",
      "[eta=0.8] END epoch 14: EMA test-acc~79.78%\n",
      "[eta=0.8 grad-compare step 5500] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000042\n",
      "[eta=0.8] step 5600: lr=0.01943  train-CE~0.3730  EMA acc=80.04%  avg_relax_steps=20.2\n",
      "[eta=0.8 grad-compare step 5600] cos(global W)=1.000000  normR(global W)=0.999998  relErr(global W)=0.000040\n",
      "[eta=0.8 grad-compare step 5700] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000025\n",
      "[eta=0.8] step 5800: lr=0.01644  train-CE~0.5980  EMA acc=80.32%  avg_relax_steps=20.1\n",
      "[eta=0.8 grad-compare step 5800] cos(global W)=1.000000  normR(global W)=1.000000  relErr(global W)=0.000029\n",
      "[eta=0.8] END epoch 15: EMA test-acc~80.45%\n",
      "[eta=0.8 grad-compare step 5900] cos(global W)=1.000000  normR(global W)=0.999992  relErr(global W)=0.000719\n",
      "[eta=0.8] step 6000: lr=0.01367  train-CE~0.3847  EMA acc=80.54%  avg_relax_steps=20.3\n",
      "[eta=0.8 grad-compare step 6000] cos(global W)=1.000000  normR(global W)=1.000004  relErr(global W)=0.000036\n",
      "[eta=0.8 grad-compare step 6100] cos(global W)=1.000000  normR(global W)=1.000006  relErr(global W)=0.000037\n",
      "[eta=0.8] step 6200: lr=0.01113  train-CE~0.5581  EMA acc=80.78%  avg_relax_steps=20.2\n",
      "[eta=0.8 grad-compare step 6200] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000037\n",
      "[eta=0.8] END epoch 16: EMA test-acc~80.89%\n",
      "[eta=0.8 grad-compare step 6300] cos(global W)=1.000000  normR(global W)=1.000003  relErr(global W)=0.000033\n",
      "[eta=0.8] step 6400: lr=0.008849  train-CE~0.4026  EMA acc=81.02%  avg_relax_steps=20.1\n",
      "[eta=0.8 grad-compare step 6400] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000030\n",
      "[eta=0.8 grad-compare step 6500] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000025\n",
      "[eta=0.8] step 6600: lr=0.006837  train-CE~0.5503  EMA acc=81.16%  avg_relax_steps=20.1\n",
      "[eta=0.8 grad-compare step 6600] cos(global W)=1.000000  normR(global W)=1.000005  relErr(global W)=0.000036\n",
      "[eta=0.8] END epoch 17: EMA test-acc~81.23%\n",
      "[eta=0.8 grad-compare step 6700] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000034\n",
      "[eta=0.8] step 6800: lr=0.005106  train-CE~0.3992  EMA acc=81.24%  avg_relax_steps=20.2\n",
      "[eta=0.8 grad-compare step 6800] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000042\n",
      "[eta=0.8 grad-compare step 6900] cos(global W)=1.000000  normR(global W)=1.000000  relErr(global W)=0.000026\n",
      "[eta=0.8] step 7000: lr=0.003668  train-CE~0.5354  EMA acc=81.41%  avg_relax_steps=20.3\n",
      "[eta=0.8 grad-compare step 7000] cos(global W)=1.000000  normR(global W)=1.000004  relErr(global W)=0.000036\n",
      "[eta=0.8] END epoch 18: EMA test-acc~81.45%\n",
      "[eta=0.8 grad-compare step 7100] cos(global W)=1.000000  normR(global W)=1.000000  relErr(global W)=0.000030\n",
      "[eta=0.8] step 7200: lr=0.002532  train-CE~0.4202  EMA acc=81.51%  avg_relax_steps=20.2\n",
      "[eta=0.8 grad-compare step 7200] cos(global W)=1.000000  normR(global W)=0.999998  relErr(global W)=0.000042\n",
      "[eta=0.8 grad-compare step 7300] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000037\n",
      "[eta=0.8] step 7400: lr=0.001706  train-CE~0.5212  EMA acc=81.61%  avg_relax_steps=20.3\n",
      "[eta=0.8 grad-compare step 7400] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000039\n",
      "[eta=0.8] END epoch 19: EMA test-acc~81.61%\n",
      "[eta=0.8 grad-compare step 7500] cos(global W)=1.000000  normR(global W)=1.000005  relErr(global W)=0.000045\n",
      "[eta=0.8] step 7600: lr=0.001195  train-CE~0.4395  EMA acc=81.60%  avg_relax_steps=20.3\n",
      "[eta=0.8 grad-compare step 7600] cos(global W)=1.000000  normR(global W)=0.999998  relErr(global W)=0.000038\n",
      "[eta=0.8 grad-compare step 7700] cos(global W)=1.000000  normR(global W)=0.999999  relErr(global W)=0.000036\n",
      "[eta=0.8] step 7800: lr=0.001002  train-CE~0.5135  EMA acc=81.67%  avg_relax_steps=20.2\n",
      "[eta=0.8 grad-compare step 7800] cos(global W)=1.000000  normR(global W)=0.999998  relErr(global W)=0.000033\n",
      "[eta=0.8] END epoch 20: EMA test-acc~81.66%\n",
      "[eta=0.8] Final EMA test-acc: 81.66%\n",
      "\n",
      "============================================================\n",
      "Training with eta = 1.0\n",
      "============================================================\n",
      "\n",
      "[eta=1.0 grad-compare step 0] cos(global W)=1.000000  normR(global W)=1.000001  relErr(global W)=0.000020\n",
      "[eta=1.0 grad-compare step 100] cos(global W)=1.000000  normR(global W)=1.000004  relErr(global W)=0.000024\n",
      "[eta=1.0] step 200: lr=0.09984  train-CE~1.8905  EMA acc=27.09%  avg_relax_steps=13.0\n",
      "[eta=1.0 grad-compare step 200] cos(global W)=1.000000  normR(global W)=1.000002  relErr(global W)=0.000027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 595\u001b[39m\n\u001b[32m    591\u001b[39m     plot_results_icml(results, eta_values)\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 498\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    495\u001b[39m lr = cosine_lr(global_step, total_steps, lr_max=\u001b[32m0.1\u001b[39m, lr_min=\u001b[32m1e-3\u001b[39m)\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Compute xz gradients\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m gradsW_xz, gradsb_xz, ce, steps_taken = \u001b[43mxz_relax_batch_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43meta\u001b[49m\u001b[43m=\u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarm_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# Compare with autograd every compare_every steps\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_step % compare_every == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 170\u001b[39m, in \u001b[36mxz_relax_batch_grad\u001b[39m\u001b[34m(net, x0, y, eta, K, state, tol, warm_start, beta)\u001b[39m\n\u001b[32m    167\u001b[39m x4.add_(dx4, alpha=eta); z4.add_(dz4, alpha=eta)\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# Check convergence\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m upd = \u001b[43m(\u001b[49m\u001b[43mdx1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx4\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m upd < tol:\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from dataclasses import dataclass\n",
    "from torch.nn.grad import conv2d_weight\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Activations\n",
    "# -----------------------------\n",
    "def relu(u): return torch.relu(u)\n",
    "def relu_prime(u): return (u > 0).to(u.dtype)\n",
    "\n",
    "# -----------------------------\n",
    "# Model container (modern CNN for CIFAR-10)\n",
    "# Architecture: 3 conv blocks with batch norm-like scaling + fc\n",
    "# Input: 32x32x3 -> Output: 10 classes\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class CNN_CIFAR10:\n",
    "    W1: torch.Tensor; b1: torch.Tensor   # (64,3,3,3) stride 1 pad 1 -> 32x32x64\n",
    "    W2: torch.Tensor; b2: torch.Tensor   # (128,64,3,3) stride 2 pad 1 -> 16x16x128\n",
    "    W3: torch.Tensor; b3: torch.Tensor   # (256,128,3,3) stride 2 pad 1 -> 8x8x256\n",
    "    W4: torch.Tensor; b4: torch.Tensor   # (10, 256*8*8)\n",
    "\n",
    "    @property\n",
    "    def device(self): return self.W1.device\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Forward at mean states\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def forward_u_sig(net: CNN_CIFAR10, x0, m1, m2, m3):\n",
    "    \"\"\"Forward pass through the network at mean states\"\"\"\n",
    "    u1 = F.conv2d(x0, net.W1, net.b1, stride=1, padding=1)  # (B,64,32,32)\n",
    "    sig1 = relu(u1)\n",
    "\n",
    "    u2 = F.conv2d(m1, net.W2, net.b2, stride=2, padding=1)  # (B,128,16,16)\n",
    "    sig2 = relu(u2)\n",
    "\n",
    "    u3 = F.conv2d(m2, net.W3, net.b3, stride=2, padding=1)  # (B,256,8,8)\n",
    "    sig3 = relu(u3)\n",
    "\n",
    "    B = x0.shape[0]\n",
    "    u4 = m3.reshape(B, -1) @ net.W4.t() + net.b4            # (B,10)\n",
    "    sig4 = u4  # linear logits\n",
    "    return (u1, sig1), (u2, sig2), (u3, sig3), (u4, sig4)\n",
    "\n",
    "\n",
    "class XZState:\n",
    "    \"\"\"Container for x-z state variables\"\"\"\n",
    "    def __init__(self):\n",
    "        self.x1 = None; self.z1 = None\n",
    "        self.x2 = None; self.z2 = None\n",
    "        self.x3 = None; self.z3 = None\n",
    "        self.x4 = None; self.z4 = None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Hamiltonian x-z relaxation gradient for batch (CNN)\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def xz_relax_batch_grad(\n",
    "    net: CNN_CIFAR10, x0, y,\n",
    "    eta=1.0, K=30,\n",
    "    state: XZState | None = None,\n",
    "    tol: float = 1e-4,\n",
    "    warm_start: bool = True,\n",
    "    beta: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute gradients via Hamiltonian x-z relaxation.\n",
    "    \n",
    "    Args:\n",
    "        net: Network parameters\n",
    "        x0: Input images (B, 3, 32, 32)\n",
    "        y: Labels (B,)\n",
    "        eta: Step size for relaxation\n",
    "        K: Max iterations\n",
    "        state: State container for warm starting\n",
    "        tol: Convergence tolerance\n",
    "        warm_start: Whether to use previous state\n",
    "        beta: Weight on classification loss\n",
    "    \"\"\"\n",
    "    device = net.device\n",
    "    B = x0.shape[0]\n",
    "    y_onehot = F.one_hot(y, num_classes=10).to(x0.dtype)\n",
    "\n",
    "    def alloc():\n",
    "        \"\"\"Allocate state variables with correct shapes for CIFAR-10\"\"\"\n",
    "        x1 = torch.zeros(B, 64, 32, 32, device=device); z1 = torch.zeros_like(x1)\n",
    "        x2 = torch.zeros(B, 128, 16, 16, device=device); z2 = torch.zeros_like(x2)\n",
    "        x3 = torch.zeros(B, 256, 8, 8, device=device);   z3 = torch.zeros_like(x3)\n",
    "        x4 = torch.zeros(B, 10, device=device);          z4 = torch.zeros_like(x4)\n",
    "        return x1,z1,x2,z2,x3,z3,x4,z4\n",
    "\n",
    "    # Initialize or reuse state\n",
    "    if (state is None) or (not warm_start) or (state.x1 is None) or (state.x1.shape[0] != B):\n",
    "        x1,z1,x2,z2,x3,z3,x4,z4 = alloc()\n",
    "        if state is not None:\n",
    "            state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4 = x1,z1,x2,z2,x3,z3,x4,z4\n",
    "    else:\n",
    "        x1,z1,x2,z2,x3,z3,x4,z4 = state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4\n",
    "\n",
    "    # Relaxation iterations\n",
    "    steps_taken = 0\n",
    "    for _ in range(K):\n",
    "        steps_taken += 1\n",
    "\n",
    "        # Compute means and differences\n",
    "        m1 = (x1 + z1) * 0.5; s1 = (x1 - z1)\n",
    "        m2 = (x2 + z2) * 0.5; s2 = (x2 - z2)\n",
    "        m3 = (x3 + z3) * 0.5; s3 = (x3 - z3)\n",
    "        m4 = (x4 + z4) * 0.5; s4 = (x4 - z4)\n",
    "\n",
    "        # Forward pass\n",
    "        (u1, sig1), (u2, sig2), (u3, sig3), (u4, sig4) = forward_u_sig(net, x0, m1, m2, m3)\n",
    "\n",
    "        # Constraint violations\n",
    "        F1 = sig1 - m1\n",
    "        F2 = sig2 - m2\n",
    "        F3 = sig3 - m3\n",
    "        F4 = sig4 - m4\n",
    "\n",
    "        # Classification gradient\n",
    "        p = torch.softmax(m4, dim=1)\n",
    "        g4 = (p - y_onehot)\n",
    "\n",
    "        # Backward pass for Jacobian transpose\n",
    "        q2 = relu_prime(u2) * s2\n",
    "        q3 = relu_prime(u3) * s3\n",
    "        q4 = s4\n",
    "\n",
    "        WTq3 = (q4 @ net.W4).reshape(B, 256, 8, 8)\n",
    "        WTq2 = F.conv_transpose2d(q3, net.W3, bias=None, stride=2, padding=1, output_padding=1)\n",
    "        WTq1 = F.conv_transpose2d(q2, net.W2, bias=None, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        # Jacobian transpose terms\n",
    "        Jt1 = -s1 + WTq1\n",
    "        Jt2 = -s2 + WTq2\n",
    "        Jt3 = -s3 + WTq3\n",
    "        Jt4 = -s4\n",
    "\n",
    "        # Gradient updates\n",
    "        dx1 = F1 + 0.5 * Jt1\n",
    "        dz1 = F1 - 0.5 * Jt1\n",
    "\n",
    "        dx2 = F2 + 0.5 * Jt2\n",
    "        dz2 = F2 - 0.5 * Jt2\n",
    "\n",
    "        dx3 = F3 + 0.5 * Jt3\n",
    "        dz3 = F3 - 0.5 * Jt3\n",
    "\n",
    "        dx4 = F4 + 0.5 * Jt4 + 0.5 * beta * g4\n",
    "        dz4 = F4 - 0.5 * Jt4 - 0.5 * beta * g4\n",
    "\n",
    "        # Update states\n",
    "        x1.add_(dx1, alpha=eta); z1.add_(dz1, alpha=eta)\n",
    "        x2.add_(dx2, alpha=eta); z2.add_(dz2, alpha=eta)\n",
    "        x3.add_(dx3, alpha=eta); z3.add_(dz3, alpha=eta)\n",
    "        x4.add_(dx4, alpha=eta); z4.add_(dz4, alpha=eta)\n",
    "\n",
    "        # Check convergence\n",
    "        upd = (dx1.abs().mean() + dx2.abs().mean() + dx3.abs().mean() + dx4.abs().mean()).item()\n",
    "        if upd < tol:\n",
    "            break\n",
    "\n",
    "    # Save state if warm starting\n",
    "    if state is not None and warm_start:\n",
    "        state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4 = x1,z1,x2,z2,x3,z3,x4,z4\n",
    "\n",
    "    # Final state for gradient computation\n",
    "    m1 = (x1 + z1) * 0.5; s1 = (x1 - z1)\n",
    "    m2 = (x2 + z2) * 0.5; s2 = (x2 - z2)\n",
    "    m3 = (x3 + z3) * 0.5; s3 = (x3 - z3)\n",
    "    m4 = (x4 + z4) * 0.5; s4 = (x4 - z4)\n",
    "\n",
    "    # Recompute forward for gradient\n",
    "    (u1, _), (u2, _), (u3, _), (_, _) = forward_u_sig(net, x0, m1, m2, m3)\n",
    "\n",
    "    # Delta terms for gradient\n",
    "    delta1 = relu_prime(u1) * s1\n",
    "    delta2 = relu_prime(u2) * s2\n",
    "    delta3 = relu_prime(u3) * s3\n",
    "    delta4 = s4\n",
    "\n",
    "    # Weight gradients using conv2d_weight\n",
    "    dW1 = conv2d_weight(x0, net.W1.shape, delta1, stride=1, padding=1) / B\n",
    "    dW2 = conv2d_weight(m1, net.W2.shape, delta2, stride=2, padding=1) / B\n",
    "    dW3 = conv2d_weight(m2, net.W3.shape, delta3, stride=2, padding=1) / B\n",
    "\n",
    "    # Bias gradients (sum spatial, avg batch)\n",
    "    db1 = delta1.sum(dim=(0,2,3)) / B\n",
    "    db2 = delta2.sum(dim=(0,2,3)) / B\n",
    "    db3 = delta3.sum(dim=(0,2,3)) / B\n",
    "\n",
    "    # Fully connected layer gradients\n",
    "    m3_flat = m3.reshape(B, -1)\n",
    "    dW4 = (delta4.t() @ m3_flat) / B\n",
    "    db4 = delta4.mean(dim=0)\n",
    "\n",
    "    ce = F.cross_entropy(m4, y).item()\n",
    "    return (dW1,dW2,dW3,dW4), (db1,db2,db3,db4), ce, steps_taken\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SGD + momentum + wd + clip\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def sgd_momentum_step(net: CNN_CIFAR10, gradsW, gradsb, vW, vb,\n",
    "                      lr=0.1, momentum=0.9, weight_decay=5e-4, clip=1.0):\n",
    "    \"\"\"SGD with momentum, weight decay, and gradient clipping\"\"\"\n",
    "    dW1,dW2,dW3,dW4 = gradsW\n",
    "    db1,db2,db3,db4 = gradsb\n",
    "\n",
    "    # Add weight decay\n",
    "    dW1 = dW1 + weight_decay * net.W1\n",
    "    dW2 = dW2 + weight_decay * net.W2\n",
    "    dW3 = dW3 + weight_decay * net.W3\n",
    "    dW4 = dW4 + weight_decay * net.W4\n",
    "\n",
    "    # Gradient clipping\n",
    "    gn2 = float(dW1.norm()**2 + dW2.norm()**2 + dW3.norm()**2 + dW4.norm()**2 +\n",
    "                db1.norm()**2 + db2.norm()**2 + db3.norm()**2 + db4.norm()**2)\n",
    "    gn = gn2**0.5\n",
    "    scale = 1.0 if gn <= clip else (clip / (gn + 1e-12))\n",
    "    if scale != 1.0:\n",
    "        dW1,dW2,dW3,dW4 = dW1*scale, dW2*scale, dW3*scale, dW4*scale\n",
    "        db1,db2,db3,db4 = db1*scale, db2*scale, db3*scale, db4*scale\n",
    "\n",
    "    # Momentum update\n",
    "    vW[0].mul_(momentum).add_(dW1); vb[0].mul_(momentum).add_(db1)\n",
    "    vW[1].mul_(momentum).add_(dW2); vb[1].mul_(momentum).add_(db2)\n",
    "    vW[2].mul_(momentum).add_(dW3); vb[2].mul_(momentum).add_(db3)\n",
    "    vW[3].mul_(momentum).add_(dW4); vb[3].mul_(momentum).add_(db4)\n",
    "\n",
    "    # Parameter update\n",
    "    net.W1.sub_(lr * vW[0]); net.b1.sub_(lr * vb[0])\n",
    "    net.W2.sub_(lr * vW[1]); net.b2.sub_(lr * vb[1])\n",
    "    net.W3.sub_(lr * vW[2]); net.b3.sub_(lr * vb[2])\n",
    "    net.W4.sub_(lr * vW[3]); net.b4.sub_(lr * vb[3])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(ema_net: CNN_CIFAR10, net: CNN_CIFAR10, decay=0.999):\n",
    "    \"\"\"Exponential moving average update\"\"\"\n",
    "    for name in [\"W1\",\"b1\",\"W2\",\"b2\",\"W3\",\"b3\",\"W4\",\"b4\"]:\n",
    "        a = getattr(ema_net, name)\n",
    "        b = getattr(net, name)\n",
    "        a.mul_(decay).add_(b, alpha=(1.0 - decay))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(net: CNN_CIFAR10, loader, device, max_batches=None):\n",
    "    \"\"\"Compute accuracy on a dataset\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x,y) in enumerate(loader):\n",
    "        if max_batches is not None and i >= max_batches: break\n",
    "        x = x.to(device); y = y.to(device)\n",
    "\n",
    "        h1 = relu(F.conv2d(x, net.W1, net.b1, stride=1, padding=1))\n",
    "        h2 = relu(F.conv2d(h1, net.W2, net.b2, stride=2, padding=1))\n",
    "        h3 = relu(F.conv2d(h2, net.W3, net.b3, stride=2, padding=1))\n",
    "        logits = h3.reshape(x.size(0), -1) @ net.W4.t() + net.b4\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "\n",
    "def cosine_lr(step, total_steps, lr_max=0.1, lr_min=1e-3):\n",
    "    \"\"\"Cosine learning rate schedule\"\"\"\n",
    "    t = step / max(1, total_steps)\n",
    "    return lr_min + 0.5*(lr_max - lr_min)*(1.0 + math.cos(math.pi * t))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Autograd gradient (reference)\n",
    "# -----------------------------\n",
    "def autograd_grads_like(net: CNN_CIFAR10, x, y):\n",
    "    \"\"\"Compute gradients using PyTorch autograd for comparison\"\"\"\n",
    "    # Make differentiable copies\n",
    "    W1 = net.W1.detach().clone().requires_grad_(True)\n",
    "    b1 = net.b1.detach().clone().requires_grad_(True)\n",
    "    W2 = net.W2.detach().clone().requires_grad_(True)\n",
    "    b2 = net.b2.detach().clone().requires_grad_(True)\n",
    "    W3 = net.W3.detach().clone().requires_grad_(True)\n",
    "    b3 = net.b3.detach().clone().requires_grad_(True)\n",
    "    W4 = net.W4.detach().clone().requires_grad_(True)\n",
    "    b4 = net.b4.detach().clone().requires_grad_(True)\n",
    "\n",
    "    # Forward pass\n",
    "    h1 = relu(F.conv2d(x, W1, b1, stride=1, padding=1))\n",
    "    h2 = relu(F.conv2d(h1, W2, b2, stride=2, padding=1))\n",
    "    h3 = relu(F.conv2d(h2, W3, b3, stride=2, padding=1))\n",
    "    logits = h3.reshape(x.size(0), -1) @ W4.t() + b4\n",
    "\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    loss.backward()\n",
    "\n",
    "    gradsW = (W1.grad, W2.grad, W3.grad, W4.grad)\n",
    "    gradsb = (b1.grad, b2.grad, b3.grad, b4.grad)\n",
    "    return gradsW, gradsb, float(loss.detach())\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Gradient comparison utilities\n",
    "# -----------------------------\n",
    "def flat_cat(tup):\n",
    "    \"\"\"Flatten and concatenate tensors\"\"\"\n",
    "    return torch.cat([t.reshape(-1) for t in tup], dim=0)\n",
    "\n",
    "def cos_sim(a, b, eps=1e-12):\n",
    "    \"\"\"Cosine similarity\"\"\"\n",
    "    denom = (a.norm() * b.norm()).clamp_min(eps)\n",
    "    return float((a @ b) / denom)\n",
    "\n",
    "def norm_ratio(a, b, eps=1e-12):\n",
    "    \"\"\"Norm ratio\"\"\"\n",
    "    return float(a.norm() / b.norm().clamp_min(eps))\n",
    "\n",
    "def relative_error(a, b, eps=1e-12):\n",
    "    \"\"\"Relative error ||a - b|| / ||b||\"\"\"\n",
    "    return float((a - b).norm() / b.norm().clamp_min(eps))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PLOTTING FUNCTION FOR ICML\n",
    "# ---------------------------------------------------------\n",
    "def plot_results_icml(results, eta_values):\n",
    "    \"\"\"\n",
    "    Generates a publication-quality plot for ICML/NeurIPS.\n",
    "    Focus: Global Relative Error between Relaxed Gradients and Autograd.\n",
    "    \"\"\"\n",
    "    mpl.rcParams.update({\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times New Roman\", \"DejaVu Serif\"],\n",
    "        \"font.size\": 14,\n",
    "        \"axes.labelsize\": 16,\n",
    "        \"axes.titlesize\": 18,\n",
    "        \"xtick.labelsize\": 14,\n",
    "        \"ytick.labelsize\": 14,\n",
    "        \"legend.fontsize\": 14,\n",
    "        \"lines.linewidth\": 2.5,\n",
    "        \"lines.markersize\": 8,\n",
    "        \"figure.figsize\": (8, 6),\n",
    "        \"text.usetex\": False,\n",
    "    })\n",
    "\n",
    "    plt.figure()\n",
    "    \n",
    "    # Define styles for different eta values\n",
    "    styles = {\n",
    "        0.8: {'color': '#d62728', 'marker': 'o', 'linestyle': '-'},\n",
    "        1.0: {'color': '#1f77b4', 'marker': 's', 'linestyle': '--'},\n",
    "        1.2: {'color': '#2ca02c', 'marker': '^', 'linestyle': '-.'},\n",
    "    }\n",
    "\n",
    "    for eta in eta_values:\n",
    "        if eta not in results: continue\n",
    "        \n",
    "        data = results[eta]\n",
    "        cos_steps = data['cos_steps']\n",
    "        rel_err = data['relerr_globalW_hist']\n",
    "        \n",
    "        if len(cos_steps) == 0: continue\n",
    "\n",
    "        style = styles.get(eta, {'color': 'black', 'marker': 'x', 'linestyle': ':'})\n",
    "        \n",
    "        plt.plot(cos_steps, rel_err, \n",
    "                 label=fr'$\\eta={eta}$', \n",
    "                 color=style['color'], \n",
    "                 marker=style['marker'], \n",
    "                 linestyle=style['linestyle'])\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel(r'Relative Gradient Error $\\frac{||\\nabla_{xz} - \\nabla_{BP}||}{||\\nabla_{BP}||}$')\n",
    "    plt.title('Gradient Fidelity throughout Training (CIFAR-10)')\n",
    "    \n",
    "    plt.grid(True, which=\"both\", ls=\":\", alpha=0.6)\n",
    "    plt.legend(frameon=True, fancybox=False, edgecolor='k')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/mnt/user-data/outputs/cifar10_gradient_fidelity.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Plot saved to: /mnt/user-data/outputs/cifar10_gradient_fidelity.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # CIFAR-10 data augmentation (standard)\n",
    "    train_tfm = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    test_tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    # Load CIFAR-10\n",
    "    train_ds = datasets.CIFAR10(\"./data\", train=True, download=True, transform=train_tfm)\n",
    "    test_ds  = datasets.CIFAR10(\"./data\", train=False, download=True, transform=test_tfm)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Training hyperparameters (standard for CIFAR-10)\n",
    "    epochs = 20  # More epochs for CIFAR-10\n",
    "    K = 30  # Max relaxation iterations\n",
    "    tol = 1e-6  # Convergence tolerance\n",
    "    momentum = 0.9\n",
    "    weight_decay = 5e-4\n",
    "    clip = 1.0\n",
    "    ema_decay = 0.999\n",
    "\n",
    "    eval_every = 200\n",
    "    compare_every = 100\n",
    "\n",
    "    # Test different eta values\n",
    "    eta_values = [0.8, 1.0, 1.2]\n",
    "    layer_names = [\"W1\",\"b1\",\"W2\",\"b2\",\"W3\",\"b3\",\"W4\",\"b4\"]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for eta in eta_values:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training with eta = {eta}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Initialize network (He initialization for ReLU)\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        # He initialization: std = sqrt(2 / fan_in)\n",
    "        W1 = torch.randn(64, 3, 3, 3, device=device) * math.sqrt(2.0 / (3*3*3))\n",
    "        b1 = torch.zeros(64, device=device)\n",
    "        \n",
    "        W2 = torch.randn(128, 64, 3, 3, device=device) * math.sqrt(2.0 / (64*3*3))\n",
    "        b2 = torch.zeros(128, device=device)\n",
    "        \n",
    "        W3 = torch.randn(256, 128, 3, 3, device=device) * math.sqrt(2.0 / (128*3*3))\n",
    "        b3 = torch.zeros(256, device=device)\n",
    "        \n",
    "        W4 = torch.randn(10, 256*8*8, device=device) * math.sqrt(2.0 / (256*8*8))\n",
    "        b4 = torch.zeros(10, device=device)\n",
    "\n",
    "        net = CNN_CIFAR10(W1,b1,W2,b2,W3,b3,W4,b4)\n",
    "        ema_net = CNN_CIFAR10(W1.clone(),b1.clone(),W2.clone(),b2.clone(),W3.clone(),b3.clone(),W4.clone(),b4.clone())\n",
    "\n",
    "        vW = [torch.zeros_like(net.W1), torch.zeros_like(net.W2), torch.zeros_like(net.W3), torch.zeros_like(net.W4)]\n",
    "        vb = [torch.zeros_like(net.b1), torch.zeros_like(net.b2), torch.zeros_like(net.b3), torch.zeros_like(net.b4)]\n",
    "\n",
    "        total_steps = epochs * len(train_loader)\n",
    "        global_step = 0\n",
    "\n",
    "        ce_hist, acc_hist, step_hist = [], [], []\n",
    "\n",
    "        # Gradient comparison histories\n",
    "        cos_globalW_hist = []\n",
    "        cos_layer_hist = {name: [] for name in layer_names}\n",
    "        cos_steps = []\n",
    "\n",
    "        norm_globalW_hist = []\n",
    "        norm_layer_hist = {name: [] for name in layer_names}\n",
    "\n",
    "        relerr_globalW_hist = []\n",
    "        relerr_layer_hist = {name: [] for name in layer_names}\n",
    "\n",
    "        state = XZState()\n",
    "\n",
    "        for ep in range(1, epochs+1):\n",
    "            running = 0.0\n",
    "            steps_sum = 0\n",
    "            steps_count = 0\n",
    "\n",
    "            for x, y in train_loader:\n",
    "                x = x.to(device, non_blocking=True)\n",
    "                y = y.to(device, non_blocking=True)\n",
    "\n",
    "                # Cosine learning rate schedule\n",
    "                lr = cosine_lr(global_step, total_steps, lr_max=0.1, lr_min=1e-3)\n",
    "\n",
    "                # Compute xz gradients\n",
    "                gradsW_xz, gradsb_xz, ce, steps_taken = xz_relax_batch_grad(\n",
    "                    net, x, y,\n",
    "                    eta=eta, K=K,\n",
    "                    state=state,\n",
    "                    tol=tol,\n",
    "                    warm_start=True\n",
    "                )\n",
    "\n",
    "                # Compare with autograd every compare_every steps\n",
    "                if global_step % compare_every == 0:\n",
    "                    gradsW_ag, gradsb_ag, ce_ag = autograd_grads_like(net, x, y)\n",
    "\n",
    "                    # Global weight metrics\n",
    "                    gx = flat_cat(gradsW_xz)\n",
    "                    ga = flat_cat(gradsW_ag)\n",
    "\n",
    "                    cos_globalW_hist.append(cos_sim(gx, ga))\n",
    "                    norm_globalW_hist.append(norm_ratio(gx, ga))\n",
    "                    relerr_globalW_hist.append(relative_error(gx, ga))\n",
    "                    cos_steps.append(global_step)\n",
    "\n",
    "                    # Per-layer metrics\n",
    "                    xz_all = {\n",
    "                        \"W1\": gradsW_xz[0], \"W2\": gradsW_xz[1], \"W3\": gradsW_xz[2], \"W4\": gradsW_xz[3],\n",
    "                        \"b1\": gradsb_xz[0], \"b2\": gradsb_xz[1], \"b3\": gradsb_xz[2], \"b4\": gradsb_xz[3],\n",
    "                    }\n",
    "                    ag_all = {\n",
    "                        \"W1\": gradsW_ag[0], \"W2\": gradsW_ag[1], \"W3\": gradsW_ag[2], \"W4\": gradsW_ag[3],\n",
    "                        \"b1\": gradsb_ag[0], \"b2\": gradsb_ag[1], \"b3\": gradsb_ag[2], \"b4\": gradsb_ag[3],\n",
    "                    }\n",
    "                    for name in layer_names:\n",
    "                        cos_layer_hist[name].append(cos_sim(xz_all[name].reshape(-1), ag_all[name].reshape(-1)))\n",
    "                        norm_layer_hist[name].append(norm_ratio(xz_all[name].reshape(-1), ag_all[name].reshape(-1)))\n",
    "                        relerr_layer_hist[name].append(relative_error(xz_all[name].reshape(-1), ag_all[name].reshape(-1)))\n",
    "\n",
    "                    print(\n",
    "                        f\"[eta={eta} grad-compare step {global_step}] \"\n",
    "                        f\"cos(global W)={cos_globalW_hist[-1]:.6f}  \"\n",
    "                        f\"normR(global W)={norm_globalW_hist[-1]:.6f}  \"\n",
    "                        f\"relErr(global W)={relerr_globalW_hist[-1]:.6f}\"\n",
    "                    )\n",
    "\n",
    "                # Update parameters\n",
    "                sgd_momentum_step(net, gradsW_xz, gradsb_xz, vW, vb,\n",
    "                                  lr=lr, momentum=momentum, weight_decay=weight_decay, clip=clip)\n",
    "\n",
    "                ema_update(ema_net, net, decay=ema_decay)\n",
    "\n",
    "                global_step += 1\n",
    "                running += ce\n",
    "                ce_hist.append(ce)\n",
    "                step_hist.append(global_step)\n",
    "\n",
    "                steps_sum += steps_taken\n",
    "                steps_count += 1\n",
    "\n",
    "                # Evaluation\n",
    "                if global_step % eval_every == 0:\n",
    "                    acc = accuracy(ema_net, test_loader, device=device, max_batches=None)\n",
    "                    acc_hist.append(acc)\n",
    "\n",
    "                    avgK = steps_sum / max(1, steps_count)\n",
    "                    print(f\"[eta={eta}] step {global_step}: lr={lr:.4g}  train-CE~{running/eval_every:.4f}  \"\n",
    "                          f\"EMA acc={acc*100:.2f}%  avg_relax_steps={avgK:.1f}\")\n",
    "                    running = 0.0\n",
    "                    steps_sum = 0\n",
    "                    steps_count = 0\n",
    "\n",
    "            # End of epoch\n",
    "            acc = accuracy(ema_net, test_loader, device=device, max_batches=None)\n",
    "            print(f\"[eta={eta}] END epoch {ep:02d}: EMA test-acc~{acc*100:.2f}%\")\n",
    "\n",
    "        # Final evaluation\n",
    "        final_acc = accuracy(ema_net, test_loader, device=device, max_batches=None)\n",
    "        print(f\"[eta={eta}] Final EMA test-acc: {final_acc*100:.2f}%\")\n",
    "        \n",
    "        # Store results\n",
    "        results[eta] = {\n",
    "            'ce_hist': ce_hist,\n",
    "            'acc_hist': acc_hist,\n",
    "            'step_hist': step_hist,\n",
    "            'cos_globalW_hist': cos_globalW_hist,\n",
    "            'cos_layer_hist': cos_layer_hist.copy(),\n",
    "            'cos_steps': cos_steps,\n",
    "            'norm_globalW_hist': norm_globalW_hist,\n",
    "            'norm_layer_hist': norm_layer_hist.copy(),\n",
    "            'relerr_globalW_hist': relerr_globalW_hist,\n",
    "            'relerr_layer_hist': relerr_layer_hist.copy(),\n",
    "            'final_acc': final_acc,\n",
    "        }\n",
    "\n",
    "    # Generate plot\n",
    "    print(\"\\nGenerating ICML publication-quality plot...\")\n",
    "    plot_results_icml(results, eta_values)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c0619",
   "metadata": {},
   "source": [
    "CODE OF 85 ON CIFAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e10b50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "\n",
      "Starting training: 5-Layer CNN (1.5M params), 40 epochs\n",
      "------------------------------------------------------------\n",
      "Ep 1 | Step 1 | LR 0.0500 | Loss 2.8092\n",
      "Ep 1 | Step 101 | LR 0.0500 | Loss 1.6681\n",
      "Ep 1 | Step 201 | LR 0.0500 | Loss 1.6249\n",
      "Ep 1 | Step 301 | LR 0.0500 | Loss 1.3416\n",
      "\n",
      ">>> Epoch 1 Done | Train Loss: 1.5797 | TEST ACC: 34.74%\n",
      "\n",
      "Ep 2 | Step 392 | LR 0.0499 | Loss 1.1013\n",
      "Ep 2 | Step 492 | LR 0.0499 | Loss 1.2947\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 367\u001b[39m\n\u001b[32m    364\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinal Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_acc*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 344\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    340\u001b[39m y = y.to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    342\u001b[39m lr = cosine_lr(global_step, total_steps, lr_max=lr_max, lr_min=lr_min)\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m gradsW, gradsb, ce = \u001b[43mxz_relax_batch_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m=\u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    346\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m sgd_momentum_step(net, gradsW, gradsb, vW, vb, lr=lr)\n\u001b[32m    349\u001b[39m ema_update(ema_net, net)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 159\u001b[39m, in \u001b[36mxz_relax_batch_grad\u001b[39m\u001b[34m(net, x0, y, eta, K, state, tol, warm_start, beta)\u001b[39m\n\u001b[32m    156\u001b[39m x4.add_(dx4, alpha=eta); z4.add_(dz4, alpha=eta)\n\u001b[32m    157\u001b[39m x5.add_(dx5, alpha=eta); z5.add_(dz5, alpha=eta)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m upd = \u001b[43m(\u001b[49m\u001b[43mdx1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx4\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx5\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m upd < tol:\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from dataclasses import dataclass\n",
    "from torch.nn.grad import conv2d_weight\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Activations\n",
    "# -----------------------------\n",
    "def relu(u): return torch.relu(u)\n",
    "def relu_prime(u): return (u > 0).to(u.dtype)\n",
    "\n",
    "# -----------------------------\n",
    "# Model container (4 Conv + 1 FC = 5 Layers)\n",
    "# \"VGG-Style\": 64 -> 128 -> 256 -> 512\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class CNN5:\n",
    "    # 32x32 Input\n",
    "    W1: torch.Tensor; b1: torch.Tensor   # (64, 3, 3, 3)   stride 1 -> 32x32\n",
    "    W2: torch.Tensor; b2: torch.Tensor   # (128, 64, 3, 3) stride 2 -> 16x16 (Downsample)\n",
    "    W3: torch.Tensor; b3: torch.Tensor   # (256, 128, 3, 3) stride 1 -> 16x16\n",
    "    W4: torch.Tensor; b4: torch.Tensor   # (512, 256, 3, 3) stride 2 -> 8x8  (Downsample)\n",
    "    W5: torch.Tensor; b5: torch.Tensor   # (10, 512*8*8)   Linear\n",
    "\n",
    "    @property\n",
    "    def device(self): return self.W1.device\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Forward at mean states\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def forward_u_sig(net: CNN5, x0, m1, m2, m3, m4):\n",
    "    # Layer 1: Conv 3->64\n",
    "    u1 = F.conv2d(x0, net.W1, net.b1, stride=1, padding=1)\n",
    "    sig1 = relu(u1)\n",
    "\n",
    "    # Layer 2: Conv 64->128 (Stride 2)\n",
    "    u2 = F.conv2d(m1, net.W2, net.b2, stride=2, padding=1)\n",
    "    sig2 = relu(u2)\n",
    "\n",
    "    # Layer 3: Conv 128->256\n",
    "    u3 = F.conv2d(m2, net.W3, net.b3, stride=1, padding=1)\n",
    "    sig3 = relu(u3)\n",
    "\n",
    "    # Layer 4: Conv 256->512 (Stride 2)\n",
    "    u4 = F.conv2d(m3, net.W4, net.b4, stride=2, padding=1)\n",
    "    sig4 = relu(u4)\n",
    "\n",
    "    # Layer 5: Linear Classifier\n",
    "    B = x0.shape[0]\n",
    "    u5 = m4.reshape(B, -1) @ net.W5.t() + net.b5\n",
    "    sig5 = u5 # Linear logits\n",
    "\n",
    "    return (u1, sig1), (u2, sig2), (u3, sig3), (u4, sig4), (u5, sig5)\n",
    "\n",
    "class XZState:\n",
    "    def __init__(self):\n",
    "        self.x1 = None; self.z1 = None\n",
    "        self.x2 = None; self.z2 = None\n",
    "        self.x3 = None; self.z3 = None\n",
    "        self.x4 = None; self.z4 = None\n",
    "        self.x5 = None; self.z5 = None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Hamiltonian x-z relaxation gradient for batch (CNN)\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def xz_relax_batch_grad(\n",
    "    net: CNN5, x0, y,\n",
    "    eta=1.0, K=30,\n",
    "    state: XZState | None = None,\n",
    "    tol: float = 1e-4,\n",
    "    warm_start: bool = True,\n",
    "    beta: float = 1.0,\n",
    "):\n",
    "    device = net.device\n",
    "    B = x0.shape[0]\n",
    "    y_onehot = F.one_hot(y, num_classes=10).to(x0.dtype)\n",
    "\n",
    "    def alloc():\n",
    "        # L1: 32x32, 64ch\n",
    "        x1 = torch.zeros(B, 64, 32, 32, device=device); z1 = torch.zeros_like(x1)\n",
    "        # L2: 16x16, 128ch\n",
    "        x2 = torch.zeros(B, 128, 16, 16, device=device); z2 = torch.zeros_like(x2)\n",
    "        # L3: 16x16, 256ch\n",
    "        x3 = torch.zeros(B, 256, 16, 16, device=device); z3 = torch.zeros_like(x3)\n",
    "        # L4: 8x8, 512ch\n",
    "        x4 = torch.zeros(B, 512, 8, 8, device=device);   z4 = torch.zeros_like(x4)\n",
    "        # L5: 10 classes\n",
    "        x5 = torch.zeros(B, 10, device=device);          z5 = torch.zeros_like(x5)\n",
    "        return x1,z1,x2,z2,x3,z3,x4,z4,x5,z5\n",
    "\n",
    "    # Check state validity\n",
    "    if (state is None) or (not warm_start) or (state.x1 is None) or (state.x1.shape[0] != B):\n",
    "        x1,z1,x2,z2,x3,z3,x4,z4,x5,z5 = alloc()\n",
    "        if state is not None:\n",
    "            state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4,state.x5,state.z5 = x1,z1,x2,z2,x3,z3,x4,z4,x5,z5\n",
    "    else:\n",
    "        x1,z1,x2,z2,x3,z3,x4,z4,x5,z5 = state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4,state.x5,state.z5\n",
    "\n",
    "    for _ in range(K):\n",
    "        m1 = (x1 + z1) * 0.5; s1 = (x1 - z1)\n",
    "        m2 = (x2 + z2) * 0.5; s2 = (x2 - z2)\n",
    "        m3 = (x3 + z3) * 0.5; s3 = (x3 - z3)\n",
    "        m4 = (x4 + z4) * 0.5; s4 = (x4 - z4)\n",
    "        m5 = (x5 + z5) * 0.5; s5 = (x5 - z5)\n",
    "\n",
    "        (u1, sig1), (u2, sig2), (u3, sig3), (u4, sig4), (u5, sig5) = forward_u_sig(net, x0, m1, m2, m3, m4)\n",
    "\n",
    "        F1 = sig1 - m1\n",
    "        F2 = sig2 - m2\n",
    "        F3 = sig3 - m3\n",
    "        F4 = sig4 - m4\n",
    "        F5 = sig5 - m5\n",
    "\n",
    "        p = torch.softmax(m5, dim=1)\n",
    "        g5 = (p - y_onehot)\n",
    "\n",
    "        # Back-propagate 's' terms through weights\n",
    "        q2 = relu_prime(u2) * s2\n",
    "        q3 = relu_prime(u3) * s3\n",
    "        q4 = relu_prime(u4) * s4\n",
    "        q5 = s5\n",
    "\n",
    "        # Transposed Convolutions (Adjoints)\n",
    "        WTq4 = (q5 @ net.W5).reshape(B, 512, 8, 8)\n",
    "        WTq3 = F.conv_transpose2d(q4, net.W4, bias=None, stride=2, padding=1, output_padding=1)\n",
    "        WTq2 = F.conv_transpose2d(q3, net.W3, bias=None, stride=1, padding=1)\n",
    "        WTq1 = F.conv_transpose2d(q2, net.W2, bias=None, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        Jt1 = -s1 + WTq1\n",
    "        Jt2 = -s2 + WTq2\n",
    "        Jt3 = -s3 + WTq3\n",
    "        Jt4 = -s4 + WTq4\n",
    "        Jt5 = -s5\n",
    "\n",
    "        dx1 = F1 + 0.5 * Jt1; dz1 = F1 - 0.5 * Jt1\n",
    "        dx2 = F2 + 0.5 * Jt2; dz2 = F2 - 0.5 * Jt2\n",
    "        dx3 = F3 + 0.5 * Jt3; dz3 = F3 - 0.5 * Jt3\n",
    "        dx4 = F4 + 0.5 * Jt4; dz4 = F4 - 0.5 * Jt4\n",
    "        dx5 = F5 + 0.5 * Jt5 + 0.5 * beta * g5\n",
    "        dz5 = F5 - 0.5 * Jt5 - 0.5 * beta * g5\n",
    "\n",
    "        x1.add_(dx1, alpha=eta); z1.add_(dz1, alpha=eta)\n",
    "        x2.add_(dx2, alpha=eta); z2.add_(dz2, alpha=eta)\n",
    "        x3.add_(dx3, alpha=eta); z3.add_(dz3, alpha=eta)\n",
    "        x4.add_(dx4, alpha=eta); z4.add_(dz4, alpha=eta)\n",
    "        x5.add_(dx5, alpha=eta); z5.add_(dz5, alpha=eta)\n",
    "\n",
    "        upd = (dx1.abs().mean() + dx2.abs().mean() + dx3.abs().mean() + dx4.abs().mean() + dx5.abs().mean()).item()\n",
    "        if upd < tol:\n",
    "            break\n",
    "\n",
    "    if state is not None and warm_start:\n",
    "        state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4,state.x5,state.z5 = x1,z1,x2,z2,x3,z3,x4,z4,x5,z5\n",
    "\n",
    "    # Final gradients\n",
    "    m1 = (x1 + z1) * 0.5; s1 = (x1 - z1)\n",
    "    m2 = (x2 + z2) * 0.5; s2 = (x2 - z2)\n",
    "    m3 = (x3 + z3) * 0.5; s3 = (x3 - z3)\n",
    "    m4 = (x4 + z4) * 0.5; s4 = (x4 - z4)\n",
    "    # m5 is not needed for weights, only s5 (delta5)\n",
    "\n",
    "    (u1, _), (u2, _), (u3, _), (u4, _), (_, _) = forward_u_sig(net, x0, m1, m2, m3, m4)\n",
    "\n",
    "    delta1 = relu_prime(u1) * s1\n",
    "    delta2 = relu_prime(u2) * s2\n",
    "    delta3 = relu_prime(u3) * s3\n",
    "    delta4 = relu_prime(u4) * s4\n",
    "    delta5 = s5\n",
    "\n",
    "    dW1 = conv2d_weight(x0, net.W1.shape, delta1, stride=1, padding=1) / B\n",
    "    dW2 = conv2d_weight(m1, net.W2.shape, delta2, stride=2, padding=1) / B\n",
    "    dW3 = conv2d_weight(m2, net.W3.shape, delta3, stride=1, padding=1) / B\n",
    "    dW4 = conv2d_weight(m3, net.W4.shape, delta4, stride=2, padding=1) / B\n",
    "    \n",
    "    m4_flat = m4.reshape(B, -1)\n",
    "    dW5 = (delta5.t() @ m4_flat) / B\n",
    "\n",
    "    db1 = delta1.sum(dim=(0,2,3)) / B\n",
    "    db2 = delta2.sum(dim=(0,2,3)) / B\n",
    "    db3 = delta3.sum(dim=(0,2,3)) / B\n",
    "    db4 = delta4.sum(dim=(0,2,3)) / B\n",
    "    db5 = delta5.mean(dim=0)\n",
    "\n",
    "    ce = F.cross_entropy(m5, y).item()\n",
    "    return (dW1,dW2,dW3,dW4,dW5), (db1,db2,db3,db4,db5), ce\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SGD + momentum + wd + clip + EMA\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def sgd_momentum_step(net: CNN5, gradsW, gradsb, vW, vb,\n",
    "                      lr=0.01, momentum=0.9, weight_decay=1e-4, clip=1.0):\n",
    "    for i in range(5):\n",
    "        dWi = gradsW[i] + weight_decay * getattr(net, f\"W{i+1}\")\n",
    "        dbi = gradsb[i]\n",
    "        \n",
    "        # Simple clipping per layer for stability in deep nets\n",
    "        gn = (dWi.norm()**2 + dbi.norm()**2)**0.5\n",
    "        scale = 1.0 if gn <= clip else (clip / (gn + 1e-12))\n",
    "        dWi *= scale; dbi *= scale\n",
    "\n",
    "        vW[i].mul_(momentum).add_(dWi)\n",
    "        vb[i].mul_(momentum).add_(dbi)\n",
    "\n",
    "        getattr(net, f\"W{i+1}\").sub_(lr * vW[i])\n",
    "        getattr(net, f\"b{i+1}\").sub_(lr * vb[i])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(ema_net: CNN5, net: CNN5, decay=0.999):\n",
    "    for i in range(1, 6):\n",
    "        for param in [\"W\", \"b\"]:\n",
    "            name = f\"{param}{i}\"\n",
    "            a = getattr(ema_net, name)\n",
    "            b = getattr(net, name)\n",
    "            a.mul_(decay).add_(b, alpha=(1.0 - decay))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(net: CNN5, loader, device, max_batches=800):\n",
    "    correct = 0; total = 0\n",
    "    for i, (x,y) in enumerate(loader):\n",
    "        if i >= max_batches: break\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        \n",
    "        h1 = relu(F.conv2d(x, net.W1, net.b1, stride=1, padding=1))\n",
    "        h2 = relu(F.conv2d(h1, net.W2, net.b2, stride=2, padding=1))\n",
    "        h3 = relu(F.conv2d(h2, net.W3, net.b3, stride=1, padding=1))\n",
    "        h4 = relu(F.conv2d(h3, net.W4, net.b4, stride=2, padding=1))\n",
    "        logits = h4.reshape(x.size(0), -1) @ net.W5.t() + net.b5\n",
    "        \n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "\n",
    "def cosine_lr(step, total_steps, lr_max=0.05, lr_min=5e-4):\n",
    "    t = step / max(1, total_steps)\n",
    "    return lr_min + 0.5*(lr_max - lr_min)*(1.0 + math.cos(math.pi * t))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# He/Kaiming Initialization Helper\n",
    "# -----------------------------\n",
    "def kaiming_init(shape, device):\n",
    "    # Fan-in is roughly channels * kernel_h * kernel_w\n",
    "    if len(shape) == 4:\n",
    "        fan_in = shape[1] * shape[2] * shape[3]\n",
    "    else:\n",
    "        fan_in = shape[1] # Linear layer (out, in)\n",
    "    std = math.sqrt(2.0 / fan_in)\n",
    "    return std * torch.randn(shape, device=device)\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # CIFAR-10 Data Setup\n",
    "    # ----------------------------------------------------\n",
    "    cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "    cifar_std  = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "    train_tfm = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cifar_mean, cifar_std)\n",
    "    ])\n",
    "    test_tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cifar_mean, cifar_std)\n",
    "    ])\n",
    "\n",
    "    train_ds = datasets.CIFAR10(\"./data\", train=True, download=True, transform=train_tfm)\n",
    "    test_ds  = datasets.CIFAR10(\"./data\", train=False, download=True, transform=test_tfm)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Configuration\n",
    "    # ----------------------------------------------------\n",
    "    epochs = 40          # Increased epochs for deeper model\n",
    "    eta = 1.0            # Relaxation step size (tuned)\n",
    "    K = 25               # Relax steps (slightly lower K for speed, usually fine)\n",
    "    lr_max = 0.05        # Higher LR allowed due to better architecture\n",
    "    lr_min = 0.0005\n",
    "    \n",
    "    # Init Weights (He Initialization)\n",
    "    torch.manual_seed(42)\n",
    "    W1 = kaiming_init((64, 3, 3, 3), device)\n",
    "    W2 = kaiming_init((128, 64, 3, 3), device) # Stride 2\n",
    "    W3 = kaiming_init((256, 128, 3, 3), device)\n",
    "    W4 = kaiming_init((512, 256, 3, 3), device) # Stride 2\n",
    "    W5 = kaiming_init((10, 512*8*8), device)\n",
    "\n",
    "    b1 = torch.zeros(64, device=device)\n",
    "    b2 = torch.zeros(128, device=device)\n",
    "    b3 = torch.zeros(256, device=device)\n",
    "    b4 = torch.zeros(512, device=device)\n",
    "    b5 = torch.zeros(10, device=device)\n",
    "\n",
    "    net = CNN5(W1,b1,W2,b2,W3,b3,W4,b4,W5,b5)\n",
    "    ema_net = CNN5(W1.clone(),b1.clone(),W2.clone(),b2.clone(),\n",
    "                   W3.clone(),b3.clone(),W4.clone(),b4.clone(),W5.clone(),b5.clone())\n",
    "\n",
    "    # Velocity buffers\n",
    "    vW = [torch.zeros_like(getattr(net, f\"W{i}\")) for i in range(1,6)]\n",
    "    vb = [torch.zeros_like(getattr(net, f\"b{i}\")) for i in range(1,6)]\n",
    "\n",
    "    total_steps = epochs * len(train_loader)\n",
    "    global_step = 0\n",
    "    \n",
    "    state = XZState()\n",
    "\n",
    "    print(f\"\\nStarting training: 5-Layer CNN (1.5M params), {epochs} epochs\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        running_ce = 0.0\n",
    "        \n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            lr = cosine_lr(global_step, total_steps, lr_max=lr_max, lr_min=lr_min)\n",
    "\n",
    "            gradsW, gradsb, ce = xz_relax_batch_grad(\n",
    "                net, x, y, eta=eta, K=K, state=state, warm_start=True\n",
    "            )\n",
    "\n",
    "            sgd_momentum_step(net, gradsW, gradsb, vW, vb, lr=lr)\n",
    "            ema_update(ema_net, net)\n",
    "\n",
    "            global_step += 1\n",
    "            running_ce += ce\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Ep {ep} | Step {global_step} | LR {lr:.4f} | Loss {ce:.4f}\")\n",
    "\n",
    "        # End of epoch eval\n",
    "        test_acc = accuracy(ema_net, test_loader, device)\n",
    "        train_loss_avg = running_ce / len(train_loader)\n",
    "        print(f\"\\n>>> Epoch {ep} Done | Train Loss: {train_loss_avg:.4f} | TEST ACC: {test_acc*100:.2f}%\\n\")\n",
    "\n",
    "    print(\"Training Complete.\")\n",
    "    final_acc = accuracy(ema_net, test_loader, device, max_batches=2000)\n",
    "    print(f\"Final Test Accuracy: {final_acc*100:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94045790",
   "metadata": {},
   "source": [
    "CODE WITH SIMPLIFIED DYNAMICS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3defedb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "step 400: lr=0.01976  train-CE~0.8315  EMA acc=78.87%  avg_relax_steps=11.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 448\u001b[39m\n\u001b[32m    443\u001b[39m     plt.show()\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 416\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    413\u001b[39m             steps_sum = \u001b[32m0\u001b[39m\n\u001b[32m    414\u001b[39m             steps_count = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     acc = \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mema_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m800\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEND epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: EMA test-acc~\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    419\u001b[39m final_acc = accuracy(ema_net, test_loader, device=device, max_batches=\u001b[32m1200\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 285\u001b[39m, in \u001b[36maccuracy\u001b[39m\u001b[34m(net, loader, device, max_batches)\u001b[39m\n\u001b[32m    283\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m    284\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m;\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1443\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1442\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1444\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1445\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/sw/arch/easybuild/2025a/software/Python/3.13.1-GCCcore-14.2.0/lib/python3.13/queue.py:213\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ShutDown\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/sw/arch/easybuild/2025a/software/Python/3.13.1-GCCcore-14.2.0/lib/python3.13/threading.py:363\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    365\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from torch.nn.grad import conv2d_weight\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Activations\n",
    "# -----------------------------\n",
    "def relu(u): return torch.relu(u)\n",
    "def relu_prime(u): return (u > 0).to(u.dtype)\n",
    "\n",
    "# -----------------------------\n",
    "# Model container (3 conv + fc)\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class CNN3:\n",
    "    W1: torch.Tensor; b1: torch.Tensor   # (64,1,3,3)\n",
    "    W2: torch.Tensor; b2: torch.Tensor   # (64,64,3,3)\n",
    "    W3: torch.Tensor; b3: torch.Tensor   # (128,64,3,3) stride 2\n",
    "    W4: torch.Tensor; b4: torch.Tensor   # (10, 128*14*14)\n",
    "\n",
    "    @property\n",
    "    def device(self): return self.W1.device\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Forward at mean states\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def forward_u_sig(net: CNN3, x0, m1, m2, m3):\n",
    "    # layer1 uses raw input\n",
    "    u1 = F.conv2d(x0, net.W1, net.b1, stride=1, padding=1)  # (B,64,28,28)\n",
    "    sig1 = relu(u1)\n",
    "\n",
    "    # layer2 uses mean m1\n",
    "    u2 = F.conv2d(m1, net.W2, net.b2, stride=1, padding=1)  # (B,64,28,28)\n",
    "    sig2 = relu(u2)\n",
    "\n",
    "    # layer3 uses mean m2\n",
    "    u3 = F.conv2d(m2, net.W3, net.b3, stride=2, padding=1)  # (B,128,14,14)\n",
    "    sig3 = relu(u3)\n",
    "\n",
    "    # fc uses mean m3\n",
    "    B = x0.shape[0]\n",
    "    u4 = m3.reshape(B, -1) @ net.W4.t() + net.b4            # (B,10)\n",
    "    sig4 = u4  # linear logits\n",
    "    return (u1, sig1), (u2, sig2), (u3, sig3), (u4, sig4)\n",
    "\n",
    "class XZState:\n",
    "    def __init__(self):\n",
    "        self.x1 = None; self.z1 = None\n",
    "        self.x2 = None; self.z2 = None\n",
    "        self.x3 = None; self.z3 = None\n",
    "        self.x4 = None; self.z4 = None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Hamiltonian x-z relaxation gradient for batch (CNN)\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def xz_relax_batch_grad(\n",
    "    net: CNN3, x0, y,\n",
    "    eta=1.0, K=30,\n",
    "    state: XZState | None = None,\n",
    "    tol: float = 1e-4,\n",
    "    warm_start: bool = True,\n",
    "    beta: float = 1.0,   # set beta=1 by default\n",
    "):\n",
    "    device = net.device\n",
    "    B = x0.shape[0]\n",
    "    y_onehot = F.one_hot(y, num_classes=10).to(x0.dtype)\n",
    "\n",
    "    # -------------------------\n",
    "    # Init / warm-start x,z\n",
    "    # -------------------------\n",
    "    def alloc():\n",
    "        x1 = torch.zeros(B, 64, 28, 28, device=device); z1 = torch.zeros_like(x1)\n",
    "        x2 = torch.zeros(B, 64, 28, 28, device=device); z2 = torch.zeros_like(x2)\n",
    "        x3 = torch.zeros(B, 128, 14, 14, device=device); z3 = torch.zeros_like(x3)\n",
    "        x4 = torch.zeros(B, 10, device=device);         z4 = torch.zeros_like(x4)\n",
    "        return x1,z1,x2,z2,x3,z3,x4,z4\n",
    "\n",
    "    if (state is None) or (not warm_start) or (state.x1 is None) or (state.x1.shape[0] != B):\n",
    "        x1,z1,x2,z2,x3,z3,x4,z4 = alloc()\n",
    "        if state is not None:\n",
    "            state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4 = x1,z1,x2,z2,x3,z3,x4,z4\n",
    "    else:\n",
    "        x1,z1,x2,z2,x3,z3,x4,z4 = state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4\n",
    "\n",
    "    # -------------------------\n",
    "    # Relaxation loop (Hamiltonian)\n",
    "    # -------------------------\n",
    "    steps_taken = 0\n",
    "    for _ in range(K):\n",
    "        steps_taken += 1\n",
    "\n",
    "        # mean/stress\n",
    "        m1 = (x1 + z1) * 0.5; s1 = (x1 - z1)\n",
    "        m2 = (x2 + z2) * 0.5; s2 = (x2 - z2)\n",
    "        m3 = (x3 + z3) * 0.5; s3 = (x3 - z3)\n",
    "        m4 = (x4 + z4) * 0.5; s4 = (x4 - z4)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Forward at mean states (KEEP for Jt / q computations etc.)\n",
    "        # ------------------------------------------------------------\n",
    "        (u1, sig1), (u2m, sig2m), (u3m, sig3m), (u4m, sig4m) = forward_u_sig(net, x0, m1, m2, m3)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # CHANGED PART:\n",
    "        # Instead of force at midpoint (sigma(u(m)) - m),\n",
    "        # compute mean of forces at x and z:\n",
    "        #   F = 0.5*( F(x) + F(z) )\n",
    "        # where F_l(x) = sigma( W * x_{l-1} ) - x_l, and same for z.\n",
    "        # (Layer1 depends only on x0, so averaging yields same as before.)\n",
    "        # ------------------------------------------------------------\n",
    "\n",
    "        # Layer1 forces (u1 from x0, same for x/z)\n",
    "        F1x = sig1 - x1\n",
    "        F1z = sig1 - z1\n",
    "        F1  = 0.5 * (F1x + F1z)\n",
    "\n",
    "        # Layer2 forces\n",
    "        u2x = F.conv2d(x1, net.W2, net.b2, stride=1, padding=1)\n",
    "        u2z = F.conv2d(z1, net.W2, net.b2, stride=1, padding=1)\n",
    "        sig2x = relu(u2x)\n",
    "        sig2z = relu(u2z)\n",
    "        F2x = sig2x - x2\n",
    "        F2z = sig2z - z2\n",
    "        F2  = 0.5 * (F2x + F2z)\n",
    "\n",
    "        # Layer3 forces\n",
    "        u3x = F.conv2d(x2, net.W3, net.b3, stride=2, padding=1)\n",
    "        u3z = F.conv2d(z2, net.W3, net.b3, stride=2, padding=1)\n",
    "        sig3x = relu(u3x)\n",
    "        sig3z = relu(u3z)\n",
    "        F3x = sig3x - x3\n",
    "        F3z = sig3z - z3\n",
    "        F3  = 0.5 * (F3x + F3z)\n",
    "\n",
    "        # Layer4 forces (linear)\n",
    "        u4x = x3.reshape(B, -1) @ net.W4.t() + net.b4\n",
    "        u4z = z3.reshape(B, -1) @ net.W4.t() + net.b4\n",
    "        sig4x = u4x\n",
    "        sig4z = u4z\n",
    "        F4x = sig4x - x4\n",
    "        F4z = sig4z - z4\n",
    "        F4  = 0.5 * (F4x + F4z)\n",
    "\n",
    "        # loss gradient at output (only) -- keep as-is (uses m4)\n",
    "        p = torch.softmax(m4, dim=1)\n",
    "        g4 = (p - y_onehot)  # ∇_{m4} C\n",
    "\n",
    "        # q_l = D_l ⊙ s_l  (D4 = I)\n",
    "        # keep as-is (uses u2/u3 from mean forward)\n",
    "        q2 = relu_prime(u2m) * s2\n",
    "        q3 = relu_prime(u3m) * s3\n",
    "        q4 = s4\n",
    "\n",
    "        # W^T q backprop terms (next-layer contributions)\n",
    "        WTq3 = (q4 @ net.W4).reshape(B, 128, 14, 14)\n",
    "        WTq2 = F.conv_transpose2d(q3, net.W3, bias=None, stride=2, padding=1, output_padding=1)\n",
    "        WTq1 = F.conv_transpose2d(q2, net.W2, bias=None, stride=1, padding=1)\n",
    "\n",
    "        # J_F(m)^T s = -s + W_{next}^T q_{next}\n",
    "        Jt1 = -s1 + WTq1\n",
    "        Jt2 = -s2 + WTq2\n",
    "        Jt3 = -s3 + WTq3\n",
    "        Jt4 = -s4  # no next layer\n",
    "\n",
    "        # Hamiltonian dynamics (beta defaults to 1)\n",
    "        dx1 = F1 + 0.5 * Jt1\n",
    "        dz1 = F1 - 0.5 * Jt1\n",
    "\n",
    "        dx2 = F2 + 0.5 * Jt2\n",
    "        dz2 = F2 - 0.5 * Jt2\n",
    "\n",
    "        dx3 = F3 + 0.5 * Jt3\n",
    "        dz3 = F3 - 0.5 * Jt3\n",
    "\n",
    "        dx4 = F4 + 0.5 * Jt4 + 0.5 * beta * g4\n",
    "        dz4 = F4 - 0.5 * Jt4 - 0.5 * beta * g4\n",
    "\n",
    "        # Euler update\n",
    "        x1.add_(dx1, alpha=eta); z1.add_(dz1, alpha=eta)\n",
    "        x2.add_(dx2, alpha=eta); z2.add_(dz2, alpha=eta)\n",
    "        x3.add_(dx3, alpha=eta); z3.add_(dz3, alpha=eta)\n",
    "        x4.add_(dx4, alpha=eta); z4.add_(dz4, alpha=eta)\n",
    "\n",
    "        # early stop\n",
    "        upd = (dx1.abs().mean() + dx2.abs().mean() + dx3.abs().mean() + dx4.abs().mean()).item()\n",
    "        if upd < tol:\n",
    "            break\n",
    "\n",
    "    # keep warm-start buffers\n",
    "    if state is not None and warm_start:\n",
    "        state.x1,state.z1,state.x2,state.z2,state.x3,state.z3,state.x4,state.z4 = x1,z1,x2,z2,x3,z3,x4,z4\n",
    "\n",
    "    # -------------------------\n",
    "    # Final readout for grads\n",
    "    # -------------------------\n",
    "    m1 = (x1 + z1) * 0.5; s1 = (x1 - z1)\n",
    "    m2 = (x2 + z2) * 0.5; s2 = (x2 - z2)\n",
    "    m3 = (x3 + z3) * 0.5; s3 = (x3 - z3)\n",
    "    m4 = (x4 + z4) * 0.5; s4 = (x4 - z4)\n",
    "\n",
    "    (u1, _), (u2, _), (u3, _), (_, _) = forward_u_sig(net, x0, m1, m2, m3)\n",
    "\n",
    "    delta1 = relu_prime(u1) * s1\n",
    "    delta2 = relu_prime(u2) * s2\n",
    "    delta3 = relu_prime(u3) * s3\n",
    "    delta4 = s4  # linear\n",
    "\n",
    "    dW1 = conv2d_weight(x0, net.W1.shape, delta1, stride=1, padding=1) / B\n",
    "    dW2 = conv2d_weight(m1, net.W2.shape, delta2, stride=1, padding=1) / B\n",
    "    dW3 = conv2d_weight(m2, net.W3.shape, delta3, stride=2, padding=1) / B\n",
    "\n",
    "    db1 = delta1.mean(dim=(0,2,3))\n",
    "    db2 = delta2.mean(dim=(0,2,3))\n",
    "    db3 = delta3.mean(dim=(0,2,3))\n",
    "\n",
    "    m3_flat = m3.reshape(B, -1)\n",
    "    dW4 = (delta4.t() @ m3_flat) / B\n",
    "    db4 = delta4.mean(dim=0)\n",
    "\n",
    "    ce = F.cross_entropy(m4, y).item()\n",
    "    return (dW1,dW2,dW3,dW4), (db1,db2,db3,db4), ce, steps_taken\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SGD + momentum + wd + clip + EMA\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def sgd_momentum_step(net: CNN3, gradsW, gradsb, vW, vb,\n",
    "                      lr=0.01, momentum=0.9, weight_decay=5e-4, clip=1.0):\n",
    "    dW1,dW2,dW3,dW4 = gradsW\n",
    "    db1,db2,db3,db4 = gradsb\n",
    "\n",
    "    # weight decay on weights only\n",
    "    dW1 = dW1 + weight_decay * net.W1\n",
    "    dW2 = dW2 + weight_decay * net.W2\n",
    "    dW3 = dW3 + weight_decay * net.W3\n",
    "    dW4 = dW4 + weight_decay * net.W4\n",
    "\n",
    "    # global norm clip\n",
    "    gn2 = float(dW1.norm()**2 + dW2.norm()**2 + dW3.norm()**2 + dW4.norm()**2 +\n",
    "                db1.norm()**2 + db2.norm()**2 + db3.norm()**2 + db4.norm()**2)\n",
    "    gn = gn2**0.5\n",
    "    scale = 1.0 if gn <= clip else (clip / (gn + 1e-12))\n",
    "    if scale != 1.0:\n",
    "        dW1,dW2,dW3,dW4 = dW1*scale, dW2*scale, dW3*scale, dW4*scale\n",
    "        db1,db2,db3,db4 = db1*scale, db2*scale, db3*scale, db4*scale\n",
    "\n",
    "    # momentum buffers\n",
    "    vW[0].mul_(momentum).add_(dW1); vb[0].mul_(momentum).add_(db1)\n",
    "    vW[1].mul_(momentum).add_(dW2); vb[1].mul_(momentum).add_(db2)\n",
    "    vW[2].mul_(momentum).add_(dW3); vb[2].mul_(momentum).add_(db3)\n",
    "    vW[3].mul_(momentum).add_(dW4); vb[3].mul_(momentum).add_(db4)\n",
    "\n",
    "    net.W1.sub_(lr * vW[0]); net.b1.sub_(lr * vb[0])\n",
    "    net.W2.sub_(lr * vW[1]); net.b2.sub_(lr * vb[1])\n",
    "    net.W3.sub_(lr * vW[2]); net.b3.sub_(lr * vb[2])\n",
    "    net.W4.sub_(lr * vW[3]); net.b4.sub_(lr * vb[3])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(ema_net: CNN3, net: CNN3, decay=0.999):\n",
    "    for name in [\"W1\",\"b1\",\"W2\",\"b2\",\"W3\",\"b3\",\"W4\",\"b4\"]:\n",
    "        a = getattr(ema_net, name)\n",
    "        b = getattr(net, name)\n",
    "        a.mul_(decay).add_(b, alpha=(1.0 - decay))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(net: CNN3, loader, device, max_batches=800):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x,y) in enumerate(loader):\n",
    "        if i >= max_batches: break\n",
    "        x = x.to(device); y = y.to(device)\n",
    "\n",
    "        h1 = relu(F.conv2d(x, net.W1, net.b1, stride=1, padding=1))\n",
    "        h2 = relu(F.conv2d(h1, net.W2, net.b2, stride=1, padding=1))\n",
    "        h3 = relu(F.conv2d(h2, net.W3, net.b3, stride=2, padding=1))\n",
    "        logits = h3.reshape(x.size(0), -1) @ net.W4.t() + net.b4\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "\n",
    "def cosine_lr(step, total_steps, lr_max=0.02, lr_min=2e-4):\n",
    "    t = step / max(1, total_steps)\n",
    "    return lr_min + 0.5*(lr_max - lr_min)*(1.0 + math.cos(math.pi * t))\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    # (optional but good for speed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    use_aug = True\n",
    "    if use_aug:\n",
    "        train_tfm = transforms.Compose([\n",
    "            transforms.RandomAffine(degrees=10, translate=(0.1,0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    else:\n",
    "        train_tfm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    test_tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_ds = datasets.MNIST(\"./data\", train=True, download=True, transform=train_tfm)\n",
    "    test_ds  = datasets.MNIST(\"./data\", train=False, download=True, transform=test_tfm)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # model init\n",
    "    torch.manual_seed(0)\n",
    "    init = 0.02\n",
    "    W1 = init * torch.randn(64, 1, 3, 3, device=device);   b1 = torch.zeros(64, device=device)\n",
    "    W2 = init * torch.randn(64, 64, 3, 3, device=device);  b2 = torch.zeros(64, device=device)\n",
    "    W3 = init * torch.randn(128, 64, 3, 3, device=device); b3 = torch.zeros(128, device=device)\n",
    "    W4 = init * torch.randn(10, 128*14*14, device=device); b4 = torch.zeros(10, device=device)\n",
    "\n",
    "    net = CNN3(W1,b1,W2,b2,W3,b3,W4,b4)\n",
    "    ema_net = CNN3(W1.clone(),b1.clone(),W2.clone(),b2.clone(),W3.clone(),b3.clone(),W4.clone(),b4.clone())\n",
    "\n",
    "    vW = [torch.zeros_like(net.W1), torch.zeros_like(net.W2), torch.zeros_like(net.W3), torch.zeros_like(net.W4)]\n",
    "    vb = [torch.zeros_like(net.b1), torch.zeros_like(net.b2), torch.zeros_like(net.b3), torch.zeros_like(net.b4)]\n",
    "\n",
    "    # hyperparams\n",
    "    epochs = 12\n",
    "    eta = 1\n",
    "    K = 30\n",
    "    tol = 1e-4        # <-- NEW: early-stop tolerance\n",
    "    momentum = 0.9\n",
    "    weight_decay = 5e-4\n",
    "    clip = 1.0\n",
    "    ema_decay = 0.999\n",
    "\n",
    "    total_steps = epochs * len(train_loader)\n",
    "    global_step = 0\n",
    "    eval_every = 400\n",
    "\n",
    "    ce_hist, acc_hist, step_hist = [], [], []\n",
    "\n",
    "    # -------------------------\n",
    "    # NEW: persistent warm-start buffers\n",
    "    # -------------------------\n",
    "    state = XZState()\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        running = 0.0\n",
    "        steps_sum = 0\n",
    "        steps_count = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            lr = cosine_lr(global_step, total_steps, lr_max=0.02, lr_min=2e-4)\n",
    "\n",
    "            # -------------------------\n",
    "            # CHANGED CALL: now returns ce + steps_taken\n",
    "            # -------------------------\n",
    "            gradsW, gradsb, ce, steps_taken = xz_relax_batch_grad(\n",
    "                net, x, y,\n",
    "                eta=eta, K=K,\n",
    "                state=state,\n",
    "                tol=tol,\n",
    "                warm_start=True\n",
    "            )\n",
    "\n",
    "            sgd_momentum_step(net, gradsW, gradsb, vW, vb,\n",
    "                              lr=lr, momentum=momentum, weight_decay=weight_decay, clip=clip)\n",
    "\n",
    "            ema_update(ema_net, net, decay=ema_decay)\n",
    "\n",
    "            global_step += 1\n",
    "            running += ce\n",
    "            ce_hist.append(ce)\n",
    "            step_hist.append(global_step)\n",
    "\n",
    "            steps_sum += steps_taken\n",
    "            steps_count += 1\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                acc = accuracy(ema_net, test_loader, device=device, max_batches=800)\n",
    "                acc_hist.append(acc)\n",
    "\n",
    "                avgK = steps_sum / max(1, steps_count)\n",
    "                print(f\"step {global_step}: lr={lr:.4g}  train-CE~{running/eval_every:.4f}  \"\n",
    "                      f\"EMA acc={acc*100:.2f}%  avg_relax_steps={avgK:.1f}\")\n",
    "                running = 0.0\n",
    "                steps_sum = 0\n",
    "                steps_count = 0\n",
    "\n",
    "        acc = accuracy(ema_net, test_loader, device=device, max_batches=800)\n",
    "        print(f\"END epoch {ep:02d}: EMA test-acc~{acc*100:.2f}%\")\n",
    "\n",
    "    final_acc = accuracy(ema_net, test_loader, device=device, max_batches=1200)\n",
    "    print(f\"Final EMA test-acc (approx): {final_acc*100:.2f}%\")\n",
    "\n",
    "    # plots\n",
    "    plt.figure()\n",
    "    win = 200\n",
    "    if len(ce_hist) >= win:\n",
    "        sm = [sum(ce_hist[i-win:i])/win for i in range(win, len(ce_hist)+1)]\n",
    "        plt.plot(step_hist[win-1:], sm)\n",
    "    else:\n",
    "        plt.plot(step_hist, ce_hist)\n",
    "    plt.xlabel(\"Update step\")\n",
    "    plt.ylabel(\"Train CE (moving avg)\")\n",
    "    plt.title(\"3-Conv CNN + x-z relaxation: training loss\")\n",
    "    plt.grid(True, linestyle=\":\")\n",
    "\n",
    "    plt.figure()\n",
    "    eval_steps = [eval_every*(i+1) for i in range(len(acc_hist))]\n",
    "    plt.plot(eval_steps, [100*a for a in acc_hist], marker=\"o\")\n",
    "    plt.xlabel(\"Update step\")\n",
    "    plt.ylabel(\"Test accuracy (%)\")\n",
    "    plt.title(\"3-Conv CNN + x-z relaxation: EMA test accuracy\")\n",
    "    plt.grid(True, linestyle=\":\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06592f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Training CNN9 (eta=0.75)...\n",
      "[Step 0] CosSim: 1.0000 | RelErr: 0.0045\n",
      "[Step 100] CosSim: 1.0000 | RelErr: 0.0066\n",
      "[Step 200] CosSim: 1.0000 | RelErr: 0.0087\n",
      "[Step 300] CosSim: 1.0000 | RelErr: 0.0077\n",
      "[Step 400] CosSim: 1.0000 | RelErr: 0.0083\n",
      "[Step 500] CosSim: 1.0000 | RelErr: 0.0092\n",
      "[Step 600] CosSim: 1.0000 | RelErr: 0.0093\n",
      "[Step 700] CosSim: 1.0000 | RelErr: 0.0079\n",
      ">>> Ep 1: Loss 1.6982 | ACC 34.13% | Avg K: 30.0\n",
      "[Step 800] CosSim: 1.0000 | RelErr: 0.0079\n",
      "[Step 900] CosSim: 0.9999 | RelErr: 0.0105\n",
      "[Step 1000] CosSim: 0.9999 | RelErr: 0.0101\n",
      "[Step 1100] CosSim: 0.9999 | RelErr: 0.0131\n",
      "[Step 1200] CosSim: 1.0000 | RelErr: 0.0058\n",
      "[Step 1300] CosSim: 0.9999 | RelErr: 0.0113\n",
      "[Step 1400] CosSim: 1.0000 | RelErr: 0.0068\n",
      "[Step 1500] CosSim: 0.9999 | RelErr: 0.0158\n",
      ">>> Ep 2: Loss 1.3525 | ACC 47.17% | Avg K: 30.0\n",
      "[Step 1600] CosSim: 0.9999 | RelErr: 0.0104\n",
      "[Step 1700] CosSim: 0.9999 | RelErr: 0.0140\n",
      "[Step 1800] CosSim: 0.9999 | RelErr: 0.0108\n",
      "[Step 1900] CosSim: 1.0000 | RelErr: 0.0086\n",
      "[Step 2000] CosSim: 1.0000 | RelErr: 0.0096\n",
      "[Step 2100] CosSim: 1.0000 | RelErr: 0.0061\n",
      "[Step 2200] CosSim: 0.9999 | RelErr: 0.0109\n",
      "[Step 2300] CosSim: 1.0000 | RelErr: 0.0049\n",
      ">>> Ep 3: Loss 1.1780 | ACC 55.35% | Avg K: 30.0\n",
      "[Step 2400] CosSim: 1.0000 | RelErr: 0.0094\n",
      "[Step 2500] CosSim: 1.0000 | RelErr: 0.0071\n",
      "[Step 2600] CosSim: 0.9999 | RelErr: 0.0112\n",
      "[Step 2700] CosSim: 0.9999 | RelErr: 0.0121\n",
      "[Step 2800] CosSim: 1.0000 | RelErr: 0.0084\n",
      "[Step 2900] CosSim: 1.0000 | RelErr: 0.0070\n",
      "[Step 3000] CosSim: 1.0000 | RelErr: 0.0087\n",
      "[Step 3100] CosSim: 1.0000 | RelErr: 0.0079\n",
      ">>> Ep 4: Loss 1.0376 | ACC 62.97% | Avg K: 30.0\n",
      "[Step 3200] CosSim: 1.0000 | RelErr: 0.0063\n",
      "[Step 3300] CosSim: 1.0000 | RelErr: 0.0082\n",
      "[Step 3400] CosSim: 0.9999 | RelErr: 0.0107\n",
      "[Step 3500] CosSim: 1.0000 | RelErr: 0.0079\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 535\u001b[39m\n\u001b[32m    532\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_acc*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 487\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    484\u001b[39m y = y.to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    485\u001b[39m lr = cosine_lr(global_step, total_steps, lr_max=lr_max, lr_min=lr_min)\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m gradsW, gradsb, ce, steps_taken = \u001b[43mxz_relax_batch_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m=\u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    489\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: state = XZState(x.shape[\u001b[32m0\u001b[39m], device)\n\u001b[32m    492\u001b[39m \u001b[38;5;66;03m# Fidelity Check\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 169\u001b[39m, in \u001b[36mxz_relax_batch_grad\u001b[39m\u001b[34m(net, x0, y, eta, K, state, tol, warm_start, beta)\u001b[39m\n\u001b[32m    167\u001b[39m     x[i].add_(dx, alpha=eta)\n\u001b[32m    168\u001b[39m     z[i].add_(dz, alpha=eta)\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     total_change += \u001b[43mdx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.item()\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# Layer 8 (Output)\u001b[39;00m\n\u001b[32m    172\u001b[39m dx8 = F_err[\u001b[32m8\u001b[39m] + \u001b[32m0.5\u001b[39m * Jt[\u001b[32m8\u001b[39m] + \u001b[32m0.5\u001b[39m * beta * g_last\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from dataclasses import dataclass\n",
    "from torch.nn.grad import conv2d_weight\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Activations (BN REMOVED to restore Fidelity)\n",
    "# -----------------------------\n",
    "def relu(u): return torch.relu(u)\n",
    "def relu_prime(u): return (u > 0).to(u.dtype)\n",
    "\n",
    "# -----------------------------\n",
    "# Model container (8 Conv + 1 FC = 9 Layers)\n",
    "# Structure: [64-64] -> [128-128] -> [256-256] -> [512-512] -> Linear\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class CNN9:\n",
    "    # Block 1: 32x32 -> 16x16\n",
    "    W1: torch.Tensor; b1: torch.Tensor   # (64, 3, 3, 3)   s=1\n",
    "    W2: torch.Tensor; b2: torch.Tensor   # (64, 64, 3, 3)  s=2 (pool)\n",
    "    \n",
    "    # Block 2: 16x16 -> 8x8\n",
    "    W3: torch.Tensor; b3: torch.Tensor   # (128, 64, 3, 3) s=1\n",
    "    W4: torch.Tensor; b4: torch.Tensor   # (128, 128, 3, 3) s=2 (pool)\n",
    "    \n",
    "    # Block 3: 8x8 -> 4x4\n",
    "    W5: torch.Tensor; b5: torch.Tensor   # (256, 128, 3, 3) s=1\n",
    "    W6: torch.Tensor; b6: torch.Tensor   # (256, 256, 3, 3) s=2 (pool)\n",
    "    \n",
    "    # Block 4: 4x4 -> 2x2\n",
    "    W7: torch.Tensor; b7: torch.Tensor   # (512, 256, 3, 3) s=1\n",
    "    W8: torch.Tensor; b8: torch.Tensor   # (512, 512, 3, 3) s=2 (pool)\n",
    "    \n",
    "    # Classifier: 2x2 -> Flat\n",
    "    W9: torch.Tensor; b9: torch.Tensor   # (10, 512*2*2)\n",
    "\n",
    "    @property\n",
    "    def device(self): return self.W1.device\n",
    "\n",
    "# -----------------------------\n",
    "# Forward at mean states (Standard Conv -> ReLU)\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def forward_u_sig(net: CNN9, x0, m):\n",
    "    # m is a list of [m1, m2, ..., m8]\n",
    "    u = [None] * 9\n",
    "    sig = [None] * 9\n",
    "    \n",
    "    # Block 1\n",
    "    u[0] = F.conv2d(x0, net.W1, net.b1, stride=1, padding=1);   sig[0] = relu(u[0])\n",
    "    u[1] = F.conv2d(m[0], net.W2, net.b2, stride=2, padding=1); sig[1] = relu(u[1])\n",
    "    \n",
    "    # Block 2\n",
    "    u[2] = F.conv2d(m[1], net.W3, net.b3, stride=1, padding=1); sig[2] = relu(u[2])\n",
    "    u[3] = F.conv2d(m[2], net.W4, net.b4, stride=2, padding=1); sig[3] = relu(u[3])\n",
    "    \n",
    "    # Block 3\n",
    "    u[4] = F.conv2d(m[3], net.W5, net.b5, stride=1, padding=1); sig[4] = relu(u[4])\n",
    "    u[5] = F.conv2d(m[4], net.W6, net.b6, stride=2, padding=1); sig[5] = relu(u[5])\n",
    "    \n",
    "    # Block 4\n",
    "    u[6] = F.conv2d(m[5], net.W7, net.b7, stride=1, padding=1); sig[6] = relu(u[6])\n",
    "    u[7] = F.conv2d(m[6], net.W8, net.b8, stride=2, padding=1); sig[7] = relu(u[7])\n",
    "    \n",
    "    # FC\n",
    "    B = x0.shape[0]\n",
    "    m8_flat = m[7].reshape(B, -1)\n",
    "    u[8] = m8_flat @ net.W9.t() + net.b9\n",
    "    sig[8] = u[8] \n",
    "\n",
    "    return u, sig\n",
    "\n",
    "class XZState:\n",
    "    def __init__(self, B, device):\n",
    "        self.dims = [\n",
    "            (B, 64, 32, 32), (B, 64, 16, 16),\n",
    "            (B, 128, 16, 16), (B, 128, 8, 8),\n",
    "            (B, 256, 8, 8),   (B, 256, 4, 4),\n",
    "            (B, 512, 4, 4),   (B, 512, 2, 2),\n",
    "            (B, 10)\n",
    "        ]\n",
    "        self.x = [torch.zeros(d, device=device) for d in self.dims]\n",
    "        self.z = [torch.zeros(d, device=device) for d in self.dims]\n",
    "\n",
    "    def reset(self, B, device):\n",
    "        if self.x[0].shape[0] != B:\n",
    "            self.__init__(B, device)\n",
    "\n",
    "# -----------------------------\n",
    "# Hamiltonian x-z relaxation gradient for batch (CNN9)\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def xz_relax_batch_grad(\n",
    "    net: CNN9, x0, y,\n",
    "    eta=1.0, K=25,\n",
    "    state: XZState | None = None,\n",
    "    tol: float = 1e-8,\n",
    "    warm_start: bool = True,\n",
    "    beta: float = 1.0,\n",
    "):\n",
    "    device = net.device\n",
    "    B = x0.shape[0]\n",
    "    num_layers = 9\n",
    "    \n",
    "    # --- CHANGE 1: Label Smoothing (Preserves Fidelity, improves Acc) ---\n",
    "    eps_ls = 0.1\n",
    "    y_onehot = F.one_hot(y, num_classes=10).to(x0.dtype)\n",
    "    y_smooth = (1.0 - eps_ls) * y_onehot + eps_ls / 10.0\n",
    "\n",
    "    # Initialize State\n",
    "    if state is None:\n",
    "        state = XZState(B, device)\n",
    "    else:\n",
    "        state.reset(B, device)\n",
    "    \n",
    "    x = state.x; z = state.z\n",
    "    \n",
    "    steps_taken = 0\n",
    "    for _ in range(K):\n",
    "        steps_taken += 1\n",
    "        \n",
    "        m = [(xi + zi) * 0.5 for xi, zi in zip(x, z)]\n",
    "        s = [(xi - zi) for xi, zi in zip(x, z)]\n",
    "        \n",
    "        u, sig = forward_u_sig(net, x0, m)\n",
    "\n",
    "        F_err = [(si - mi) for si, mi in zip(sig, m)]\n",
    "        \n",
    "        p = torch.softmax(m[8], dim=1)\n",
    "        \n",
    "        # Use smoothed labels for gradient\n",
    "        g_last = (p - y_smooth)\n",
    "        \n",
    "        q = [None] * num_layers\n",
    "        q[8] = s[8] \n",
    "        for i in range(7, -1, -1):\n",
    "            q[i] = relu_prime(u[i]) * s[i]\n",
    "\n",
    "        Jt = [None] * num_layers\n",
    "        Jt[8] = -s[8] \n",
    "\n",
    "        # Jt for Layer 8\n",
    "        WTq8 = (q[8] @ net.W9).reshape(B, 512, 2, 2)\n",
    "        Jt[7] = -s[7] + WTq8\n",
    "        \n",
    "        # Jt for Conv Layers\n",
    "        Jt[6] = -s[6] + F.conv_transpose2d(q[7], net.W8, stride=2, padding=1, output_padding=1)\n",
    "        Jt[5] = -s[5] + F.conv_transpose2d(q[6], net.W7, stride=1, padding=1)\n",
    "        Jt[4] = -s[4] + F.conv_transpose2d(q[5], net.W6, stride=2, padding=1, output_padding=1)\n",
    "        Jt[3] = -s[3] + F.conv_transpose2d(q[4], net.W5, stride=1, padding=1)\n",
    "        Jt[2] = -s[2] + F.conv_transpose2d(q[3], net.W4, stride=2, padding=1, output_padding=1)\n",
    "        Jt[1] = -s[1] + F.conv_transpose2d(q[2], net.W3, stride=1, padding=1)\n",
    "        Jt[0] = -s[0] + F.conv_transpose2d(q[1], net.W2, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        total_change = 0.0\n",
    "        # Layers 0 to 7\n",
    "        for i in range(8):\n",
    "            dx = F_err[i] + 0.5 * Jt[i]\n",
    "            dz = F_err[i] - 0.5 * Jt[i]\n",
    "            x[i].add_(dx, alpha=eta)\n",
    "            z[i].add_(dz, alpha=eta)\n",
    "            total_change += dx.abs().mean().item()\n",
    "            \n",
    "        # Layer 8 (Output)\n",
    "        dx8 = F_err[8] + 0.5 * Jt[8] + 0.5 * beta * g_last\n",
    "        dz8 = F_err[8] - 0.5 * Jt[8] - 0.5 * beta * g_last\n",
    "        x[8].add_(dx8, alpha=eta)\n",
    "        z[8].add_(dz8, alpha=eta)\n",
    "        total_change += dx8.abs().mean().item()\n",
    "\n",
    "        if total_change < tol:\n",
    "            break\n",
    "\n",
    "    # Compute Gradients\n",
    "    m = [(xi + zi) * 0.5 for xi, zi in zip(x, z)]\n",
    "    s = [(xi - zi) for xi, zi in zip(x, z)]\n",
    "    u, _ = forward_u_sig(net, x0, m)\n",
    "    \n",
    "    delta = [None] * num_layers\n",
    "    delta[8] = s[8]\n",
    "    for i in range(8):\n",
    "        delta[i] = relu_prime(u[i]) * s[i]\n",
    "\n",
    "    gradsW = []\n",
    "    gradsb = []\n",
    "\n",
    "    # W1 (stride 1)\n",
    "    gradsW.append(conv2d_weight(x0, net.W1.shape, delta[0], stride=1, padding=1) / B)\n",
    "    gradsb.append(delta[0].sum(dim=(0,2,3)) / B)\n",
    "    # W2 (stride 2)\n",
    "    gradsW.append(conv2d_weight(m[0], net.W2.shape, delta[1], stride=2, padding=1) / B)\n",
    "    gradsb.append(delta[1].sum(dim=(0,2,3)) / B)\n",
    "    # W3 (stride 1)\n",
    "    gradsW.append(conv2d_weight(m[1], net.W3.shape, delta[2], stride=1, padding=1) / B)\n",
    "    gradsb.append(delta[2].sum(dim=(0,2,3)) / B)\n",
    "    # W4 (stride 2)\n",
    "    gradsW.append(conv2d_weight(m[2], net.W4.shape, delta[3], stride=2, padding=1) / B)\n",
    "    gradsb.append(delta[3].sum(dim=(0,2,3)) / B)\n",
    "    # W5 (stride 1)\n",
    "    gradsW.append(conv2d_weight(m[3], net.W5.shape, delta[4], stride=1, padding=1) / B)\n",
    "    gradsb.append(delta[4].sum(dim=(0,2,3)) / B)\n",
    "    # W6 (stride 2)\n",
    "    gradsW.append(conv2d_weight(m[4], net.W6.shape, delta[5], stride=2, padding=1) / B)\n",
    "    gradsb.append(delta[5].sum(dim=(0,2,3)) / B)\n",
    "    # W7 (stride 1)\n",
    "    gradsW.append(conv2d_weight(m[5], net.W7.shape, delta[6], stride=1, padding=1) / B)\n",
    "    gradsb.append(delta[6].sum(dim=(0,2,3)) / B)\n",
    "    # W8 (stride 2)\n",
    "    gradsW.append(conv2d_weight(m[6], net.W8.shape, delta[7], stride=2, padding=1) / B)\n",
    "    gradsb.append(delta[7].sum(dim=(0,2,3)) / B)\n",
    "    # W9 (Linear)\n",
    "    m7_flat = m[7].reshape(B, -1)\n",
    "    gradsW.append((delta[8].t() @ m7_flat) / B)\n",
    "    gradsb.append(delta[8].mean(dim=0))\n",
    "\n",
    "    ce = F.cross_entropy(m[8], y).item()\n",
    "    return tuple(gradsW), tuple(gradsb), ce, steps_taken\n",
    "\n",
    "# -----------------------------\n",
    "# Autograd Reference (Standard CNN9, No BN to match Relaxation)\n",
    "# -----------------------------\n",
    "# -----------------------------\n",
    "# Autograd Reference (Now matches Label Smoothing)\n",
    "# -----------------------------\n",
    "def autograd_grads_like_cnn9(net: CNN9, x, y):\n",
    "    params = {}\n",
    "    for i in range(1, 10):\n",
    "        params[f\"W{i}\"] = getattr(net, f\"W{i}\").detach().clone().requires_grad_(True)\n",
    "        params[f\"b{i}\"] = getattr(net, f\"b{i}\").detach().clone().requires_grad_(True)\n",
    "    \n",
    "    h = x\n",
    "    h = relu(F.conv2d(h, params['W1'], params['b1'], stride=1, padding=1))\n",
    "    h = relu(F.conv2d(h, params['W2'], params['b2'], stride=2, padding=1))\n",
    "    h = relu(F.conv2d(h, params['W3'], params['b3'], stride=1, padding=1))\n",
    "    h = relu(F.conv2d(h, params['W4'], params['b4'], stride=2, padding=1))\n",
    "    h = relu(F.conv2d(h, params['W5'], params['b5'], stride=1, padding=1))\n",
    "    h = relu(F.conv2d(h, params['W6'], params['b6'], stride=2, padding=1))\n",
    "    h = relu(F.conv2d(h, params['W7'], params['b7'], stride=1, padding=1))\n",
    "    h = relu(F.conv2d(h, params['W8'], params['b8'], stride=2, padding=1))\n",
    "    logits = h.reshape(x.size(0), -1) @ params['W9'].t() + params['b9']\n",
    "    \n",
    "    # --- FIX: Match the relaxation's label smoothing (eps=0.1) ---\n",
    "    loss = F.cross_entropy(logits, y, label_smoothing=0.1)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    gradsW = tuple(params[f\"W{i}\"].grad for i in range(1, 10))\n",
    "    gradsb = tuple(params[f\"b{i}\"].grad for i in range(1, 10))\n",
    "    return gradsW, gradsb, float(loss.detach())\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics & Plotting\n",
    "# -----------------------------\n",
    "def flat_cat(tup):\n",
    "    return torch.cat([t.reshape(-1) for t in tup], dim=0)\n",
    "\n",
    "def cos_sim(a, b, eps=1e-12):\n",
    "    denom = (a.norm() * b.norm()).clamp_min(eps)\n",
    "    return float((a @ b) / denom)\n",
    "\n",
    "def relative_error(a, b, eps=1e-12):\n",
    "    return float((a - b).norm() / b.norm().clamp_min(eps))\n",
    "\n",
    "def set_style():\n",
    "    mpl.rcParams.update({\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times New Roman\", \"DejaVu Serif\"],\n",
    "        \"font.size\": 14,\n",
    "        \"axes.labelsize\": 16,\n",
    "        \"axes.titlesize\": 18,\n",
    "        \"lines.linewidth\": 2.5,\n",
    "        \"figure.figsize\": (8, 6),\n",
    "    })\n",
    "\n",
    "def plot_results_icml(results, eta_values):\n",
    "    set_style()\n",
    "    plt.figure()\n",
    "    styles = {\n",
    "        0.8: {'color': '#d62728', 'marker': 'o', 'linestyle': '-'},\n",
    "        1.0: {'color': '#1f77b4', 'marker': 's', 'linestyle': '--'},\n",
    "        1.2: {'color': '#2ca02c', 'marker': '^', 'linestyle': '-.'},\n",
    "    }\n",
    "    for eta in eta_values:\n",
    "        if eta not in results: continue\n",
    "        data = results[eta]\n",
    "        cos_steps = data['cos_steps']\n",
    "        rel_err = data['relerr_globalW_hist']\n",
    "        if len(cos_steps) == 0: continue\n",
    "        style = styles.get(eta, {'color': 'black', 'marker': 'x', 'linestyle': ':'})\n",
    "        plt.plot(cos_steps, rel_err, label=fr'$\\eta={eta}$', **style)\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel(r'Relative Grad Error')\n",
    "    plt.title('Gradient Fidelity (CNN9)')\n",
    "    plt.grid(True, which=\"both\", ls=\":\", alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cnn9_grad_fidelity.png', dpi=300)\n",
    "    print(\"Saved cnn9_grad_fidelity.png\")\n",
    "\n",
    "def plot_convergence_icml(results, eta_values):\n",
    "    set_style()\n",
    "    plt.figure()\n",
    "    styles = {\n",
    "        0.8: {'color': '#d62728', 'marker': 'o', 'linestyle': '-'},\n",
    "        1.0: {'color': '#1f77b4', 'marker': 's', 'linestyle': '--'},\n",
    "        1.2: {'color': '#2ca02c', 'marker': '^', 'linestyle': '-.'},\n",
    "    }\n",
    "    for eta in eta_values:\n",
    "        if eta not in results: continue\n",
    "        data = results[eta]\n",
    "        avg_steps = data['avg_steps_per_epoch']\n",
    "        epochs = range(1, len(avg_steps) + 1)\n",
    "        style = styles.get(eta, {'color': 'black', 'marker': 'x', 'linestyle': ':'})\n",
    "        plt.plot(epochs, avg_steps, label=fr'$\\eta={eta}$', **style)\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Avg. Convergence Steps ($K$)')\n",
    "    plt.title('Relaxation Convergence Speed (CNN9)')\n",
    "    plt.grid(True, which=\"both\", ls=\":\", alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cnn9_convergence_steps.png', dpi=300)\n",
    "    print(\"Saved cnn9_convergence_steps.png\")\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def sgd_momentum_step(net: CNN9, gradsW, gradsb, vW, vb,\n",
    "                      lr=0.01, momentum=0.9, weight_decay=5e-4, clip=1.0):\n",
    "    for i in range(9):\n",
    "        dWi = gradsW[i] + weight_decay * getattr(net, f\"W{i+1}\")\n",
    "        dbi = gradsb[i]\n",
    "        \n",
    "        gn = (dWi.norm()**2 + dbi.norm()**2)**0.5\n",
    "        scale = 1.0 if gn <= clip else (clip / (gn + 1e-12))\n",
    "        dWi *= scale; dbi *= scale\n",
    "\n",
    "        vW[i].mul_(momentum).add_(dWi)\n",
    "        vb[i].mul_(momentum).add_(dbi)\n",
    "\n",
    "        getattr(net, f\"W{i+1}\").sub_(lr * vW[i])\n",
    "        getattr(net, f\"b{i+1}\").sub_(lr * vb[i])\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(ema_net: CNN9, net: CNN9, decay=0.999):\n",
    "    for i in range(1, 10):\n",
    "        for param in [\"W\", \"b\"]:\n",
    "            name = f\"{param}{i}\"\n",
    "            getattr(ema_net, name).mul_(decay).add_(getattr(net, name), alpha=(1.0 - decay))\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(net: CNN9, loader, device, max_batches=800):\n",
    "    correct = 0; total = 0\n",
    "    for i, (x,y) in enumerate(loader):\n",
    "        if i >= max_batches: break\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        h = x\n",
    "        h = relu(F.conv2d(h, net.W1, net.b1, stride=1, padding=1))\n",
    "        h = relu(F.conv2d(h, net.W2, net.b2, stride=2, padding=1))\n",
    "        h = relu(F.conv2d(h, net.W3, net.b3, stride=1, padding=1))\n",
    "        h = relu(F.conv2d(h, net.W4, net.b4, stride=2, padding=1))\n",
    "        h = relu(F.conv2d(h, net.W5, net.b5, stride=1, padding=1))\n",
    "        h = relu(F.conv2d(h, net.W6, net.b6, stride=2, padding=1))\n",
    "        h = relu(F.conv2d(h, net.W7, net.b7, stride=1, padding=1))\n",
    "        h = relu(F.conv2d(h, net.W8, net.b8, stride=2, padding=1))\n",
    "        logits = h.reshape(x.size(0), -1) @ net.W9.t() + net.b9\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "def kaiming_init(shape, device):\n",
    "    fan_in = (shape[1] * shape[2] * shape[3]) if len(shape) == 4 else shape[1]\n",
    "    return math.sqrt(2.0 / fan_in) * torch.randn(shape, device=device)\n",
    "\n",
    "def cosine_lr(step, total_steps, lr_max=0.05, lr_min=5e-4):\n",
    "    t = step / max(1, total_steps)\n",
    "    return lr_min + 0.5*(lr_max - lr_min)*(1.0 + math.cos(math.pi * t))\n",
    "\n",
    "# -----------------------------\n",
    "# Data Augmentation (Cutout - KEEPING THIS)\n",
    "# -----------------------------\n",
    "class Cutout(object):\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        _, h, w = img.shape\n",
    "        mask = torch.ones((h, w), dtype=torch.float32)\n",
    "        y = torch.randint(h, (1,)).item()\n",
    "        x = torch.randint(w, (1,)).item()\n",
    "        y1 = max(0, y - self.length // 2)\n",
    "        y2 = min(h, y + self.length // 2)\n",
    "        x1 = max(0, x - self.length // 2)\n",
    "        x2 = min(w, x + self.length // 2)\n",
    "        img[:, y1:y2, x1:x2] = 0.0\n",
    "        return img\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "    cifar_std  = (0.2470, 0.2435, 0.2616)\n",
    "    \n",
    "    # --- CHANGE 2: Cutout (Safe for gradient fidelity) ---\n",
    "    train_tfm = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cifar_mean, cifar_std),\n",
    "        Cutout(length=8)\n",
    "    ])\n",
    "    test_tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cifar_mean, cifar_std)\n",
    "    ])\n",
    "    \n",
    "    train_ds = datasets.CIFAR10(\"./data\", train=True, download=True, transform=train_tfm)\n",
    "    test_ds  = datasets.CIFAR10(\"./data\", train=False, download=True, transform=test_tfm)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    epochs = 100\n",
    "    K = 30\n",
    "    lr_max = 0.033 # Back to slightly safer LR without BN\n",
    "    lr_min = 0.0002\n",
    "    eta_values = [0.75] \n",
    "    compare_every = 100\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for eta in eta_values:\n",
    "        print(f\"\\nTraining CNN9 (eta={eta})...\")\n",
    "        \n",
    "        W1 = kaiming_init((64, 3, 3, 3), device);   b1 = torch.zeros(64, device=device)\n",
    "        W2 = kaiming_init((64, 64, 3, 3), device);  b2 = torch.zeros(64, device=device)\n",
    "        W3 = kaiming_init((128, 64, 3, 3), device);  b3 = torch.zeros(128, device=device)\n",
    "        W4 = kaiming_init((128, 128, 3, 3), device); b4 = torch.zeros(128, device=device)\n",
    "        W5 = kaiming_init((256, 128, 3, 3), device); b5 = torch.zeros(256, device=device)\n",
    "        W6 = kaiming_init((256, 256, 3, 3), device); b6 = torch.zeros(256, device=device)\n",
    "        W7 = kaiming_init((512, 256, 3, 3), device); b7 = torch.zeros(512, device=device)\n",
    "        W8 = kaiming_init((512, 512, 3, 3), device); b8 = torch.zeros(512, device=device)\n",
    "        W9 = kaiming_init((10, 512*2*2), device);    b9 = torch.zeros(10, device=device)\n",
    "\n",
    "        net = CNN9(W1,b1,W2,b2,W3,b3,W4,b4,W5,b5,W6,b6,W7,b7,W8,b8,W9,b9)\n",
    "        ema_net = CNN9(W1.clone(),b1.clone(),W2.clone(),b2.clone(),W3.clone(),b3.clone(),\n",
    "                       W4.clone(),b4.clone(),W5.clone(),b5.clone(),W6.clone(),b6.clone(),\n",
    "                       W7.clone(),b7.clone(),W8.clone(),b8.clone(),W9.clone(),b9.clone())\n",
    "\n",
    "        vW = [torch.zeros_like(getattr(net, f\"W{i}\")) for i in range(1,10)]\n",
    "        vb = [torch.zeros_like(getattr(net, f\"b{i}\")) for i in range(1,10)]\n",
    "\n",
    "        total_steps = epochs * len(train_loader)\n",
    "        global_step = 0\n",
    "        state = None \n",
    "        \n",
    "        cos_globalW_hist = []\n",
    "        relerr_globalW_hist = []\n",
    "        cos_steps = []\n",
    "        avg_steps_per_epoch = []\n",
    "\n",
    "        for ep in range(1, epochs+1):\n",
    "            running_ce = 0.0\n",
    "            epoch_steps_accum = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for i, (x, y) in enumerate(train_loader):\n",
    "                x = x.to(device, non_blocking=True)\n",
    "                y = y.to(device, non_blocking=True)\n",
    "                lr = cosine_lr(global_step, total_steps, lr_max=lr_max, lr_min=lr_min)\n",
    "\n",
    "                gradsW, gradsb, ce, steps_taken = xz_relax_batch_grad(\n",
    "                    net, x, y, eta=eta, K=K, state=state, warm_start=True\n",
    "                )\n",
    "                if state is None: state = XZState(x.shape[0], device)\n",
    "\n",
    "                # Fidelity Check\n",
    "                if global_step % compare_every == 0:\n",
    "                    gradsW_ag, _, _ = autograd_grads_like_cnn9(net, x, y)\n",
    "                    gx = flat_cat(gradsW)\n",
    "                    ga = flat_cat(gradsW_ag)\n",
    "                    c_sim = cos_sim(gx, ga)\n",
    "                    r_err = relative_error(gx, ga)\n",
    "                    cos_globalW_hist.append(c_sim)\n",
    "                    relerr_globalW_hist.append(r_err)\n",
    "                    cos_steps.append(global_step)\n",
    "                    print(f\"[Step {global_step}] CosSim: {c_sim:.4f} | RelErr: {r_err:.4f}\")\n",
    "\n",
    "                sgd_momentum_step(net, gradsW, gradsb, vW, vb, lr=lr)\n",
    "                \n",
    "                # --- CHANGE 3: EMA decay higher ---\n",
    "                ema_update(ema_net, net, decay=0.9995)\n",
    "\n",
    "                global_step += 1\n",
    "                running_ce += ce\n",
    "                epoch_steps_accum += steps_taken\n",
    "                num_batches += 1\n",
    "\n",
    "            test_acc = accuracy(ema_net, test_loader, device)\n",
    "            train_loss = running_ce / num_batches\n",
    "            avg_k = epoch_steps_accum / num_batches\n",
    "            avg_steps_per_epoch.append(avg_k)\n",
    "            print(f\">>> Ep {ep}: Loss {train_loss:.4f} | ACC {test_acc*100:.2f}% | Avg K: {avg_k:.1f}\")\n",
    "\n",
    "        results[eta] = {\n",
    "            'cos_globalW_hist': cos_globalW_hist,\n",
    "            'relerr_globalW_hist': relerr_globalW_hist,\n",
    "            'cos_steps': cos_steps,\n",
    "            'avg_steps_per_epoch': avg_steps_per_epoch\n",
    "        }\n",
    "\n",
    "    print(\"Generating Plots...\")\n",
    "    plot_results_icml(results, eta_values)\n",
    "    plot_convergence_icml(results, eta_values)\n",
    "    \n",
    "    final_acc = accuracy(ema_net, test_loader, device, max_batches=2000)\n",
    "    print(f\"\\nFinal Test Accuracy: {final_acc*100:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a75295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Starting training with Standard Backprop...\n",
      "Architecture: CNN9 (PyTorch Module)\n",
      "Epochs: 100 | Batch: 64 | LR: 0.035->0.0002\n",
      "------------------------------------------------------------\n",
      "Ep   1/100 | Loss: 1.8874 | LR: 0.03499 | Test Acc (EMA): 25.86%\n",
      "Ep   2/100 | Loss: 1.6094 | LR: 0.03497 | Test Acc (EMA): 36.63%\n",
      "Ep   3/100 | Loss: 1.4555 | LR: 0.03492 | Test Acc (EMA): 48.04%\n",
      "Ep   4/100 | Loss: 1.3438 | LR: 0.03486 | Test Acc (EMA): 57.27%\n",
      "Ep   5/100 | Loss: 1.2633 | LR: 0.03479 | Test Acc (EMA): 64.45%\n",
      "Ep   6/100 | Loss: 1.2070 | LR: 0.03469 | Test Acc (EMA): 69.80%\n",
      "Ep   7/100 | Loss: 1.1574 | LR: 0.03458 | Test Acc (EMA): 73.59%\n",
      "Ep   8/100 | Loss: 1.1177 | LR: 0.03445 | Test Acc (EMA): 76.31%\n",
      "Ep   9/100 | Loss: 1.0984 | LR: 0.03431 | Test Acc (EMA): 78.49%\n",
      "Ep  10/100 | Loss: 1.0712 | LR: 0.03415 | Test Acc (EMA): 80.17%\n",
      "Ep  11/100 | Loss: 1.0470 | LR: 0.03397 | Test Acc (EMA): 81.59%\n",
      "Ep  12/100 | Loss: 1.0343 | LR: 0.03378 | Test Acc (EMA): 82.24%\n",
      "Ep  13/100 | Loss: 1.0190 | LR: 0.03357 | Test Acc (EMA): 83.18%\n",
      "Ep  14/100 | Loss: 1.0075 | LR: 0.03334 | Test Acc (EMA): 83.75%\n",
      "Ep  15/100 | Loss: 0.9887 | LR: 0.03310 | Test Acc (EMA): 84.34%\n",
      "Ep  16/100 | Loss: 0.9806 | LR: 0.03285 | Test Acc (EMA): 84.62%\n",
      "Ep  17/100 | Loss: 0.9642 | LR: 0.03258 | Test Acc (EMA): 85.32%\n",
      "Ep  18/100 | Loss: 0.9657 | LR: 0.03229 | Test Acc (EMA): 85.59%\n",
      "Ep  19/100 | Loss: 0.9504 | LR: 0.03199 | Test Acc (EMA): 85.99%\n",
      "Ep  20/100 | Loss: 0.9413 | LR: 0.03168 | Test Acc (EMA): 86.28%\n",
      "Ep  21/100 | Loss: 0.9313 | LR: 0.03135 | Test Acc (EMA): 86.70%\n",
      "Ep  22/100 | Loss: 0.9223 | LR: 0.03101 | Test Acc (EMA): 87.05%\n",
      "Ep  23/100 | Loss: 0.9186 | LR: 0.03065 | Test Acc (EMA): 87.19%\n",
      "Ep  24/100 | Loss: 0.9119 | LR: 0.03028 | Test Acc (EMA): 87.46%\n",
      "Ep  25/100 | Loss: 0.9046 | LR: 0.02990 | Test Acc (EMA): 87.83%\n",
      "Ep  26/100 | Loss: 0.9001 | LR: 0.02951 | Test Acc (EMA): 87.94%\n",
      "Ep  27/100 | Loss: 0.8930 | LR: 0.02911 | Test Acc (EMA): 88.19%\n",
      "Ep  28/100 | Loss: 0.8843 | LR: 0.02869 | Test Acc (EMA): 88.37%\n",
      "Ep  29/100 | Loss: 0.8762 | LR: 0.02827 | Test Acc (EMA): 88.56%\n",
      "Ep  30/100 | Loss: 0.8777 | LR: 0.02783 | Test Acc (EMA): 88.72%\n",
      "Ep  31/100 | Loss: 0.8682 | LR: 0.02738 | Test Acc (EMA): 88.91%\n",
      "Ep  32/100 | Loss: 0.8602 | LR: 0.02692 | Test Acc (EMA): 88.97%\n",
      "Ep  33/100 | Loss: 0.8582 | LR: 0.02646 | Test Acc (EMA): 88.90%\n",
      "Ep  34/100 | Loss: 0.8507 | LR: 0.02598 | Test Acc (EMA): 89.19%\n",
      "Ep  35/100 | Loss: 0.8422 | LR: 0.02550 | Test Acc (EMA): 89.17%\n",
      "Ep  36/100 | Loss: 0.8418 | LR: 0.02501 | Test Acc (EMA): 89.25%\n",
      "Ep  37/100 | Loss: 0.8372 | LR: 0.02451 | Test Acc (EMA): 89.32%\n",
      "Ep  38/100 | Loss: 0.8310 | LR: 0.02401 | Test Acc (EMA): 89.48%\n",
      "Ep  39/100 | Loss: 0.8210 | LR: 0.02349 | Test Acc (EMA): 89.48%\n",
      "Ep  40/100 | Loss: 0.8187 | LR: 0.02298 | Test Acc (EMA): 89.47%\n",
      "Ep  41/100 | Loss: 0.8111 | LR: 0.02246 | Test Acc (EMA): 89.60%\n",
      "Ep  42/100 | Loss: 0.8087 | LR: 0.02193 | Test Acc (EMA): 89.72%\n",
      "Ep  43/100 | Loss: 0.8051 | LR: 0.02140 | Test Acc (EMA): 89.82%\n",
      "Ep  44/100 | Loss: 0.7936 | LR: 0.02086 | Test Acc (EMA): 89.97%\n",
      "Ep  45/100 | Loss: 0.7922 | LR: 0.02032 | Test Acc (EMA): 90.01%\n",
      "Ep  46/100 | Loss: 0.7838 | LR: 0.01978 | Test Acc (EMA): 90.11%\n",
      "Ep  47/100 | Loss: 0.7802 | LR: 0.01924 | Test Acc (EMA): 90.31%\n",
      "Ep  48/100 | Loss: 0.7782 | LR: 0.01869 | Test Acc (EMA): 90.53%\n",
      "Ep  49/100 | Loss: 0.7668 | LR: 0.01815 | Test Acc (EMA): 90.48%\n",
      "Ep  50/100 | Loss: 0.7658 | LR: 0.01760 | Test Acc (EMA): 90.69%\n",
      "Ep  51/100 | Loss: 0.7639 | LR: 0.01705 | Test Acc (EMA): 90.80%\n",
      "Ep  52/100 | Loss: 0.7554 | LR: 0.01651 | Test Acc (EMA): 90.66%\n",
      "Ep  53/100 | Loss: 0.7459 | LR: 0.01596 | Test Acc (EMA): 90.77%\n",
      "Ep  54/100 | Loss: 0.7430 | LR: 0.01542 | Test Acc (EMA): 90.76%\n",
      "Ep  55/100 | Loss: 0.7389 | LR: 0.01488 | Test Acc (EMA): 90.95%\n",
      "Ep  56/100 | Loss: 0.7324 | LR: 0.01434 | Test Acc (EMA): 91.21%\n",
      "Ep  57/100 | Loss: 0.7308 | LR: 0.01380 | Test Acc (EMA): 91.24%\n",
      "Ep  58/100 | Loss: 0.7209 | LR: 0.01327 | Test Acc (EMA): 91.28%\n",
      "Ep  59/100 | Loss: 0.7218 | LR: 0.01275 | Test Acc (EMA): 91.43%\n",
      "Ep  60/100 | Loss: 0.7112 | LR: 0.01222 | Test Acc (EMA): 91.56%\n",
      "Ep  61/100 | Loss: 0.7052 | LR: 0.01171 | Test Acc (EMA): 91.51%\n",
      "Ep  62/100 | Loss: 0.7009 | LR: 0.01120 | Test Acc (EMA): 91.46%\n",
      "Ep  63/100 | Loss: 0.6964 | LR: 0.01069 | Test Acc (EMA): 91.48%\n",
      "Ep  64/100 | Loss: 0.6890 | LR: 0.01019 | Test Acc (EMA): 91.51%\n",
      "Ep  65/100 | Loss: 0.6878 | LR: 0.00970 | Test Acc (EMA): 91.59%\n",
      "Ep  66/100 | Loss: 0.6799 | LR: 0.00922 | Test Acc (EMA): 91.55%\n",
      "Ep  67/100 | Loss: 0.6721 | LR: 0.00874 | Test Acc (EMA): 91.63%\n",
      "Ep  68/100 | Loss: 0.6717 | LR: 0.00828 | Test Acc (EMA): 91.79%\n",
      "Ep  69/100 | Loss: 0.6618 | LR: 0.00782 | Test Acc (EMA): 91.90%\n",
      "Ep  70/100 | Loss: 0.6593 | LR: 0.00737 | Test Acc (EMA): 91.93%\n",
      "Ep  71/100 | Loss: 0.6540 | LR: 0.00694 | Test Acc (EMA): 91.92%\n",
      "Ep  72/100 | Loss: 0.6485 | LR: 0.00651 | Test Acc (EMA): 92.01%\n",
      "Ep  73/100 | Loss: 0.6431 | LR: 0.00609 | Test Acc (EMA): 92.00%\n",
      "Ep  74/100 | Loss: 0.6390 | LR: 0.00569 | Test Acc (EMA): 92.03%\n",
      "Ep  75/100 | Loss: 0.6362 | LR: 0.00530 | Test Acc (EMA): 92.12%\n",
      "Ep  76/100 | Loss: 0.6299 | LR: 0.00492 | Test Acc (EMA): 92.17%\n",
      "Ep  77/100 | Loss: 0.6239 | LR: 0.00455 | Test Acc (EMA): 92.19%\n",
      "Ep  78/100 | Loss: 0.6203 | LR: 0.00419 | Test Acc (EMA): 92.22%\n",
      "Ep  79/100 | Loss: 0.6150 | LR: 0.00385 | Test Acc (EMA): 92.25%\n",
      "Ep  80/100 | Loss: 0.6107 | LR: 0.00352 | Test Acc (EMA): 92.21%\n",
      "Ep  81/100 | Loss: 0.6087 | LR: 0.00321 | Test Acc (EMA): 92.21%\n",
      "Ep  82/100 | Loss: 0.6045 | LR: 0.00291 | Test Acc (EMA): 92.28%\n",
      "Ep  83/100 | Loss: 0.6013 | LR: 0.00262 | Test Acc (EMA): 92.22%\n",
      "Ep  84/100 | Loss: 0.5971 | LR: 0.00235 | Test Acc (EMA): 92.22%\n",
      "Ep  85/100 | Loss: 0.5952 | LR: 0.00210 | Test Acc (EMA): 92.15%\n",
      "Ep  86/100 | Loss: 0.5906 | LR: 0.00186 | Test Acc (EMA): 92.19%\n",
      "Ep  87/100 | Loss: 0.5889 | LR: 0.00163 | Test Acc (EMA): 92.21%\n",
      "Ep  88/100 | Loss: 0.5863 | LR: 0.00142 | Test Acc (EMA): 92.16%\n",
      "Ep  89/100 | Loss: 0.5836 | LR: 0.00123 | Test Acc (EMA): 92.15%\n",
      "Ep  90/100 | Loss: 0.5823 | LR: 0.00105 | Test Acc (EMA): 92.09%\n",
      "Ep  91/100 | Loss: 0.5797 | LR: 0.00089 | Test Acc (EMA): 92.16%\n",
      "Ep  92/100 | Loss: 0.5763 | LR: 0.00075 | Test Acc (EMA): 92.15%\n",
      "Ep  93/100 | Loss: 0.5767 | LR: 0.00062 | Test Acc (EMA): 92.21%\n",
      "Ep  94/100 | Loss: 0.5758 | LR: 0.00051 | Test Acc (EMA): 92.19%\n",
      "Ep  95/100 | Loss: 0.5740 | LR: 0.00041 | Test Acc (EMA): 92.17%\n",
      "Ep  96/100 | Loss: 0.5727 | LR: 0.00034 | Test Acc (EMA): 92.15%\n",
      "Ep  97/100 | Loss: 0.5714 | LR: 0.00028 | Test Acc (EMA): 92.12%\n",
      "Ep  98/100 | Loss: 0.5711 | LR: 0.00023 | Test Acc (EMA): 92.11%\n",
      "Ep  99/100 | Loss: 0.5714 | LR: 0.00021 | Test Acc (EMA): 92.12%\n",
      "Ep 100/100 | Loss: 0.5708 | LR: 0.00020 | Test Acc (EMA): 92.09%\n",
      "\n",
      "Training Complete.\n",
      "Saved model to cnn9_backprop.pth\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import copy\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Standard Backprop Model (CNN9)\n",
    "# -----------------------------\n",
    "class CNN9_Backprop(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Block 1: 32x32 -> 16x16\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Block 2: 16x16 -> 8x8\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Block 3: 8x8 -> 4x4\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Block 4: 4x4 -> 2x2\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(512 * 2 * 2, 10)\n",
    "\n",
    "        # Weight Initialization (Kaiming Normal) to match original\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        # Block 2\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        # Block 3\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        \n",
    "        # Block 4\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        \n",
    "        # Flatten and Classify\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "class ModelEMA:\n",
    "    \"\"\"Standard Exponential Moving Average for model weights\"\"\"\n",
    "    def __init__(self, model, decay=0.9995):\n",
    "        self.ema_model = copy.deepcopy(model)\n",
    "        self.ema_model.eval()\n",
    "        self.decay = decay\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        for ema_param, param in zip(self.ema_model.parameters(), model.parameters()):\n",
    "            ema_param.data.mul_(self.decay).add_(param.data, alpha=(1.0 - self.decay))\n",
    "\n",
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out a rectangular section of the image\"\"\"\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "    def __call__(self, img):\n",
    "        _, h, w = img.shape\n",
    "        mask = torch.ones((h, w), dtype=torch.float32)\n",
    "        y = torch.randint(h, (1,)).item()\n",
    "        x = torch.randint(w, (1,)).item()\n",
    "        y1 = max(0, y - self.length // 2)\n",
    "        y2 = min(h, y + self.length // 2)\n",
    "        x1 = max(0, x - self.length // 2)\n",
    "        x2 = min(w, x + self.length // 2)\n",
    "        img[:, y1:y2, x1:x2] = 0.0\n",
    "        return img\n",
    "\n",
    "def cosine_lr(optimizer, step, total_steps, lr_max, lr_min):\n",
    "    \"\"\"Update optimizer learning rate based on cosine schedule\"\"\"\n",
    "    t = step / max(1, total_steps)\n",
    "    lr = lr_min + 0.5 * (lr_max - lr_min) * (1.0 + math.cos(math.pi * t))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# -----------------------------\n",
    "# Main Training Loop\n",
    "# -----------------------------\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # Data config\n",
    "    cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "    cifar_std  = (0.2470, 0.2435, 0.2616)\n",
    "    \n",
    "    train_tfm = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cifar_mean, cifar_std),\n",
    "        Cutout(length=8)\n",
    "    ])\n",
    "    test_tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cifar_mean, cifar_std)\n",
    "    ])\n",
    "    \n",
    "    train_ds = datasets.CIFAR10(\"./data\", train=True, download=True, transform=train_tfm)\n",
    "    test_ds  = datasets.CIFAR10(\"./data\", train=False, download=True, transform=test_tfm)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Hyperparameters\n",
    "    epochs = 100\n",
    "    lr_max = 0.035\n",
    "    lr_min = 0.0002\n",
    "    momentum = 0.9\n",
    "    weight_decay = 5e-4\n",
    "    label_smoothing = 0.1\n",
    "    \n",
    "    # Model Setup\n",
    "    model = CNN9_Backprop().to(device)\n",
    "    ema = ModelEMA(model, decay=0.9995)\n",
    "    \n",
    "    # Optimizer (Standard SGD)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr_max, momentum=momentum, weight_decay=weight_decay)\n",
    "    \n",
    "    # Loss Function (CrossEntropy with Label Smoothing)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "    print(f\"\\nStarting training with Standard Backprop...\")\n",
    "    print(f\"Architecture: CNN9 (PyTorch Module)\")\n",
    "    print(f\"Epochs: {epochs} | Batch: 64 | LR: {lr_max}->{lr_min}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    total_steps = epochs * len(train_loader)\n",
    "    global_step = 0\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            \n",
    "            # 1. Update LR\n",
    "            curr_lr = cosine_lr(optimizer, global_step, total_steps, lr_max, lr_min)\n",
    "            \n",
    "            # 2. Forward\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            # 3. Backward\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 5. Update EMA\n",
    "            ema.update(model)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "        \n",
    "        # Evaluate\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        test_acc = evaluate(ema.ema_model, test_loader, device) # Check EMA accuracy\n",
    "        \n",
    "        print(f\"Ep {ep:3d}/{epochs} | Loss: {train_loss:.4f} | LR: {curr_lr:.5f} | Test Acc (EMA): {test_acc*100:.2f}%\")\n",
    "\n",
    "    print(\"\\nTraining Complete.\")\n",
    "    torch.save(ema.ema_model.state_dict(), \"cnn9_backprop.pth\")\n",
    "    print(\"Saved model to cnn9_backpropuglio20.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a68e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
